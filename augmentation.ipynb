{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download configuratiins\n",
    "download_from_remote = True\n",
    "load_from_local = True\n",
    "save_to_local = True\n",
    "local_dataset = \"./dataset/fever_dataset\"\n",
    "local_adversarial = \"./dataset/adversarial_dataset\"\n",
    "%mkdir -p {local_dataset}\n",
    "%mkdir -p {local_adversarial}\n",
    "\n",
    "# data analytics\n",
    "test_tokenizer = False\n",
    "print_statistics = False\n",
    "\n",
    "# dataset exploration\n",
    "general_structure = True\n",
    "show_ids = False\n",
    "show_labels = False\n",
    "wsd_exploration = False\n",
    "srl_exploration = True\n",
    "\n",
    "# semantic roles\n",
    "save_info = False\n",
    "info = \"./info/\"\n",
    "%mkdir -p {info}\n",
    "\n",
    "\n",
    "# augmentation\n",
    "syn_hyp_augment = True\n",
    "save_graph = False\n",
    "load_graph = True\n",
    "graph = \"./relational_graph\"\n",
    "%mkdir -p {graph}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leeoos/miniconda3/envs/nlp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package wordnet to /home/leeoos/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/leeoos/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# general imports\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# visualization and statistics\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from datasets import(\n",
    "  Dataset, \n",
    "  load_dataset, \n",
    "  load_from_disk, \n",
    "  concatenate_datasets,\n",
    ") \n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# spacy\n",
    "import spacy \n",
    "# Load the SpaCy model\n",
    "nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# huggingface\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "# set seeds\n",
    "set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data from local repository\n",
      "Done!\n",
      "Train ssplit length: 51086\n",
      "Max id in train split: 99998\n"
     ]
    }
   ],
   "source": [
    "#@title Download data or just load from local \n",
    "\n",
    "download_from_remote = not(os.path.exists(local_dataset) and os.path.exists(local_adversarial))\n",
    "\n",
    "if download_from_remote:\n",
    "    print(\"Downloading data from remote repository\")\n",
    "\n",
    "    # load chunk of FEVER datyaset\n",
    "    fever_dataset = load_dataset(\"tommasobonomo/sem_augmented_fever_nli\", trust_remote_code=True)\n",
    "\n",
    "    # load adversarial \n",
    "    adversarial_testset = load_dataset(\"iperbole/adversarial_fever_nli\", trust_remote_code=True)\n",
    "\n",
    "    # structure of the dataset\n",
    "    print(fever_dataset)\n",
    "\n",
    "    if save_to_local:\n",
    "        print(f\"Save data in local {local_dataset}\")\n",
    "        fever_dataset.save_to_disk(local_dataset)\n",
    "        print(f\"Save adversarial dataset in {local_adversarial}\")\n",
    "        adversarial_testset.save_to_disk(local_adversarial)\n",
    "\n",
    "elif load_from_local:\n",
    "    print(f\"Load data from local repository\")\n",
    "    fever_dataset = load_from_disk(local_dataset)\n",
    "    adversarial_testset = load_from_disk(local_adversarial)\n",
    "    \n",
    "print(\"Done!\")\n",
    "\n",
    "print(f\"Train ssplit length: {len(fever_dataset['train'])}\")\n",
    "max_id = max(fever_dataset['train']['id'])\n",
    "print(f\"Max id in train split: {max_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Tokenization function for datapoint visualization\n",
    "\n",
    "if test_tokenizer:\n",
    "        \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    label_map = {\n",
    "        'ENTAILMENT': 0,\n",
    "        'NEUTRAL': 1,\n",
    "        'CONTRADICTION': 2,\n",
    "        'NOT ENOUGH INFO': None\n",
    "    }\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        examples['label'] = [label_map[label] for label in examples['label']]\n",
    "        return tokenizer(examples['premise'], examples['hypothesis'], padding=True, truncation=True)\n",
    "\n",
    "\n",
    "    # tokenized = fever_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Exploration utils\n",
    "\n",
    "def pretty_print_dict(d, indent=0):\n",
    "    for key, value in d.items():\n",
    "        print(' ' * indent + str(key) + ':', end=' ')\n",
    "        if isinstance(value, dict):\n",
    "            print()\n",
    "            pretty_print_dict(value, indent + 4)\n",
    "        else:\n",
    "            print(value)\n",
    "\n",
    "\n",
    "def plot_labels_distribution(target_set, title=''):\n",
    "\n",
    "    labels = [\n",
    "        'ENTAILMENT',\n",
    "        'NEUTRAL',\n",
    "        'CONTRADICTION',\n",
    "    ]\n",
    "\n",
    "    label_counts = {}\n",
    "    for label in target_set['label']:\n",
    "        if label not in label_counts:\n",
    "            label_counts[label] = 0\n",
    "        label_counts[label] += 1\n",
    "\n",
    "    plt.bar(labels, label_counts.values())\n",
    "    plt.xlabel('Label')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'Distribution of labels in {title}')\n",
    "    plt.show()\n",
    "    print()\n",
    "        \n",
    "\n",
    "def plot_lengths_distribution(target_set, title='', compare_length=False):\n",
    "    # extract premises and hypotheses\n",
    "    premises = [item['premise'] for item in target_set]\n",
    "    hypotheses = [item['hypothesis'] for item in target_set]\n",
    "\n",
    "    # compute lengths\n",
    "    premise_lengths = [len(premise.split()) for premise in premises]\n",
    "    hypothesis_lengths = [len(hypothesis.split()) for hypothesis in hypotheses]\n",
    "\n",
    "    # plotting length distributions\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    sns.histplot(premise_lengths, bins=50, kde=True, ax=axes[0], color='blue', log_scale=(False, True))\n",
    "    axes[0].set_title('Premise Length Distribution')\n",
    "    axes[0].set_xlabel('Number of Words')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "\n",
    "    sns.histplot(hypothesis_lengths, bins=50, kde=True, ax=axes[1], color='green', log_scale=(False, True))\n",
    "    axes[1].set_title('Hypothesis Length Distribution')\n",
    "    axes[1].set_xlabel('Number of Words')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "\n",
    "    fig.suptitle(f'Premise and Hypothesis Length Distribution in {title}')\n",
    "    plt.show()\n",
    "\n",
    "    if compare_length:\n",
    "        # plot premise vs hypothesis length scatter plot\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(premise_lengths, hypothesis_lengths, alpha=0.5, s=1)\n",
    "        plt.title('Premise vs Hypothesis Length')\n",
    "        plt.xlabel('Premise Length')\n",
    "        plt.ylabel('Hypothesis Length')\n",
    "        plt.yscale('log')\n",
    "        plt.xscale('log')\n",
    "        plt.grid(True, which=\"both\", ls=\"--\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Statistics about the regular dataset\n",
    "if print_statistics:\n",
    "  plot_labels_distribution(fever_dataset['train'], title=\"Fever Train Set\")\n",
    "  plot_lengths_distribution(fever_dataset['train'], title=\"Fever Train Set\", compare_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets structure: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'premise', 'hypothesis', 'label', 'wsd', 'srl'],\n",
      "        num_rows: 51086\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'premise', 'hypothesis', 'label', 'wsd', 'srl'],\n",
      "        num_rows: 2288\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'premise', 'hypothesis', 'label', 'wsd', 'srl'],\n",
      "        num_rows: 2287\n",
      "    })\n",
      "})\n",
      "\n",
      "SRL structure: \n",
      "dict_keys(['premise', 'hypothesis'])\n",
      "Tokens: \n",
      "{'index': 0, 'rawText': 'Deadpool'}\n",
      "{'index': 1, 'rawText': '('}\n",
      "{'index': 2, 'rawText': 'film'}\n",
      "{'index': 3, 'rawText': ')'}\n",
      "{'index': 4, 'rawText': '.'}\n",
      "{'index': 5, 'rawText': 'It'}\n",
      "{'index': 6, 'rawText': 'is'}\n",
      "{'index': 7, 'rawText': 'the'}\n",
      "{'index': 8, 'rawText': 'eighth'}\n",
      "{'index': 9, 'rawText': 'installment'}\n",
      "{'index': 10, 'rawText': 'and'}\n",
      "{'index': 11, 'rawText': 'a'}\n",
      "{'index': 12, 'rawText': 'spin'}\n",
      "{'index': 13, 'rawText': '-'}\n",
      "{'index': 14, 'rawText': 'off'}\n",
      "{'index': 15, 'rawText': 'in'}\n",
      "{'index': 16, 'rawText': 'the'}\n",
      "{'index': 17, 'rawText': 'X'}\n",
      "{'index': 18, 'rawText': '-'}\n",
      "{'index': 19, 'rawText': 'Men'}\n",
      "{'index': 20, 'rawText': 'film'}\n",
      "{'index': 21, 'rawText': 'series'}\n",
      "{'index': 22, 'rawText': ','}\n",
      "{'index': 23, 'rawText': 'and'}\n",
      "{'index': 24, 'rawText': 'stars'}\n",
      "{'index': 25, 'rawText': 'Ryan'}\n",
      "{'index': 26, 'rawText': 'Reynolds'}\n",
      "{'index': 27, 'rawText': 'as'}\n",
      "{'index': 28, 'rawText': 'the'}\n",
      "{'index': 29, 'rawText': 'titular'}\n",
      "{'index': 30, 'rawText': 'character'}\n",
      "{'index': 31, 'rawText': ','}\n",
      "{'index': 32, 'rawText': 'as'}\n",
      "{'index': 33, 'rawText': 'well'}\n",
      "{'index': 34, 'rawText': 'as'}\n",
      "{'index': 35, 'rawText': 'Morena'}\n",
      "{'index': 36, 'rawText': 'Baccarin'}\n",
      "{'index': 37, 'rawText': ','}\n",
      "{'index': 38, 'rawText': 'Ed'}\n",
      "{'index': 39, 'rawText': 'Skrein'}\n",
      "{'index': 40, 'rawText': ','}\n",
      "{'index': 41, 'rawText': 'T.'}\n",
      "{'index': 42, 'rawText': 'J.'}\n",
      "{'index': 43, 'rawText': 'Miller'}\n",
      "{'index': 44, 'rawText': ','}\n",
      "{'index': 45, 'rawText': 'Gina'}\n",
      "{'index': 46, 'rawText': 'Carano'}\n",
      "{'index': 47, 'rawText': ','}\n",
      "{'index': 48, 'rawText': 'Leslie'}\n",
      "{'index': 49, 'rawText': 'Uggams'}\n",
      "{'index': 50, 'rawText': ','}\n",
      "{'index': 51, 'rawText': 'Brianna'}\n",
      "{'index': 52, 'rawText': 'Hildebrand'}\n",
      "{'index': 53, 'rawText': ','}\n",
      "{'index': 54, 'rawText': 'and'}\n",
      "{'index': 55, 'rawText': 'Stefan'}\n",
      "{'index': 56, 'rawText': 'Kapičić'}\n",
      "{'index': 57, 'rawText': '.'}\n",
      "{'index': 58, 'rawText': 'T.'}\n",
      "{'index': 59, 'rawText': 'J.'}\n",
      "{'index': 60, 'rawText': 'Miller'}\n",
      "{'index': 61, 'rawText': '.'}\n",
      "{'index': 62, 'rawText': 'Miller'}\n",
      "{'index': 63, 'rawText': 'has'}\n",
      "{'index': 64, 'rawText': 'also'}\n",
      "{'index': 65, 'rawText': 'performed'}\n",
      "{'index': 66, 'rawText': 'in'}\n",
      "{'index': 67, 'rawText': 'such'}\n",
      "{'index': 68, 'rawText': 'films'}\n",
      "{'index': 69, 'rawText': 'as'}\n",
      "{'index': 70, 'rawText': 'Cloverfield'}\n",
      "{'index': 71, 'rawText': '('}\n",
      "{'index': 72, 'rawText': '2008'}\n",
      "{'index': 73, 'rawText': ')'}\n",
      "{'index': 74, 'rawText': ','}\n",
      "{'index': 75, 'rawText': 'She'}\n",
      "{'index': 76, 'rawText': \"'\"}\n",
      "{'index': 77, 'rawText': 's'}\n",
      "{'index': 78, 'rawText': 'Out'}\n",
      "{'index': 79, 'rawText': 'of'}\n",
      "{'index': 80, 'rawText': 'My'}\n",
      "{'index': 81, 'rawText': 'League'}\n",
      "{'index': 82, 'rawText': '('}\n",
      "{'index': 83, 'rawText': '2010'}\n",
      "{'index': 84, 'rawText': ')'}\n",
      "{'index': 85, 'rawText': ','}\n",
      "{'index': 86, 'rawText': 'Yogi'}\n",
      "{'index': 87, 'rawText': 'Bear'}\n",
      "{'index': 88, 'rawText': '3D'}\n",
      "{'index': 89, 'rawText': '('}\n",
      "{'index': 90, 'rawText': '2010'}\n",
      "{'index': 91, 'rawText': ')'}\n",
      "{'index': 92, 'rawText': ','}\n",
      "{'index': 93, 'rawText': 'How'}\n",
      "{'index': 94, 'rawText': 'to'}\n",
      "{'index': 95, 'rawText': 'Train'}\n",
      "{'index': 96, 'rawText': 'Your'}\n",
      "{'index': 97, 'rawText': 'Dragon'}\n",
      "{'index': 98, 'rawText': '('}\n",
      "{'index': 99, 'rawText': '2010'}\n",
      "{'index': 100, 'rawText': ')'}\n",
      "{'index': 101, 'rawText': ','}\n",
      "{'index': 102, 'rawText': 'How'}\n",
      "{'index': 103, 'rawText': 'to'}\n",
      "{'index': 104, 'rawText': 'Train'}\n",
      "{'index': 105, 'rawText': 'Your'}\n",
      "{'index': 106, 'rawText': 'Dragon'}\n",
      "{'index': 107, 'rawText': '2'}\n",
      "{'index': 108, 'rawText': '('}\n",
      "{'index': 109, 'rawText': '2014'}\n",
      "{'index': 110, 'rawText': ')'}\n",
      "{'index': 111, 'rawText': ','}\n",
      "{'index': 112, 'rawText': 'Transformers'}\n",
      "{'index': 113, 'rawText': ':'}\n",
      "{'index': 114, 'rawText': 'Age'}\n",
      "{'index': 115, 'rawText': 'of'}\n",
      "{'index': 116, 'rawText': 'Extinction'}\n",
      "{'index': 117, 'rawText': '('}\n",
      "{'index': 118, 'rawText': '2014'}\n",
      "{'index': 119, 'rawText': ')'}\n",
      "{'index': 120, 'rawText': ','}\n",
      "{'index': 121, 'rawText': 'Big'}\n",
      "{'index': 122, 'rawText': 'Hero'}\n",
      "{'index': 123, 'rawText': '6'}\n",
      "{'index': 124, 'rawText': '('}\n",
      "{'index': 125, 'rawText': '2014'}\n",
      "{'index': 126, 'rawText': ')'}\n",
      "{'index': 127, 'rawText': ','}\n",
      "{'index': 128, 'rawText': 'Deadpool'}\n",
      "{'index': 129, 'rawText': '('}\n",
      "{'index': 130, 'rawText': '2016'}\n",
      "{'index': 131, 'rawText': ')'}\n",
      "{'index': 132, 'rawText': ','}\n",
      "{'index': 133, 'rawText': 'and'}\n",
      "{'index': 134, 'rawText': 'Office'}\n",
      "{'index': 135, 'rawText': 'Chri'}\n",
      "{'index': 136, 'rawText': 'stmas'}\n",
      "{'index': 137, 'rawText': 'Party'}\n",
      "{'index': 138, 'rawText': '('}\n",
      "{'index': 139, 'rawText': '2016'}\n",
      "{'index': 140, 'rawText': ')'}\n",
      "{'index': 141, 'rawText': '.'}\n",
      "\n",
      "Annotations: \n",
      "tokenIndex:\t6\n",
      "verbatlas:\t{'frameName': 'COPULA', 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [5, 6]}, {'role': 'Attribute', 'score': 1.0, 'span': [7, 22]}]}\n",
      "englishPropbank:\t{'frameName': 'be.01', 'roles': [{'role': 'ARG1', 'score': 1.0, 'span': [5, 6]}, {'role': 'ARG2', 'score': 1.0, 'span': [7, 22]}]}\n",
      "\n",
      "tokenIndex:\t24\n",
      "verbatlas:\t{'frameName': 'PERFORM', 'roles': [{'role': 'Agent', 'score': 1.0, 'span': [5, 6]}, {'role': 'Theme', 'score': 1.0, 'span': [25, 31]}, {'role': 'Predicative', 'score': 1.0, 'span': [32, 33]}]}\n",
      "englishPropbank:\t{'frameName': 'star.01', 'roles': [{'role': 'ARG1', 'score': 1.0, 'span': [5, 6]}, {'role': 'ARG2', 'score': 1.0, 'span': [25, 27]}, {'role': 'ARG2', 'score': 1.0, 'span': [27, 31]}, {'role': 'ARGM-PRD', 'score': 1.0, 'span': [32, 57]}]}\n",
      "\n",
      "tokenIndex:\t60\n",
      "verbatlas:\t{'frameName': 'COPULA', 'roles': []}\n",
      "englishPropbank:\t{'frameName': 'be.01', 'roles': [{'role': 'ARGM-ADJ', 'score': 1.0, 'span': [58, 59]}, {'role': 'ARGM-ADJ', 'score': 1.0, 'span': [59, 60]}]}\n",
      "\n",
      "tokenIndex:\t63\n",
      "verbatlas:\t{'frameName': 'AUXILIARY', 'roles': []}\n",
      "englishPropbank:\t{'frameName': 'have.01', 'roles': []}\n",
      "\n",
      "tokenIndex:\t65\n",
      "verbatlas:\t{'frameName': 'PERFORM', 'roles': [{'role': 'Agent', 'score': 1.0, 'span': [62, 63]}, {'role': 'Adverbial', 'score': 1.0, 'span': [64, 65]}, {'role': 'Location', 'score': 1.0, 'span': [66, 136]}]}\n",
      "englishPropbank:\t{'frameName': 'perform.01', 'roles': [{'role': 'ARG0', 'score': 1.0, 'span': [62, 63]}, {'role': 'ARGM-ADV', 'score': 1.0, 'span': [64, 65]}, {'role': 'ARGM-LOC', 'score': 1.0, 'span': [66, 136]}]}\n",
      "\n",
      "tokenIndex:\t95\n",
      "verbatlas:\t{'frameName': 'TEACH', 'roles': [{'role': 'Attribute', 'score': 1.0, 'span': [93, 94]}, {'role': 'Recipient', 'score': 1.0, 'span': [96, 98]}]}\n",
      "englishPropbank:\t{'frameName': 'train.01', 'roles': [{'role': 'ARGM-MNR', 'score': 1.0, 'span': [93, 94]}, {'role': 'ARG2', 'score': 1.0, 'span': [96, 98]}]}\n",
      "\n",
      "tokenIndex:\t104\n",
      "verbatlas:\t{'frameName': 'TEACH', 'roles': [{'role': 'Attribute', 'score': 1.0, 'span': [102, 103]}, {'role': 'Recipient', 'score': 1.0, 'span': [105, 108]}]}\n",
      "englishPropbank:\t{'frameName': 'train.01', 'roles': [{'role': 'ARGM-MNR', 'score': 1.0, 'span': [102, 103]}, {'role': 'ARG2', 'score': 1.0, 'span': [105, 108]}]}\n",
      "\n",
      "tokenIndex:\t137\n",
      "verbatlas:\t{'frameName': 'AUXILIARY', 'roles': []}\n",
      "englishPropbank:\t{'frameName': 'have.03', 'roles': []}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Dataset structure\n",
    "\n",
    "if general_structure:\n",
    "  print(f\"Datasets structure: {fever_dataset}\")\n",
    "\n",
    "if show_ids:\n",
    "  print(f\"Train IDs: {fever_dataset['train']['id']}\")\n",
    "\n",
    "if show_labels:\n",
    "  print(f\"Train labels: {fever_dataset['train']['label']}\")\n",
    "\n",
    "\n",
    "if wsd_exploration:\n",
    "\n",
    "  print(f\"\\nWSD srtructure: \")\n",
    "  pretty_print_dict(fever_dataset['train'][42]['wsd'])\n",
    "\n",
    "  sample_range = len(fever_dataset['train'])\n",
    "  loop = tqdm(range(sample_range))\n",
    "\n",
    "  wsd_info = dict()\n",
    "  wsd_info['pos'] = dict()\n",
    "\n",
    "  for i in loop:\n",
    "\n",
    "    data =  fever_dataset['train'][i]\n",
    "    sample_id = data['id']\n",
    "\n",
    "    hypothesis = data['hypothesis']\n",
    "    hyp_wsd = data['wsd']['hypothesis']\n",
    "\n",
    "    for hyp_wsd_dict in hyp_wsd:\n",
    "      \n",
    "      pos  = hyp_wsd_dict['pos']\n",
    "      wsd_info['pos'][pos] = 1 if pos not in wsd_info['pos'] else wsd_info['pos'][pos] + 1\n",
    "\n",
    "  print(\"POS:\")\n",
    "  pretty_print_dict(wsd_info)\n",
    "\n",
    "  with open(info + \"wsd_pos.txt\", \"w\") as wsd_pos:\n",
    "    print(f\"Saving allPOS of the dataset into {info + 'wsd_pos.txt'}\")\n",
    "    for elem in wsd_info['pos']: wsd_pos.write(f\"{elem}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "if srl_exploration:\n",
    "  print(\"\\nSRL structure: \")\n",
    "  print(fever_dataset['train'][42]['srl'].keys())\n",
    "  # print(fever_dataset['train'][42]['srl']['hypothesis']['tokens'])\n",
    "  # print(fever_dataset['train'][42]['srl']['hypothesis']['annotations'])\n",
    "\n",
    "  print(\"Tokens: \")\n",
    "  for token in fever_dataset['train'][42]['srl']['premise']['tokens']:\n",
    "      print(token)\n",
    "\n",
    "  print(\"\\nAnnotations: \")\n",
    "  for annotation in fever_dataset['train'][42]['srl']['premise']['annotations']:\n",
    "      for key, value in annotation.items():\n",
    "          print(f\"{key}:\\t{value}\")\n",
    "      print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title SRL exploration: collect frame-names and roles \n",
    "\n",
    "def get_srl_info(dataset):\n",
    "\n",
    "  sample_range = len(dataset)\n",
    "  loop = tqdm(range(sample_range))\n",
    "\n",
    "  verbs_freqs = dict()\n",
    "  set_of_va_frames = dict() # verb atlas frames\n",
    "  set_of_pb_frames = dict() # prop bank frames\n",
    "  set_of_va_roles = set() # verb atlas frames\n",
    "  set_of_pb_roles = set() # prop bank frames\n",
    "\n",
    "  for i in loop:\n",
    "\n",
    "    data =  dataset[i]\n",
    "    sample_id = data['id']\n",
    "    \n",
    "    tokens = data['srl']['premise']['tokens']\n",
    "    annotations = data['srl']['premise']['annotations']\n",
    "\n",
    "    for annotation in annotations:\n",
    "\n",
    "      token_index = annotation['tokenIndex']\n",
    "      verb = tokens[token_index]['rawText']\n",
    "      verbs_freqs[verb] =  1 if verb not in  verbs_freqs else verbs_freqs[verb] + 1\n",
    "\n",
    "      vb_frame = annotation['verbatlas']['frameName']\n",
    "      pb_frame = annotation['englishPropbank']['frameName']\n",
    "\n",
    "      set_of_va_frames[vb_frame] = 1 if vb_frame not in set_of_va_frames else set_of_va_frames[vb_frame] + 1 \n",
    "      set_of_pb_frames[pb_frame] = 1 if pb_frame not in set_of_pb_frames else set_of_pb_frames[pb_frame] + 1 \n",
    "\n",
    "      va_roles = annotation['verbatlas']['roles']\n",
    "      pb_roles = annotation['englishPropbank']['roles']\n",
    "\n",
    "      for role in va_roles:\n",
    "        set_of_va_roles.add(role['role'])\n",
    "\n",
    "      for role in pb_roles:\n",
    "        set_of_pb_roles.add(role['role'])\n",
    "\n",
    "  return set_of_va_frames, set_of_pb_frames, set_of_va_roles, set_of_pb_roles, verbs_freqs\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_info:\n",
    "\n",
    "  set_of_va_frames, set_of_pb_frames, set_of_va_roles, set_of_pb_roles, verbs_freqs = get_srl_info(fever_dataset['train'])\n",
    "\n",
    "  verbs_freqs = sorted(verbs_freqs.items(), key=lambda item: item[1], reverse=True)\n",
    "  set_of_pb_frames = sorted(set_of_pb_frames.items(), key=lambda item: item[1], reverse=True)\n",
    "  set_of_va_frames = sorted(set_of_va_frames.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "  print(f\"Number of Verb Atlas frames: {len(set_of_va_frames)}\")\n",
    "  print(f\"Number of Verb Atlas roles: {len(set_of_va_roles)}\")\n",
    "\n",
    "  print(f\"Number of Propbank frames: {len(set_of_pb_frames)}\")\n",
    "  print(f\"Number of Propbank roles: {len(set_of_pb_roles)}\")\n",
    "\n",
    "  with open(info + \"va_roles.txt\", \"w\") as va_roles:\n",
    "    print(f\"Saving all Verb Atlas roles of the dataset into {info + 'va_roles.txt'}\")\n",
    "    for elem in set_of_va_roles: va_roles.write(f\"{elem}\\n\")\n",
    "\n",
    "  with open(info + \"va_frames.txt\", \"w\") as va_frames:\n",
    "    print(f\"Saving all Verb Atlas frames of the dataset into {info + 'va_frames.txt'}\")\n",
    "    for elem in set_of_va_frames: va_frames.write(f\"{elem}\\n\")\n",
    "\n",
    "\n",
    "  with open(info + \"pb_roles.txt\", \"w\") as pb_roles:\n",
    "    print(f\"Saving all Propbank roles of the dataset into {info + 'pb_roles.txt'}\")\n",
    "    for elem in set_of_pb_roles: pb_roles.write(f\"{elem}\\n\")\n",
    "\n",
    "\n",
    "  with open(info + \"pb_frames.txt\", \"w\") as pb_frames:\n",
    "    print(f\"Saving all Propbank frames of the dataset into {info + 'pb_frames.txt'}\")\n",
    "    for elem in set_of_pb_frames: pb_frames.write(f\"{elem}\\n\")\n",
    "\n",
    "  with open(info + \"verbs_freqs.txt\", \"w\") as verbs:\n",
    "    print(f\"Saving all verbs frequencies count into {info + 'verbs_freqs.txt'}\")\n",
    "    for elem in verbs_freqs: verbs.write(f\"{elem}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Augmentation\n",
    "The following prompt can be used to correct the grammar of a modified hypotesis so if just a not is added the phrase is transformed in negative form:\n",
    "\n",
    "\"Correct the grammar in the following inputs, rephrase the input if necessary to make them more accurate. Provide just the correct version, no explanation.\"\n",
    "\n",
    "Ask for:\n",
    "symmetric relation --> Marry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Augmentation utils\n",
    "\n",
    "def get_sentence_from_span(tokens, span_begin, span_end):\n",
    "    sentence = \"\"\n",
    "    try: \n",
    "      sentence = \" \".join([tokens[index]['rawText']  for index in range(span_begin, span_end + 1)]) # if tokens[index]['rawText'] in names])\n",
    "    except:\n",
    "      sentence = \" \".join([tokens[index]['rawText'] for index in range(span_begin, span_end)]) #if tokens[index]['rawText'] in names])\n",
    "\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def join_strings_smartly(words):\n",
    "    \"\"\" Joins a list of words smartly:\n",
    "    - Adds spaces between words when appropriate.\n",
    "    - Avoids adding spaces before punctuation.\n",
    "    \"\"\"\n",
    "    punctuation = {'.', ',', ';', ':', '!', '?',')'}\n",
    "    result = words[0]\n",
    "    prev = result\n",
    "\n",
    "    for word in words[1:]:\n",
    "      if word in punctuation or \\\n",
    "        (\"'\" in prev) or \\\n",
    "        word.startswith(\"'\") or \\\n",
    "        (\".\" in prev and \".\" in word) or \\\n",
    "        (\"(\" in prev) :\n",
    "        # add word without space before\n",
    "        result += word\n",
    "      else:\n",
    "        # add with space before\n",
    "        result += \" \" + word\n",
    "      # keep track of previous word  \n",
    "      prev = word\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_synset_from_id(synset_id):\n",
    "    if synset_id == 'O':\n",
    "        return None\n",
    "    try:\n",
    "        offset = int(''.join(filter(str.isdigit, synset_id)))\n",
    "        pos = synset_id[-1]\n",
    "        synset = wn.synset_from_pos_and_offset(pos, offset)\n",
    "        return synset\n",
    "    except:\n",
    "        print(\"exception\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "def get_related_word(synset, pos): \n",
    "    info = dict()\n",
    "\n",
    "    # Map POS tags to WordNet POS tags\n",
    "    pos_map = {\n",
    "        'NOUN': wn.NOUN,\n",
    "        # 'VERB': wn.VERB,\n",
    "        'ADJ': wn.ADJ,\n",
    "        'ADV': wn.ADV\n",
    "    }\n",
    "    \n",
    "    if pos not in pos_map:\n",
    "        return None \n",
    "    \n",
    "    # get hypernyms\n",
    "    hypernyms = synset.hypernyms()\n",
    "    if not hypernyms:\n",
    "        return None\n",
    "    hypernyms = synset.hypernyms()\n",
    "    # hypernym_words = set()\n",
    "    # for hypernym in hypernyms:\n",
    "    #     hypernym_words.update(hypernym.lemma_names())\n",
    "    info['hypernyms'] = [hypernyms[0].lemma_names()[0]] #list(hypernym_words)\n",
    "\n",
    "    # get synonyms \n",
    "    synonyms = synset.lemmas()\n",
    "    if not synonyms:\n",
    "        return None\n",
    "    info['synonyms'] = [synonym.name() for synonym in synonyms if synonym.name() != synset.lemmas()[0].name()]\n",
    "\n",
    "    return info\n",
    "\n",
    "\n",
    "def extract_names(wsd_data):\n",
    "    names = []\n",
    "    current_name = []\n",
    "\n",
    "    for entry in wsd_data:\n",
    "        if entry['pos'] == 'PROPN':\n",
    "            current_name.append(entry['text'])\n",
    "        else:\n",
    "            if current_name:\n",
    "                names.append(' '.join(current_name))\n",
    "                current_name = []\n",
    "\n",
    "    # catch any remaining name at the end\n",
    "    if current_name:\n",
    "        names.append(' '.join(current_name))\n",
    "    \n",
    "    return names\n",
    "\n",
    "\n",
    "def extract_partial_match_name(text, name_list):\n",
    "    # tokenize the input text\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    names = []\n",
    "    \n",
    "    # iterate through each name in the name list\n",
    "    for name in name_list:\n",
    "        name_parts = name.split()\n",
    "\n",
    "        # check for partial match in the tokenized words\n",
    "        for i in range(len(words) - len(name_parts) + 1):\n",
    "            if words[i:i + len(name_parts)] == name_parts[:len(words[i:i + len(name_parts)])]:\n",
    "              names.append(name)\n",
    "              \n",
    "    if names: return names\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_dates(text):\n",
    "    # regular expression patterns for different date formats\n",
    "    patterns = [\n",
    "        r'\\b(\\d{4})\\b',  # matches a 4-digit year\n",
    "        r'\\b(\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4})\\b',  # matches dates like dd-mm-yyyy, dd/mm/yyyy, dd-mm-yy, dd/mm/yy\n",
    "        r'\\b(\\d{1,2} [A-Za-z]+ \\d{4})\\b',  # matches dates like 1 January 2020\n",
    "        r'\\b([A-Za-z]+ \\d{1,2}, \\d{4})\\b'  # matches dates like January 1, 2020\n",
    "    ]\n",
    "\n",
    "    months_list = [\n",
    "        \"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
    "        \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"\n",
    "    ]\n",
    "\n",
    "    pattern = re.compile(r'(?:(?P<day>\\d{1,2})(?:st|nd|rd|th)?[ ,]*)?(?:(?P<month>[A-Za-z]+)[ ,]*)?(?:(?P<year>\\d{4}))?')\n",
    "    matches = pattern.findall(text)\n",
    "    day, month, year = None, None, None\n",
    "\n",
    "    for match in matches:\n",
    "        if match[0]:\n",
    "            day = match[0]\n",
    "        if match[1]:\n",
    "            month = match[1]\n",
    "        if match[2]:\n",
    "            year = match[2]\n",
    "        \n",
    "        # more check to avoid error!\n",
    "        if day and not (1 <= int(day) <= 31):\n",
    "            day = None\n",
    "        if month and month not in months_list:\n",
    "            month = None\n",
    "            \n",
    "    return day, month, year\n",
    "\n",
    "def extract_locations(sentence):\n",
    "    locations = set()\n",
    "    entities =  nlp_spacy(sentence).ents\n",
    "    if entities:\n",
    "        found_target = False\n",
    "        for ent in entities:\n",
    "            if ent.label_ == \"GPE\":\n",
    "                found_target = True\n",
    "                locations.add(ent.text)\n",
    "    return locations\n",
    "\n",
    "\n",
    "movie_titles = [\n",
    "    \"John Wick: Chapter 2\",\n",
    "    \"On the Road (film)\",\n",
    "    \"Brave\",\n",
    "    \"Penny Dreadful\",\n",
    "    \"Snooki & Jwoww\",\n",
    "    \"Sons of Anarchy\",\n",
    "    \"The Sopranos\",\n",
    "    \"Thor: The Dark World\",\n",
    "    \"Winter Passing\",\n",
    "    \"Teen Wolf\",\n",
    "    \"Captain America: The Winter Soldier\",\n",
    "    \"The Belko Experiment\",\n",
    "    \"Fantastic Beasts and Where to Find Them\",\n",
    "    \"A Monster Calls\",\n",
    "    \"The Fate of the Furious\",\n",
    "    \"The Wolf of Wall Street (2013 film)\",\n",
    "    \"Tropico\",\n",
    "    \"Rescue Me\",\n",
    "    \"The Night Of\",\n",
    "    \"Lipstick Under My Burkha\",\n",
    "    \"The Promise\",\n",
    "    \"Sleeping Beauty\",\n",
    "    \"Mad Men\",\n",
    "    \"Avatar\",\n",
    "    \"Zootopia\",\n",
    "    \"Spider-Man\",\n",
    "    \"Enemy\",\n",
    "    \"Room\",\n",
    "    \"Cloud Atlas\",\n",
    "    \"Kong: Skull Island\",\n",
    "    \"Rick and Morty\",\n",
    "    \"Ink Master\",\n",
    "    \"Frenemies\",\n",
    "    \"Persuasion (2007 film)\",\n",
    "    \"Ballet Shoes\",\n",
    "    \"The Great Buck Howard\",\n",
    "    \"Schindler's List\",\n",
    "    \"Iron Man\",\n",
    "    \"The Illusionist\",\n",
    "    \"The Messenger\",\n",
    "    \"The Suite Life Movie\",\n",
    "    \"Wild\",\n",
    "    \"The Ren & Stimpy Show\",\n",
    "    \"Johnny Mnemonic\",\n",
    "    \"Denial (2016 film)\",\n",
    "    \"Black Sails\",\n",
    "    \"The Breakfast Club\",\n",
    "    \"Modern Family\",\n",
    "    \"Interstellar\",\n",
    "    \"To the Bone\",\n",
    "    \"Prison Break\",\n",
    "    \"Game of Thrones (season 3)\",\n",
    "    \"Line of Duty\",\n",
    "    \"In the Heart of the Sea\",\n",
    "    \"Oz the Great and Powerful\",\n",
    "    \"Attack on Titan\",\n",
    "    \"Her\",\n",
    "    \"The Carmichael Show\",\n",
    "    \"The Leftovers\",\n",
    "    \"Short Term 12\",\n",
    "    \"Stephanie Daley\",\n",
    "    \"Fargo\",\n",
    "    \"BoJack Horseman\",\n",
    "    \"New Girl\",\n",
    "    \"Glee\",\n",
    "    \"Turn: Washington's Spies\",\n",
    "    \"X-Men: Days of Future Past\",\n",
    "    \"Miss Peregrine's Home for Peculiar Children\",\n",
    "    \"Ghostbusters\",\n",
    "    \"Famous in Love\",\n",
    "    \"Legion\",\n",
    "    \"Spider-Man 3\",\n",
    "    \"Deadpool (film)\",\n",
    "    \"Doctor Who\",\n",
    "    \"San Junipero\",\n",
    "    \"Outlander (TV series)\",\n",
    "    \"Elementary\",\n",
    "    \"The Shield\",\n",
    "    \"Broadchurch\",\n",
    "    \"Fairy Tail\",\n",
    "    \"Major Barbara\",\n",
    "    \"American Horror Story\",\n",
    "    \"Trolls\",\n",
    "    \"Harry Potter\",\n",
    "    \"Whiplash\",\n",
    "    \"Split (2016 American film)\",\n",
    "    \"How to Be\",\n",
    "    \"Cars Toons\",\n",
    "    \"Little Miss Sunshine\",\n",
    "    \"Naruto\",\n",
    "    \"Horrible Bosses\",\n",
    "    \"There Will Be Blood\",\n",
    "    \"Beauty and the Beast\",\n",
    "    \"Grey's Anatomy\",\n",
    "    \"Futurama\",\n",
    "    \"The Strain\",\n",
    "    \"The Avengers\",\n",
    "    \"The Vampire Diaries\",\n",
    "    \"True Detective\",\n",
    "    \"Teen Wolf's sixth season\",\n",
    "    \"Interview\",\n",
    "    \"The Supernatural\",\n",
    "    \"The Girl on the Train\",\n",
    "    \"Interstellar\",\n",
    "    \"Guardians of the Galaxy\",\n",
    "    \"Miss Peregrine's Home for Peculiar Children\",\n",
    "    \"Love & Friendship\",\n",
    "    \"Goliyon Ki Raasleela Ram-Leela\"\n",
    "]\n",
    "\n",
    "def get_movie_title(sentence):\n",
    "    for title in movie_titles:\n",
    "        if re.search(r'\\b' + re.escape(title) + r'\\b', sentence):\n",
    "            return title\n",
    "\n",
    "# add new samples to the dataset\n",
    "# filtered_dataset = fever_dataset['train'].filter(lambda example: 'is a ' in example['premise'].lower()) # or 'is a' in example['hypothesis'].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Graph utils\n",
    "\n",
    "def get_sentences_by_id(dataset, id):\n",
    "  index = dataset['id'].index(str(id))\n",
    "  print(dataset[index]['premise'])\n",
    "  print(dataset[index]['hypothesis'])\n",
    "\n",
    "\n",
    "def format_date(dates):\n",
    "  select_date = None\n",
    "  for date in dates:\n",
    "    if not date: continue\n",
    "    if not date[2]: continue\n",
    "    if date[2]: select_date = date[2]\n",
    "    if date[1] and date[2]: select_date = date[1] + \", \" + date[2]\n",
    "    if date[0] and date[1] and date[2]:\n",
    "      select_date = date[1] + \" \" + date[0] + \", \" + date[2]\n",
    "      break\n",
    "  return select_date\n",
    "\n",
    "\n",
    "def get_all_locations(graph, frame):\n",
    "  ...\n",
    "  all_locations = set()\n",
    "  for key in graph[frame].keys():\n",
    "    for location in graph[frame][key]['locations']:\n",
    "      all_locations.add(location)\n",
    "  return list(all_locations)\n",
    "\n",
    "\n",
    "def get_all_dates(graph, frame):\n",
    "  ...\n",
    "  all_dates = set()\n",
    "  for key in graph[frame].keys():\n",
    "    date = format_date( graph[frame][key]['date'])\n",
    "    all_dates.add(date)\n",
    "  return list(all_dates)\n",
    "\n",
    "\n",
    "location_hierarchy = {\n",
    "   \"United States\": [\n",
    "      \"California\", \n",
    "      \"Texas\", \n",
    "      \"Illinois\", \n",
    "      \"District of Columbia\",\n",
    "      \"New York\",\n",
    "      \"New York City\",\n",
    "      \"Washington\",\n",
    "      \"D.C.\",\n",
    "      \"Chicago\",\n",
    "      \"Austin\",\n",
    "      \"Hollywood\",\n",
    "      \"Los Angeles\"\n",
    "    ],\n",
    "   \"the United States\": [\n",
    "      \"California\", \n",
    "      \"Texas\", \n",
    "      \"Illinois\", \n",
    "      \"District of Columbia\",\n",
    "      \"New York\",\n",
    "      \"New York City\",\n",
    "      \"Washington\",\n",
    "      \"D.C.\",\n",
    "      \"Chicago\",\n",
    "      \"Austin\",\n",
    "      \"Hollywood\",\n",
    "      \"Los Angeles\"\n",
    "    ],\n",
    "   \"US\": [\n",
    "      \"California\", \n",
    "      \"Texas\", \n",
    "      \"Illinois\", \n",
    "      \"District of Columbia\",\n",
    "      \"New York\",\n",
    "      \"New York City\",\n",
    "      \"Washington\",\n",
    "      \"D.C.\",\n",
    "      \"Chicago\",\n",
    "      \"Austin\",\n",
    "      \"Hollywood\",\n",
    "      \"Los Angeles\"\n",
    "    ],\n",
    "    \"California\": [\"Hollywood\", \"Los Angeles\"],\n",
    "    \"Texas\": [\"Austin\"],\n",
    "    \"Illinois\": [\"Chicago\"],\n",
    "    \"District of Columbia\": [\"Washington\", \"D.C.\"],\n",
    "    \"New York\": [\"New York City\"],\n",
    "    \"Canada\": [\"Alberta\", \"Edmonton\"],\n",
    "    \"Japan\": [\"Tokyo\"],\n",
    "    \"Australia\":  [\"Sydney\"],\n",
    "    \"Germany\": [\"Berlin\"],\n",
    "    \"Belgium\": [\"Belgium\"],\n",
    "    \"Philippines\": [\"Philippines\"],\n",
    "    \"United Kingdom\": [\"London\"]\n",
    "}\n",
    "\n",
    "def is_contained(location, container, location_hierarchy):\n",
    "  \n",
    "  if container in location_hierarchy:\n",
    "    if location in location_hierarchy[container]: \n",
    "      return True\n",
    "\n",
    "  return False\n",
    "\n",
    "def get_random_exclusive_element(main_list, exclusion_list):\n",
    "  exclusion_set = set(exclusion_list)\n",
    "  filtered_list = [item for item in main_list if item not in exclusion_set]\n",
    "  \n",
    "  if not filtered_list: return None  \n",
    "  choice = random.choice(filtered_list)\n",
    "  check = True\n",
    "  while check:\n",
    "    choice = random.choice(filtered_list)\n",
    "    for location in exclusion_list:\n",
    "      # print(\"choice \", choice)\n",
    "      # print(\"location \", location)\n",
    "      # print(\"is contained \", is_contained(choice, location, location_hierarchy))\n",
    "      # print(\"contains \", is_contained(location, choice, location_hierarchy))\n",
    "      # print()\n",
    "      related = is_contained(choice, location, location_hierarchy) or is_contained(location, choice, location_hierarchy) or choice == 'Netflix' or choice == 'Spike'\n",
    "      if not related: \n",
    "        check = False\n",
    "        break\n",
    "    \n",
    "  return choice\n",
    "\n",
    "# print(is_contained(\"the United States\", \"Tokyo\", location_hierarchy))  # True\n",
    "# print(is_contained(\"Tokyo\", \"the United States\", location_hierarchy))  # True\n",
    "\n",
    "def get_all_spouses(graph, frame):\n",
    "  ...\n",
    "  all_spouses = set()\n",
    "  for key in graph[frame].keys():\n",
    "    for spouse in graph[frame][key]['spouses']:\n",
    "      all_spouses.add(spouse)\n",
    "  return list(all_spouses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subsection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51086/51086 [01:18<00:00, 651.12it/s]\n"
     ]
    }
   ],
   "source": [
    "#@title Build relational graph\n",
    "\n",
    "# dataset = filtered_dataset\n",
    "dataset = fever_dataset['train']\n",
    "\n",
    "sample_range = len(dataset)\n",
    "loop = tqdm(range(sample_range))\n",
    "\n",
    "accetable_verbs = ['marry.01', 'premiere.01']\n",
    "\n",
    "relational_graph = dict()\n",
    "old_max_span = 1_000_000\n",
    "\n",
    "for i in loop:\n",
    "\n",
    "  data =  dataset[i]\n",
    "  sample_id = data['id']\n",
    "  premise = data['premise']\n",
    "  hypothesis = data['hypothesis']\n",
    "  # print(f\"premise: {premise}\")\n",
    "  \n",
    "  # srl info\n",
    "  tokens = data['srl']['premise']['tokens']\n",
    "  annotations = data['srl']['premise']['annotations']\n",
    "\n",
    "  # wsd info\n",
    "  wsd = data['wsd']['premise']\n",
    "  proper_nouns = extract_names(wsd)\n",
    "\n",
    "  hp_wsd = data['wsd']['hypothesis']\n",
    "  hp_proper_nouns = extract_names(hp_wsd)\n",
    "\n",
    "  # hp_wsd = data['wsd']['hypothesis']\n",
    "  # hp_proper_nouns = extract_names(hp_wsd)\n",
    "  # print(proper_nouns)\n",
    "\n",
    "  for annotation in annotations:\n",
    "\n",
    "    token_index = annotation['tokenIndex']\n",
    "    verb = tokens[token_index]['rawText']\n",
    "\n",
    "    # frame = annotation['verbatlas']['frameName']\n",
    "    frame = annotation['englishPropbank']['frameName']\n",
    "\n",
    "    if verb[0].isupper(): continue  # because usually capital letter verbs are movie titles\n",
    "    if frame not in accetable_verbs: continue\n",
    "    if frame not in relational_graph: relational_graph[frame] = dict()\n",
    "    # print(premise)\n",
    "\n",
    "    # roles = annotation['verbatlas']['roles']\n",
    "    roles = annotation['englishPropbank']['roles']\n",
    "\n",
    "    try:\n",
    "      span_begin = roles[0]['span'][0]\n",
    "      span_end = roles[-1]['span'][1]\n",
    "\n",
    "    except:\n",
    "      # print(roles)\n",
    "      continue\n",
    "\n",
    "    if span_begin > 0 and span_begin < old_max_span: \n",
    "      span_begin = 0\n",
    "      old_max_span = span_end\n",
    "\n",
    "    sentence = get_sentence_from_span(tokens, span_begin, span_end)\n",
    "\n",
    "\n",
    "    if frame in ['premiere.01'] and (\"premiered in \" in premise or \"premiered at \" in premise or \"premiered on \" in premise):\n",
    "\n",
    "      # from sentence extract movie title\n",
    "  \n",
    "      title = get_movie_title(hypothesis)\n",
    "      if not title: continue\n",
    "\n",
    "      # print(sentence)\n",
    "      span_1_l, span_2_l, span_1_d, span_2_d  = None, None, None, None\n",
    "      for role in roles:\n",
    "        if role['span'][1] > span_end : \n",
    "          # print(sentence)\n",
    "          # print(span_end)\n",
    "          # print(role)\n",
    "          break\n",
    "\n",
    "        if role['role'] in ['ARGM-LOC', 'R-ARGM-LOC', 'C-ARGM-LOC']: \n",
    "          span_1_l = role['span'][0]\n",
    "          span_2_l = role['span'][1]\n",
    "\n",
    "        if role['role'] in ['ARGM-TMP', 'R-ARGM-TMP']: \n",
    "          span_1_d = role['span'][0]\n",
    "          span_2_d = role['span'][1] \n",
    "\n",
    "      # from sentence extrat location and date\n",
    "      if  span_1_l and span_2_l:\n",
    "        location_sentence = get_sentence_from_span(tokens, span_1_l, span_2_l)\n",
    "        locations = extract_locations(location_sentence)\n",
    "      else: \n",
    "        locations = set()\n",
    "      \n",
    "      locations = list(locations)\n",
    "    \n",
    "      if span_1_d and span_2_d:\n",
    "        date_sentence =  get_sentence_from_span(tokens, span_1_d, span_2_d)\n",
    "        date = extract_dates(date_sentence)\n",
    "      else:\n",
    "        date = []\n",
    "        \n",
    "      # add nodes to the graph\n",
    "      if title not in relational_graph[frame]: \n",
    "        relational_graph[frame][title] = dict()\n",
    "        relational_graph[frame][title]['locations'] = locations\n",
    "        relational_graph[frame][title]['date'] = [date]\n",
    "        relational_graph[frame][title]['id'] = [sample_id]\n",
    "\n",
    "      else: \n",
    "        if set(locations).intersection(set(relational_graph[frame][title]['locations'])) == set(): \n",
    "          relational_graph[frame][title]['locations'] += locations\n",
    "\n",
    "        if date not in relational_graph[frame][title]['date']:\n",
    "          relational_graph[frame][title]['date'].append(date)\n",
    "\n",
    "        if sample_id not in relational_graph[frame][title]['id']:\n",
    "          relational_graph[frame][title]['id'].append(sample_id)\n",
    "\n",
    "\n",
    "    if frame in  ['marry.01']: \n",
    "      \n",
    "      # print(sentence)\n",
    "      # print(hypothesis)\n",
    "      main = extract_partial_match_name(hypothesis, hp_proper_nouns)\n",
    "      targets = extract_partial_match_name(sentence, proper_nouns)\n",
    "\n",
    "      if main: main = main[0]\n",
    "      # print(hp_proper_nouns)\n",
    "      # print(main)\n",
    "\n",
    "      if not targets: continue\n",
    "      targets = list(set(targets))\n",
    "\n",
    "      date = []\n",
    "      if \"married in\" in premise:\n",
    "        span_1_d, span_2_d  = None, None\n",
    "        for role in roles:\n",
    "          if role['span'][1] > span_end : break\n",
    "\n",
    "          if role['role'] in ['ARGM-TMP', 'R-ARGM-TMP']: \n",
    "            span_1_d = role['span'][0]\n",
    "            span_2_d = role['span'][1] \n",
    "\n",
    "        if span_1_d and span_2_d:\n",
    "          date_sentence =  get_sentence_from_span(tokens, span_1_d, span_2_d)\n",
    "          date = extract_dates(date_sentence)\n",
    "\n",
    "      if main in targets: \n",
    "        targets.remove(main)\n",
    "\n",
    "      if main not in relational_graph[frame]:\n",
    "         relational_graph[frame][main] = dict()\n",
    "         relational_graph[frame][main]['spouses'] = targets\n",
    "         relational_graph[frame][main]['date'] = [date]\n",
    "         relational_graph[frame][main]['id'] = [sample_id]\n",
    "\n",
    "      else:\n",
    "      \n",
    "        if targets not in relational_graph[frame][main]['spouses']:\n",
    "          relational_graph[frame][main]['date'].append(date)\n",
    "\n",
    "        if date not in relational_graph[frame][main]['date']:\n",
    "          relational_graph[frame][main]['date'].append(date)\n",
    "\n",
    "        if sample_id not in relational_graph[frame][main]['id']:\n",
    "          relational_graph[frame][main]['id'].append(sample_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Save/Load relational graph\n",
    "\n",
    "if save_graph:\n",
    "    with open(f\"{graph}/relational_graph.json\", \"w\") as fp:\n",
    "        json.dump(relational_graph, fp, indent=4)\n",
    "\n",
    "if load_graph:\n",
    "    with open(f\"{graph}/relational_graph.json\", \"r\") as f:\n",
    "        relational_graph = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "premiere.01: \n",
      "    John Wick: Chapter 2: \n",
      "        locations: ['Los Angeles']\n",
      "        date: [['30', 'January', '2017']]\n",
      "        id: ['138117', '48551', '9366', '138368', '61964', '97247', '9367']\n",
      "    Brave: \n",
      "        locations: []\n",
      "        date: [['10', 'June', '2012']]\n",
      "        id: ['209148', '209129', '209162']\n",
      "    Penny Dreadful: \n",
      "        locations: []\n",
      "        date: [['9', None, None], ['11', 'May', '2014']]\n",
      "        id: ['113668', '88185', '145475', '33511', '135923', '40973', '136982', '63647', '53104', '149528', '147369']\n",
      "    Snooki & Jwoww: \n",
      "        locations: []\n",
      "        date: [['22', 'October', '2013'], ['5', 'November', '2014'], ['8', 'January', '2013']]\n",
      "        id: ['183071', '183054', '183052', '183061', '183044', '183068', '183064', '183060']\n",
      "    Sons of Anarchy: \n",
      "        locations: []\n",
      "        date: [['3', 'September', '2008'], ['9', 'September', '2014'], ['9', 'December', '2014']]\n",
      "        id: ['57', '13852', '108672', '73620', '99584', '51560', '58', '61340', '116506', '82239']\n",
      "    The Sopranos: \n",
      "        locations: ['the United States']\n",
      "        date: [['10', 'January', '1999']]\n",
      "        id: ['28225', '24918', '12462', '139320', '116467', '70496', '9562', '96381', '135676', '7280', '57859', '136736', '8314', '134122', '75675']\n",
      "    Thor: The Dark World: \n",
      "        locations: ['London']\n",
      "        date: [['22', 'October', '2013']]\n",
      "        id: ['32278', '82503', '133505']\n",
      "    The Ren & Stimpy Show: \n",
      "        locations: []\n",
      "        date: [['11', None, '1991']]\n",
      "        id: ['121004', '22668', '53149', '143038', '6828', '97476']\n",
      "    Winter Passing: \n",
      "        locations: []\n",
      "        date: [[None, None, '2005']]\n",
      "        id: ['214644', '214654', '214659', '214658']\n",
      "    Teen Wolf: \n",
      "        locations: []\n",
      "        date: [[None, None, '2011'], ['15', 'November', '2016']]\n",
      "        id: ['84340', '30948', '16867', '152049', '23449', '1540', '110049', '34796', '62643', '142833']\n",
      "    Captain America: The Winter Soldier: \n",
      "        locations: ['Los Angeles']\n",
      "        date: [['13', 'March', '2014']]\n",
      "        id: ['217436', '217445', '217439', '217438', '217441', '217447', '217433']\n",
      "    The Belko Experiment: \n",
      "        locations: []\n",
      "        date: [['10', None, '2016']]\n",
      "        id: ['35193', '131146', '147434', '37460', '2094', '133762', '118826', '35444', '75912', '56118', '50953']\n",
      "    Fantastic Beasts and Where to Find Them: \n",
      "        locations: ['New York City']\n",
      "        date: [['10', None, '2016']]\n",
      "        id: ['58443', '34332', '32343', '111488', '32342', '145482', '75338', '2814', '38428', '143197', '73157', '153997', '113084', '36946', '86337', '106356', '100660', '151390', '3741', '60155', '29776', '55524', '109707', '22369', '66007']\n",
      "    A Monster Calls: \n",
      "        locations: []\n",
      "        date: [['10', 'September', '2016']]\n",
      "        id: ['122156', '125241', '97020', '131998', '118782']\n",
      "    Prison Break: \n",
      "        locations: []\n",
      "        date: [['4', 'April', '2017']]\n",
      "        id: ['64233', '83960', '121566', '119476', '81442', '123630', '143795', '157857', '150302', '59999', '109660', '71369', '70324', '153266', '144960', '72534', '26456', '34948']\n",
      "    The Strain: \n",
      "        locations: ['Texas', 'Austin']\n",
      "        date: [['13', 'July', '2014'], ['12', 'July', '2015'], ['28', 'August', '2016'], ['16', 'July', '2017'], [None, 'June', '2014']]\n",
      "        id: ['82642', '198200', '63615', '118187', '183835', '139049', '183845', '198194', '107552', '198198', '183844', '198203', '198195', '183851', '183840']\n",
      "    Tropico: \n",
      "        locations: []\n",
      "        date: [[None, 'December', '2013']]\n",
      "        id: ['6328']\n",
      "    Rescue Me: \n",
      "        locations: []\n",
      "        date: [['21', None, '2004']]\n",
      "        id: ['166131', '166128', '166102', '166126', '166115', '166108', '166124', '166109']\n",
      "    Lipstick Under My Burkha: \n",
      "        locations: ['Tokyo']\n",
      "        date: [[]]\n",
      "        id: ['146363', '96156', '73696', '46032', '34068', '122803']\n",
      "    The Promise: \n",
      "        locations: []\n",
      "        date: [['11', 'September', '2016']]\n",
      "        id: ['73489', '506', '29797', '44071', '95424', '152759', '122630', '142023', '29796', '24746', '74956']\n",
      "    Sleeping Beauty: \n",
      "        locations: ['US']\n",
      "        date: [[None, None, None], ['2', None, '2011']]\n",
      "        id: ['147944', '156382', '36530', '33490', '69977', '66204', '99616']\n",
      "    Mad Men: \n",
      "        locations: []\n",
      "        date: [['19', 'July', '2007']]\n",
      "        id: ['151807', '149499', '8978']\n",
      "    There Will Be Blood: \n",
      "        locations: []\n",
      "        date: [[]]\n",
      "        id: ['87259', '152464']\n",
      "    Avatar: \n",
      "        locations: ['London']\n",
      "        date: [[None, None, '2009']]\n",
      "        id: ['141791', '104519', '59621', '115904', '152457', '90547', '143669', '81504']\n",
      "    The Vampire Diaries: \n",
      "        locations: []\n",
      "        date: [['10', None, '2009']]\n",
      "        id: ['39495', '126593', '11176', '90481', '78538', '21162', '11177', '45684', '125656', '34107', '174805', '48998']\n",
      "    Cloud Atlas: \n",
      "        locations: []\n",
      "        date: [['8', None, '2012']]\n",
      "        id: ['216621', '216633', '216627', '216646', '216622', '216647', '216630', '216631', '216626']\n",
      "    Kong: Skull Island: \n",
      "        locations: ['London']\n",
      "        date: [['28', 'February', '2017']]\n",
      "        id: ['10282']\n",
      "    Rick and Morty: \n",
      "        locations: []\n",
      "        date: [['26', 'July', '2015']]\n",
      "        id: ['2395', '45505', '113031', '53304', '2396', '119451', '18977', '8798']\n",
      "    Ink Master: \n",
      "        locations: ['Spike']\n",
      "        date: [['17', 'January', '2012'], ['23', 'June', '2015'], ['23', None, '2016'], ['23', None, None], ['1', 'March', '2016']]\n",
      "        id: ['87468', '81581', '6795', '6796', '51313', '44746', '1651', '76366', '10227']\n",
      "    Frenemies: \n",
      "        locations: ['Canada', 'the United States']\n",
      "        date: [[], ['13', None, '2012']]\n",
      "        id: ['177580', '177565', '177562', '177577', '177579', '177578', '177544', '177584', '177553']\n",
      "    Room: \n",
      "        locations: []\n",
      "        date: [['4', None, '2015']]\n",
      "        id: ['2569', '2570', '65428', '45931', '66629', '15924', '152585', '23576']\n",
      "    Ballet Shoes: \n",
      "        locations: []\n",
      "        date: [[]]\n",
      "        id: ['205292', '205298', '205294', '205299', '205313', '205323', '205319']\n",
      "    New Girl: \n",
      "        locations: []\n",
      "        date: [['20', 'September', '2011'], ['20', 'September', '2016']]\n",
      "        id: ['188861', '188837', '110251', '97833', '157197', '12595', '188858', '133263']\n",
      "    Grey's Anatomy: \n",
      "        locations: []\n",
      "        date: [['27', 'March', '2005'], ['22', 'September', '2016']]\n",
      "        id: ['72977', '103880', '140993', '90853', '101815', '106340', '3934', '109404']\n",
      "    Horrible Bosses: \n",
      "        locations: ['Los Angeles']\n",
      "        date: [['30', 'June', '2011']]\n",
      "        id: ['166284', '166253']\n",
      "    The Breakfast Club: \n",
      "        locations: ['Los Angeles']\n",
      "        date: [['7', 'February', '1985']]\n",
      "        id: ['89855', '114646', '108412', '103411', '156948', '102275']\n",
      "    Broadchurch: \n",
      "        locations: []\n",
      "        date: [['5', 'January', '2015'], ['4', 'March', '2013'], ['27', 'February', '2017']]\n",
      "        id: ['149653', '128892', '41324', '14958', '105472', '116869']\n",
      "    Modern Family: \n",
      "        locations: []\n",
      "        date: [['23', 'September', '2009'], ['21', 'September', '2016']]\n",
      "        id: ['186960', '77528', '75907', '186968', '108832', '78074', '125475', '94394', '69092', '96295', '123423']\n",
      "    Wild: \n",
      "        locations: []\n",
      "        date: [['29', 'August', '2014']]\n",
      "        id: ['151765', '61474', '8293', '12393']\n",
      "    The Fate of the Furious: \n",
      "        locations: ['Berlin']\n",
      "        date: [['4', None, '2017']]\n",
      "        id: ['220892', '220895', '220906', '220894', '220907', '220891', '220887']\n",
      "    Zootopia: \n",
      "        locations: ['Belgium']\n",
      "        date: [['13', 'February', '2016']]\n",
      "        id: ['38710', '86561', '153943', '102265', '12107', '86320']\n",
      "    Spider-Man: \n",
      "        locations: ['Philippines', 'Tokyo']\n",
      "        date: [['30', 'April', '2002'], ['16', None, '2007']]\n",
      "        id: ['78132', '225075', '78965', '146774', '52001', '18061', '116601', '53900', '15927', '103122', '104933', '58475']\n",
      "    Enemy: \n",
      "        locations: []\n",
      "        date: [['8', 'September', '2013']]\n",
      "        id: ['161583', '161586', '176871', '161563']\n",
      "    The Night Of: \n",
      "        locations: []\n",
      "        date: [['10', None, '2016'], ['24', 'June', '2016']]\n",
      "        id: ['97351', '77049', '9256', '9257', '222872', '139909']\n",
      "    Johnny Mnemonic: \n",
      "        locations: []\n",
      "        date: [['15', 'April', '1995']]\n",
      "        id: ['104028', '128952', '26988', '31859', '48531', '149846', '131917', '8052', '89466', '3158']\n",
      "    Futurama: \n",
      "        locations: []\n",
      "        date: [[None, None, '1999']]\n",
      "        id: ['33396', '71258', '29646']\n",
      "    The Illusionist: \n",
      "        locations: []\n",
      "        date: [[]]\n",
      "        id: ['48932', '125718', '12694', '202390', '74309', '107728', '142158']\n",
      "    Fairy Tail: \n",
      "        locations: ['Tokyo']\n",
      "        date: [[]]\n",
      "        id: ['78819', '45676', '89357', '43745', '64530', '69372']\n",
      "    Black Sails: \n",
      "        locations: []\n",
      "        date: [['24', 'January', '2015'], [], ['29', 'January', '2017']]\n",
      "        id: ['173806', '219729', '219715']\n",
      "    In the Heart of the Sea: \n",
      "        locations: ['New York City']\n",
      "        date: [['7', None, '2015']]\n",
      "        id: ['71758', '99823', '26847', '7069', '124675', '96623', '107483', '19208']\n",
      "    The Shield: \n",
      "        locations: ['the United States']\n",
      "        date: [['12', 'March', '2002']]\n",
      "        id: ['79690', '120556', '129773', '115809', '122731', '67535', '27872', '91102']\n",
      "    The Messenger: \n",
      "        locations: []\n",
      "        date: [[]]\n",
      "        id: ['143914', '75936', '45524']\n",
      "    The Supernatural: \n",
      "        locations: []\n",
      "        date: [['13', 'September', '2005']]\n",
      "        id: ['19849', '135471', '95037', '155154', '148380', '19948']\n",
      "    The Suite Life Movie: \n",
      "        locations: []\n",
      "        date: [['25', None, '2011']]\n",
      "        id: ['91601']\n",
      "    Iron Man: \n",
      "        locations: ['Sydney']\n",
      "        date: [['14', 'April', '2008']]\n",
      "        id: ['22541', '3484']\n",
      "    Oz the Great and Powerful: \n",
      "        locations: []\n",
      "        date: [['14', 'February', '2013']]\n",
      "        id: ['193941', '193925', '193929', '193959', '193921', '193965', '193938', '193930', '193957', '193924', '193946', '193931', '193958']\n",
      "    Attack on Titan: \n",
      "        locations: []\n",
      "        date: [[None, 'April', '2017']]\n",
      "        id: ['45449', '31126', '31127', '111618', '21998', '12623', '118437']\n",
      "    X-Men: Days of Future Past: \n",
      "        locations: ['New York City']\n",
      "        date: [['10', 'May', '2014']]\n",
      "        id: ['115610', '2300']\n",
      "    Legion: \n",
      "        locations: []\n",
      "        date: [['8', 'February', None]]\n",
      "        id: ['229017', '229038', '176793', '229031', '229044', '44873', '229019', '229030', '229045', '141902', '229042', '229028', '229025', '20674']\n",
      "    Beauty and the Beast: \n",
      "        locations: ['London']\n",
      "        date: [['23', 'February', '2017']]\n",
      "        id: ['81659', '17529', '22550']\n",
      "    Famous in Love: \n",
      "        locations: []\n",
      "        date: [['18', 'April', '2017']]\n",
      "        id: ['191605', '191598', '191622', '191600', '191592']\n",
      "    Trolls: \n",
      "        locations: []\n",
      "        date: [['8', 'October', '2016']]\n",
      "        id: ['75163', '209596', '209607']\n",
      "    The Great Buck Howard: \n",
      "        locations: []\n",
      "        date: [['18', 'January', '2008']]\n",
      "        id: ['152766']\n",
      "    Glee: \n",
      "        locations: []\n",
      "        date: [['9', 'January', '2015'], ['19', 'May', '2009']]\n",
      "        id: ['175833', '8489', '16778', '175835', '175828', '175850', '33851', '175832', '175849']\n",
      "    Elementary: \n",
      "        locations: []\n",
      "        date: [['27', 'September', '2012'], []]\n",
      "        id: ['212653', '212635', '212659', '68506']\n",
      "    The Avengers: \n",
      "        locations: ['Hollywood']\n",
      "        date: [['11', 'April', '2012']]\n",
      "        id: ['67316', '103995']\n",
      "    Schindler's List: \n",
      "        locations: ['Washington', 'D.C.']\n",
      "        date: [['30', 'November', '1993']]\n",
      "        id: ['121604', '57450', '17473', '65897', '51721']\n",
      "    The Carmichael Show: \n",
      "        locations: []\n",
      "        date: [['26', 'August', '2015'], ['31', 'May', '2017']]\n",
      "        id: ['221322', '221326', '221276', '221262', '221325']\n",
      "    The Leftovers: \n",
      "        locations: []\n",
      "        date: [['29', 'June', '2014'], ['4', 'October', '2015'], ['16', 'April', '2017']]\n",
      "        id: ['106677', '120444', '152674', '103788']\n",
      "    Short Term 12: \n",
      "        locations: []\n",
      "        date: [['10', None, '2013']]\n",
      "        id: ['201973', '202001', '201974']\n",
      "    Stephanie Daley: \n",
      "        locations: []\n",
      "        date: [[]]\n",
      "        id: ['90198', '88740']\n",
      "    BoJack Horseman: \n",
      "        locations: ['Netflix']\n",
      "        date: [['22', 'August', '2014'], ['19', 'December', None]]\n",
      "        id: ['90326', '23700', '23932', '41371', '8263']\n",
      "    True Detective: \n",
      "        locations: []\n",
      "        date: [['12', 'January', '2014']]\n",
      "        id: ['20419', '95113', '38833']\n",
      "    Doctor Who: \n",
      "        locations: ['Canada', 'Alberta', 'Edmonton']\n",
      "        date: [['12', None, '1996'], ['15', None, '2017']]\n",
      "        id: ['108729', '22198', '76267']\n",
      "    Ghostbusters: \n",
      "        locations: ['Los Angeles']\n",
      "        date: [['9', None, '2016']]\n",
      "        id: ['55257', '23667', '54990', '115596', '128384']\n",
      "    Whiplash: \n",
      "        locations: []\n",
      "        date: [['16', 'January', '2014']]\n",
      "        id: ['28147', '16936', '88818']\n",
      "    Interstellar: \n",
      "        locations: ['Los Angeles']\n",
      "        date: [['26', None, '2014']]\n",
      "        id: ['79685', '117012', '157951']\n",
      "    San Junipero: \n",
      "        locations: ['Netflix']\n",
      "        date: [['21', 'October', '2016']]\n",
      "        id: ['101053', '52683', '21540']\n",
      "    To the Bone: \n",
      "        locations: []\n",
      "        date: [['22', 'January', '2017']]\n",
      "        id: ['199149', '199147', '199145']\n",
      "    How to Be: \n",
      "        locations: []\n",
      "        date: [['18', 'January', '2008']]\n",
      "        id: ['221544']\n",
      "    Major Barbara: \n",
      "        locations: []\n",
      "        date: [[None, None, '1905']]\n",
      "        id: ['161073']\n",
      "    Turn: Washington's Spies: \n",
      "        locations: []\n",
      "        date: [['13', 'April', '2015'], ['25', 'April', '2016']]\n",
      "        id: ['220597', '220592']\n",
      "    American Horror Story: \n",
      "        locations: []\n",
      "        date: [['7', 'October', '2015']]\n",
      "        id: ['39305', '126598', '10744', '19328']\n",
      "    Her: \n",
      "        locations: []\n",
      "        date: [['12', 'October', '2013']]\n",
      "        id: ['142587', '85744']\n",
      "    Little Miss Sunshine: \n",
      "        locations: []\n",
      "        date: [['20', 'January', '2006']]\n",
      "        id: ['21556', '19507', '70461', '32603']\n",
      "    Fargo: \n",
      "        locations: []\n",
      "        date: [['19', 'April', '2017']]\n",
      "        id: ['8528', '74358']\n",
      "    Naruto: \n",
      "        locations: []\n",
      "        date: [['10', 'September', '2005']]\n",
      "        id: ['51407', '31064', '29350', '51406']\n",
      "    Line of Duty: \n",
      "        locations: []\n",
      "        date: [['26', 'June', '2012']]\n",
      "        id: ['41466', '68951', '6552', '101605', '17193', '55751']\n",
      "    Harry Potter: \n",
      "        locations: ['Chicago']\n",
      "        date: [[None, None, '2009'], [None, 'November', '2016']]\n",
      "        id: ['23313']\n",
      "    Interview: \n",
      "        locations: []\n",
      "        date: [[None, None, '2007']]\n",
      "        id: ['136444']\n",
      "    Cars Toons: \n",
      "        locations: []\n",
      "        date: [[]]\n",
      "        id: ['93277']\n",
      "    Miss Peregrine's Home for Peculiar Children: \n",
      "        locations: ['Texas', 'Austin']\n",
      "        date: [['25', None, '2016']]\n",
      "        id: ['218405', '116509', '218401']\n",
      "    Guardians of the Galaxy: \n",
      "        locations: ['Hollywood']\n",
      "        date: [['21', 'July', '2014']]\n",
      "        id: ['168658']\n",
      "    The Girl on the Train: \n",
      "        locations: ['London']\n",
      "        date: [['20', None, '2016']]\n",
      "        id: ['169413']\n",
      "    Goliyon Ki Raasleela Ram-Leela: \n",
      "        locations: []\n",
      "        date: [[None, None, None]]\n",
      "        id: ['207641']\n",
      "    Love & Friendship: \n",
      "        locations: []\n",
      "        date: [[None, 'January', '2016']]\n",
      "        id: ['190883']\n",
      "marry.01: \n",
      "    Wyatt Earp: \n",
      "        spouses: ['Earp', 'Urilla Sutherland Earp']\n",
      "        date: [[], [], [], [], [], [], [], []]\n",
      "        id: ['30166', '124003', '19664', '140605', '39026', '18407', '140570', '114675']\n",
      "    Paul McCartney: \n",
      "        spouses: []\n",
      "        date: [[], [], [], [], [], [], [], []]\n",
      "        id: ['52015', '149441', '7983', '135818', '21689', '157227', '47751', '62218']\n",
      "    Linda McCartney: \n",
      "        spouses: ['Paul McCartney', 'Linda Louise', 'April', 'Lady McCartney', 'Beatles', 'September']\n",
      "        date: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "        id: ['117931', '212352', '32317', '139088', '71073', '9274', '67951', '78468', '9275', '87083', '112671', '212351']\n",
      "    Jennifer Garner: \n",
      "        spouses: ['Garner', 'Scott Foley']\n",
      "        date: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "        id: ['229394', '4974', '71362', '61558', '229389', '229391', '229401', '229396', '229397', '4975', '133008', '57188']\n",
      "    Ellen Pompeo: \n",
      "        spouses: ['Pompeo', 'Chris Ivery']\n",
      "        date: [[], [], [], [], [], []]\n",
      "        id: ['89086', '114846', '137940', '61666', '83319', '118335']\n",
      "    Harald V: \n",
      "        spouses: ['Harald', 'Norway', 'Sonja Haraldsen']\n",
      "        date: [[], [], [], [], [], [], [], [], []]\n",
      "        id: ['54024', '93102', '128193', '122388', '59710', '92600', '67479', '20773', '59655']\n",
      "    Empress Matilda: \n",
      "        spouses: ['King Henry', 'England', 'Germany', 'Holy Roman Emperor Henry V']\n",
      "        date: [[], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "        id: ['93936', '136915', '5479', '149116', '45782', '83945', '115000', '24754', '8287', '71897', '3549', '100761', '76648', '126945']\n",
      "    Ann Romney: \n",
      "        spouses: ['Brigham Young University', 'BYU', 'Mitt Romney']\n",
      "        date: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "        id: ['180302', '180275', '180295', '180270', '204148', '180272', '180293', '180265', '180279', '204131', '204140', '204155', '204138', '180262', '180300', '180269', '180281', '180278']\n",
      "    Ellen DeGeneres: \n",
      "        spouses: ['Portia de Rossi']\n",
      "        date: [[], [], [], [], [], [], [], [], []]\n",
      "        id: ['36216', '8020', '102972', '128929', '13074', '36215', '123007', '133370']\n",
      "    Jennifer Aniston: \n",
      "        spouses: ['Brad Pitt']\n",
      "        date: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "        id: ['14294', '14295', '122621', '134684', '121989', '110870', '82957']\n",
      "    Turner: \n",
      "        spouses: ['Elizabeth Swann', 'World']\n",
      "        date: [[], [], []]\n",
      "        id: ['185318']\n",
      "    April: \n",
      "        spouses: ['Paul McCartney', 'Beatles']\n",
      "        date: [[]]\n",
      "        id: ['149917']\n",
      "    Joan Crawford: \n",
      "        spouses: ['Crawford']\n",
      "        date: [[], [], [], []]\n",
      "        id: ['122550', '82151', '92957', '50440']\n",
      "    George VI: \n",
      "        spouses: ['Lyon', 'Lady Elizabeth Bowes', 'Elizabeth']\n",
      "        date: [[], [], [], [], [], []]\n",
      "        id: ['5910', '50142', '63873', '8554', '52828', '101696']\n",
      "    Elizabeth II: \n",
      "        spouses: ['Greece', 'Philip', 'Denmark', 'Prince', 'Wales', 'Duke', 'Charles', 'Edinburgh']\n",
      "        date: [[]]\n",
      "        id: ['24301']\n",
      "    Christie Brinkley: \n",
      "        spouses: ['Billy Joel', 'Brinkley']\n",
      "        date: [[], [], [], []]\n",
      "        id: ['202971', '202963', '202974', '202977']\n",
      "    Sharon Tate: \n",
      "        spouses: ['Sharon Marie Tate Polanski', 'Fearless Vampire Killers', 'Tate', 'August', 'Roman Polanski', 'January']\n",
      "        date: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "        id: ['34060', '122616', '56225', '34903', '84904', '106014', '92075', '62326', '50485', '30838', '109761', '145794', '29467', '27908', '111283']\n",
      "    Michelle Obama: \n",
      "        spouses: ['Michelle', 'Barack']\n",
      "        date: [[None, None, '1992'], [None, None, '1992'], [None, None, '1992']]\n",
      "        id: ['93454', '56474', '63241']\n",
      "    Katie Price: \n",
      "        spouses: ['Kieran Hayler', 'Alex Reid', 'Peter Andre']\n",
      "        date: [[], [], [], [], [], [], [], [], []]\n",
      "        id: ['39567', '104859', '19398', '38159', '65122', '50018', '4508', '90892', '93240']\n",
      "    Charles: \n",
      "        spouses: ['Bourbon', 'France', 'England', 'Henrietta Maria']\n",
      "        date: [[], [], [], [], [], [], [], [], []]\n",
      "        id: ['84162', '80448', '43793', '7073', '43114', '208490', '208487']\n",
      "    Faith Evans: \n",
      "        spouses: ['Christopher', 'Wallace', 'New York', 'August', 'Bad Boy']\n",
      "        date: [[], [], [], [], []]\n",
      "        id: ['126634', '130700', '102683', '44827', '107379']\n",
      "    Mel Brooks: \n",
      "        spouses: ['Oscar', 'Brooks', 'Anne Bancroft']\n",
      "        date: [[]]\n",
      "        id: ['97998']\n",
      "    Jacqueline Kennedy Onassis: \n",
      "        spouses: ['Aristotle Onassis']\n",
      "        date: [[None, None, '1968']]\n",
      "        id: ['4276']\n",
      "    Bruce Springsteen: \n",
      "        spouses: ['Patti Scialfa']\n",
      "        date: [[], [], [], [], [], []]\n",
      "        id: ['91266', '136247', '150581', '82667', '60871', '3364']\n",
      "    Triple H: \n",
      "        spouses: ['H']\n",
      "        date: [[], []]\n",
      "        id: ['185480']\n",
      "    Kris Jenner: \n",
      "        spouses: ['Kardashian', 'Caitlyn', 'Jenner', 'Robert', 'Olympic Games', 'Bruce Jenner', 'Robert Kardashian']\n",
      "        date: [[]]\n",
      "        id: ['135196']\n",
      "    Mamie Doud: \n",
      "        spouses: ['Mamie', 'West Point']\n",
      "        date: [[], [], []]\n",
      "        id: ['88306', '153695']\n",
      "    Elizabeth: \n",
      "        spouses: []\n",
      "        date: [[], []]\n",
      "        id: ['19758', '89046']\n",
      "    Grace Kelly: \n",
      "        spouses: ['Monaco', 'April', 'Prince Rainier III', 'Princess', 'November', 'Patricia Kelly']\n",
      "        date: [[], [], [], []]\n",
      "        id: ['94120', '122062', '49444']\n",
      "    null: \n",
      "        spouses: ['Ann Davies']\n",
      "        date: [[], [], []]\n",
      "        id: ['209662', '140833', '16799']\n",
      "    Franklin Roosevelt: \n",
      "        spouses: ['Eleanor Roosevelt']\n",
      "        date: [[], [], [], []]\n",
      "        id: ['151172', '69891', '2168', '70851']\n",
      "    Pamela Anderson: \n",
      "        spouses: ['Anderson', 'Pamela Anderson Lee', 'Pamela Lee', 'Tommy Lee']\n",
      "        date: [[]]\n",
      "        id: ['66831']\n",
      "    Margaret: \n",
      "        spouses: ['Navarre', 'King Henry III', 'Valois', 'Charles IX']\n",
      "        date: [[], [], [], []]\n",
      "        id: ['220330', '128032', '220340', '220333']\n",
      "    William Shakespeare: \n",
      "        spouses: ['Judith', 'Susanna', 'Hamnet', 'Anne Hathaway']\n",
      "        date: [[]]\n",
      "        id: ['216889']\n",
      "    Tom Brady: \n",
      "        spouses: ['Patriots', 'New England Patriots']\n",
      "        date: [[], [], []]\n",
      "        id: ['67834']\n",
      "    Wonder Woman: \n",
      "        spouses: ['South America', 'Diana Prince', 'Army']\n",
      "        date: [[], [], [], [], [], []]\n",
      "        id: ['92192', '38963']\n",
      "    Keturah: \n",
      "        spouses: ['Genesis', 'Abraham', 'Sarah', 'Book']\n",
      "        date: [[], [], []]\n",
      "        id: ['79530', '32998']\n",
      "    November: \n",
      "        spouses: ['Mamie Doud', 'West Point']\n",
      "        date: [[], []]\n",
      "        id: ['82144', '16586']\n",
      "    Lady Elizabeth Bowes: \n",
      "        spouses: ['Lyon', 'George VI', 'Elizabeth']\n",
      "        date: [[]]\n",
      "        id: ['140009']\n",
      "    Barbara Bush: \n",
      "        spouses: ['World War II', 'New York', 'Rye']\n",
      "        date: [[None, None, None], [None, None, None]]\n",
      "        id: ['163489', '163492']\n",
      "    Mitt Romney: \n",
      "        spouses: ['Ann Davies']\n",
      "        date: [[]]\n",
      "        id: ['209660']\n",
      "    Elton John: \n",
      "        spouses: ['John', 'Furnish', 'December', 'Wales', 'David Furnish', 'England']\n",
      "        date: [[]]\n",
      "        id: ['75462']\n",
      "    Cindy McCain: \n",
      "        spouses: ['United States Senator', 'Arizona', 'John McCain', 'May', 'Cindy Lou Hensley McCain']\n",
      "        date: [[], [], [], []]\n",
      "        id: ['228811', '141994', '33810', '100209']\n",
      "    Marilyn Monroe: \n",
      "        spouses: ['Monroe', 'Los Angeles']\n",
      "        date: [[]]\n",
      "        id: ['77750']\n",
      "    Andrew Jackson: \n",
      "        spouses: ['Rachel Donelson Robards']\n",
      "        date: [[]]\n",
      "        id: ['94031']\n",
      "    Kim Kardashian: \n",
      "        spouses: ['Kanye West']\n",
      "        date: [[None, None, '2014'], [None, None, '2014']]\n",
      "        id: ['15761', '125569']\n",
      "    Natalie Wood: \n",
      "        spouses: ['Robert Wagner']\n",
      "        date: [[]]\n",
      "        id: ['57015']\n",
      "    Aishwarya Rai: \n",
      "        spouses: ['Miss World', 'November', 'Rai', 'Aishwarya Rai Bachchan', 'Abhishek Bachchan']\n",
      "        date: [[], [], []]\n",
      "        id: ['64026', '153071', '35734']\n",
      "    Lisa Marie Presley: \n",
      "        spouses: ['Presley', 'Michael Jackson', 'Nicolas Cage', 'Michael Lockwood']\n",
      "        date: [[], [], [], []]\n",
      "        id: ['3099']\n",
      "    September: \n",
      "        spouses: ['Paul McCartney', 'Beatles']\n",
      "        date: [[]]\n",
      "        id: ['9087']\n",
      "    Bruce Willis: \n",
      "        spouses: ['Demi Moore']\n",
      "        date: [[]]\n",
      "        id: ['129188']\n",
      "    Brad Pitt: \n",
      "        spouses: ['Jennifer Aniston']\n",
      "        date: [[]]\n",
      "        id: ['17198']\n",
      "    Queen Victoria: \n",
      "        spouses: ['Europe']\n",
      "        date: [[]]\n",
      "        id: ['142919']\n",
      "    Edward VIII: \n",
      "        spouses: ['Wallis']\n",
      "        date: [[]]\n",
      "        id: ['93524']\n",
      "    Chris Pérez: \n",
      "        spouses: ['April', 'Selena']\n",
      "        date: [[], []]\n",
      "        id: ['189682']\n",
      "    Kristen Bell: \n",
      "        spouses: ['Dax Shepard']\n",
      "        date: [[]]\n",
      "        id: ['156136']\n",
      "    Felicity Jones: \n",
      "        spouses: ['Stephen', 'Infinity']\n",
      "        date: [[]]\n",
      "        id: ['172784']\n",
      "    United States Naval Academy: \n",
      "        spouses: ['Mamie Doud', 'West Point']\n",
      "        date: [[]]\n",
      "        id: ['128777']\n",
      "    Cleopatra: \n",
      "        spouses: ['Ptolemy XIV', 'Ptolemy XIII']\n",
      "        date: [[]]\n",
      "        id: ['35321']\n",
      "    David Beckham: \n",
      "        spouses: ['Victoria Beckham']\n",
      "        date: [[]]\n",
      "        id: ['60776']\n"
     ]
    }
   ],
   "source": [
    "pretty_print_dict(relational_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contraddictions 88\n",
      "----------------------\n",
      "John Wick: Chapter 2 premiered in New York City on January 30, 2022.\n",
      "Brave premiered on June 10, 2015.\n",
      "Penny Dreadful premiered on May 11, 2012.\n",
      "Snooki & Jwoww premiered on October 22, 2008.\n",
      "Sons of Anarchy premiered on September 3, 2013.\n",
      "The Sopranos premiered in the United States on January 10, 2003.\n",
      "Thor: The Dark World premiered in Hollywood on October 22, 2008.\n",
      "The Ren & Stimpy Show premiered on 1989.\n",
      "Winter Passing premiered on 2000.\n",
      "Teen Wolf premiered on November 15, 2018.\n",
      "Captain America: The Winter Soldier premiered in Los Angeles on March 13, 2009.\n",
      "The Belko Experiment premiered on 2019.\n",
      "Fantastic Beasts and Where to Find Them premiered in New York City on 2013.\n",
      "A Monster Calls premiered on September 10, 2013.\n",
      "Prison Break premiered on April 4, 2013.\n",
      "The Strain premiered in Philippines on July 13, 2016.\n",
      "Tropico premiered on December, 2009.\n",
      "Rescue Me premiered on 2003.\n",
      "Lipstick Under My Burkha premiered in Berlin.\n",
      "The Promise premiered on September 11, 2021.\n",
      "Sleeping Beauty premiered in Philippines on 2007.\n",
      "Mad Men premiered on July 19, 2006.\n",
      "Avatar premiered in New York City on 2005.\n",
      "The Vampire Diaries premiered on 2013.\n",
      "Cloud Atlas premiered on 2015.\n",
      "Kong: Skull Island premiered in Berlin on February 28, 2012.\n",
      "Rick and Morty premiered on July 26, 2010.\n",
      "Ink Master premiered in Spike on January 17, 2014.\n",
      "Frenemies premiered in US on 2015.\n",
      "Room premiered on 2018.\n",
      "New Girl premiered on September 20, 2007.\n",
      "Grey's Anatomy premiered on March 27, 2000.\n",
      "Horrible Bosses premiered in Texas on June 30, 2015.\n",
      "The Breakfast Club premiered in Washington on February 7, 1989.\n",
      "Broadchurch premiered on January 5, 2013.\n",
      "Modern Family premiered on September 23, 2008.\n",
      "Wild premiered on August 29, 2019.\n",
      "The Fate of the Furious premiered in Alberta on 2012.\n",
      "Zootopia premiered in Los Angeles on February 13, 2014.\n",
      "Spider-Man premiered in Canada on April 30, 1999.\n",
      "Enemy premiered on September 8, 2016.\n",
      "The Night Of premiered on June 24, 2020.\n",
      "Johnny Mnemonic premiered on April 15, 1992.\n",
      "Futurama premiered on 1994.\n",
      "Fairy Tail premiered in Berlin.\n",
      "Black Sails premiered on January 24, 2020.\n",
      "In the Heart of the Sea premiered in Edmonton on 2013.\n",
      "The Shield premiered in the United States on March 12, 2004.\n",
      "The Supernatural premiered on September 13, 2006.\n",
      "The Suite Life Movie premiered on 2009.\n",
      "Iron Man premiered in Sydney on April 14, 2003.\n",
      "Oz the Great and Powerful premiered on February 14, 2011.\n",
      "Attack on Titan premiered on April, 2013.\n",
      "X-Men: Days of Future Past premiered in Sydney on May 10, 2010.\n",
      "Beauty and the Beast premiered in London on February 23, 2014.\n",
      "Famous in Love premiered on April 18, 2019.\n",
      "Trolls premiered on October 8, 2012.\n",
      "The Great Buck Howard premiered on January 18, 2003.\n",
      "Glee premiered on January 9, 2013.\n",
      "Elementary premiered on September 27, 2008.\n",
      "The Avengers premiered in Austin on April 11, 2015.\n",
      "Schindler's List premiered in Washington on November 30, 1998.\n",
      "The Carmichael Show premiered on August 26, 2016.\n",
      "The Leftovers premiered on June 29, 2016.\n",
      "Short Term 12 premiered on 2015.\n",
      "BoJack Horseman premiered in Texas on August 22, 2014.\n",
      "True Detective premiered on January 12, 2010.\n",
      "Doctor Who premiered in Alberta on 2022.\n",
      "Ghostbusters premiered in Chicago on 2017.\n",
      "Whiplash premiered on January 16, 2013.\n",
      "Interstellar premiered in Los Angeles on 2017.\n",
      "San Junipero premiered in the United States on October 21, 2013.\n",
      "To the Bone premiered on January 22, 2019.\n",
      "How to Be premiered on January 18, 2003.\n",
      "Major Barbara premiered on 1904.\n",
      "Turn: Washington's Spies premiered on April 13, 2016.\n",
      "American Horror Story premiered on October 7, 2010.\n",
      "Her premiered on October 12, 2010.\n",
      "Little Miss Sunshine premiered on January 20, 2004.\n",
      "Fargo premiered on April 19, 2019.\n",
      "Naruto premiered on September 10, 2002.\n",
      "Line of Duty premiered on June 26, 2009.\n",
      "Harry Potter premiered in Hollywood on November, 2013.\n",
      "Interview premiered on 2008.\n",
      "Miss Peregrine's Home for Peculiar Children premiered in Canada on 2015.\n",
      "Guardians of the Galaxy premiered in Edmonton on July 21, 2011.\n",
      "The Girl on the Train premiered in Edmonton on 2018.\n",
      "Love & Friendship premiered on January, 2017.\n",
      "\n",
      "Neutrals 88\n",
      "----------------------\n",
      "John Wick: Chapter 2 was also released at Hollywood on January 30, 2019\n",
      "Brave was also released at Belgium on June 10, 2013\n",
      "Penny Dreadful was also released at Alberta on May 11, 2015\n",
      "Snooki & Jwoww was also released at Chicago on October 22, 2015\n",
      "Sons of Anarchy was also released at Philippines on September 3, 2009\n",
      "The Sopranos was also released at Alberta on January 10, 2000\n",
      "Thor: The Dark World was also released at D.C. on October 22, 2015\n",
      "The Ren & Stimpy Show was also released at Alberta on 1992\n",
      "Winter Passing was also released at Tokyo on 2007\n",
      "Teen Wolf was also released at Tokyo on November 15, 2018\n",
      "Captain America: The Winter Soldier was also released at New York City on March 13, 2016\n",
      "The Belko Experiment was also released at Austin on 2017\n",
      "Fantastic Beasts and Where to Find Them was also released at Belgium on 2017\n",
      "A Monster Calls was also released at London on September 10, 2018\n",
      "Prison Break was also released at Edmonton on April 4, 2019\n",
      "The Strain was also released at Edmonton on July 13, 2015\n",
      "Tropico was also released at Chicago on December, 2014\n",
      "Rescue Me was also released at Sydney on 2005\n",
      "Lipstick Under My Burkha was also released at Alberta\n",
      "The Promise was also released at Chicago on September 11, 2017\n",
      "Sleeping Beauty was also released at the United States on 2012\n",
      "Mad Men was also released at Belgium on July 19, 2008\n",
      "Avatar was also released at Canada on 2010\n",
      "The Vampire Diaries was also released at Los Angeles on 2011\n",
      "Cloud Atlas was also released at the United States on 2013\n",
      "Kong: Skull Island was also released at Edmonton on February 28, 2018\n",
      "Rick and Morty was also released at Canada on July 26, 2016\n",
      "Ink Master was also released at Hollywood on January 17, 2013\n",
      "Frenemies was also released at Belgium on 2014\n",
      "Room was also released at D.C. on 2016\n",
      "New Girl was also released at Los Angeles on September 20, 2013\n",
      "Grey's Anatomy was also released at Alberta on March 27, 2007\n",
      "Horrible Bosses was also released at Hollywood on June 30, 2013\n",
      "The Breakfast Club was also released at Alberta on February 7, 1986\n",
      "Broadchurch was also released at New York City on January 5, 2016\n",
      "Modern Family was also released at Edmonton on September 23, 2011\n",
      "Wild was also released at Los Angeles on August 29, 2016\n",
      "The Fate of the Furious was also released at Texas on 2018\n",
      "Zootopia was also released at Austin on February 13, 2017\n",
      "Spider-Man was also released at Berlin on April 30, 2004\n",
      "Enemy was also released at London on September 8, 2014\n",
      "The Night Of was also released at the United States on June 24, 2018\n",
      "Johnny Mnemonic was also released at Philippines on April 15, 1997\n",
      "Futurama was also released at Texas on 2001\n",
      "Fairy Tail was also released at Chicago\n",
      "Black Sails was also released at Hollywood on January 24, 2017\n",
      "In the Heart of the Sea was also released at Belgium on 2016\n",
      "The Shield was also released at Sydney on March 12, 2003\n",
      "The Supernatural was also released at Edmonton on September 13, 2007\n",
      "The Suite Life Movie was also released at Canada on 2013\n",
      "Iron Man was also released at the United States on April 14, 2009\n",
      "Oz the Great and Powerful was also released at Alberta on February 14, 2014\n",
      "Attack on Titan was also released at Berlin on April, 2018\n",
      "X-Men: Days of Future Past was also released at Edmonton on May 10, 2015\n",
      "Beauty and the Beast was also released at Texas on February 23, 2018\n",
      "Famous in Love was also released at London on April 18, 2018\n",
      "Trolls was also released at Hollywood on October 8, 2017\n",
      "The Great Buck Howard was also released at Hollywood on January 18, 2009\n",
      "Glee was also released at Chicago on January 9, 2016\n",
      "Elementary was also released at Chicago on September 27, 2013\n",
      "The Avengers was also released at London on April 11, 2014\n",
      "Schindler's List was also released at Canada on November 30, 1995\n",
      "The Carmichael Show was also released at New York City on August 26, 2016\n",
      "The Leftovers was also released at Tokyo on June 29, 2016\n",
      "Short Term 12 was also released at US on 2015\n",
      "BoJack Horseman was also released at US on August 22, 2015\n",
      "True Detective was also released at US on January 12, 2016\n",
      "Doctor Who was also released at Washington on 2019\n",
      "Ghostbusters was also released at Chicago on 2017\n",
      "Whiplash was also released at Austin on January 16, 2015\n",
      "Interstellar was also released at Chicago on 2016\n",
      "San Junipero was also released at Sydney on October 21, 2017\n",
      "To the Bone was also released at the United States on January 22, 2019\n",
      "How to Be was also released at Alberta on January 18, 2009\n",
      "Major Barbara was also released at New York City on 1906\n",
      "Turn: Washington's Spies was also released at Texas on April 13, 2017\n",
      "American Horror Story was also released at Berlin on October 7, 2016\n",
      "Her was also released at Canada on October 12, 2015\n",
      "Little Miss Sunshine was also released at New York City on January 20, 2008\n",
      "Fargo was also released at Berlin on April 19, 2018\n",
      "Naruto was also released at US on September 10, 2006\n",
      "Line of Duty was also released at Philippines on June 26, 2014\n",
      "Harry Potter was also released at Washington on November, 2017\n",
      "Interview was also released at D.C. on 2008\n",
      "Miss Peregrine's Home for Peculiar Children was also released at D.C. on 2017\n",
      "Guardians of the Galaxy was also released at Edmonton on July 21, 2016\n",
      "The Girl on the Train was also released at Tokyo on 2017\n",
      "Love & Friendship was also released at Austin on January, 2017\n"
     ]
    }
   ],
   "source": [
    "#@title Relational graph augmentation\n",
    "\n",
    "# dataset = filtered_dataset\n",
    "dataset = fever_dataset['train']\n",
    "new_hypotesys = []\n",
    "\n",
    "curr_id = max_id\n",
    "\n",
    "all_movie_locations = get_all_locations(relational_graph, \"premiere.01\")\n",
    "# print(f\"All movies locations: {all_movie_locations}\")\n",
    "all_movie_dates = get_all_dates(relational_graph, \"premiere.01\")\n",
    "# print(f\"All movies dates: {all_movie_dates}\")\n",
    "\n",
    "\n",
    "for frame in relational_graph.keys():\n",
    "\n",
    "  if frame == \"premiere.01\":\n",
    "\n",
    "    # synset = get_synset_from_id('1718331v')\n",
    "    contraddictions = []\n",
    "    neutrals = []\n",
    "\n",
    "    # go over all the titles in the graph\n",
    "    for title, infos in relational_graph[frame].items():\n",
    "      locations = relational_graph[frame][title]['locations']\n",
    "      dates =  relational_graph[frame][title]['date']\n",
    "      ids =  relational_graph[frame][title]['id']\n",
    "\n",
    "      if locations: select_location = locations[random.randint(0, len(locations)-1)]\n",
    "      # print(dates)\n",
    "      date = format_date(dates)\n",
    "      select_date = date\n",
    "\n",
    "      contraddiction = title + \" \"\n",
    "      if len(locations) >= 1 and date:\n",
    "        ... # choose randomly what to change\n",
    "        # print(title)\n",
    "\n",
    "        if random.randint(1, 100) <= 33: \n",
    "          # change date by randomly adding or subtracting 1 to 10 years\n",
    "          sign = 1 if random.randint(1, 100) <= 50 else -1\n",
    "          new_date = date[:-4] + str(int(date[-4:]) + (sign * random.randint(1, 5)))\n",
    "          # print(date)\n",
    "          # print(new_date)\n",
    "          select_date = new_date\n",
    "\n",
    "        elif 33 <= random.randint(1, 100) <= 66:\n",
    "          # change location\n",
    "          new_location = get_random_exclusive_element(all_movie_locations, locations)\n",
    "          # print(f\"curr locations: {locations}\")\n",
    "          # print(f\"new location = {new_location}\\n\")\n",
    "          select_location = new_location\n",
    "\n",
    "        else:\n",
    "          # change both\n",
    "          sign = 1 if random.randint(1, 100) <= 50 else -1\n",
    "          select_date = date[:-4] + str(int(date[-4:]) + (sign * random.randint(1, 5)))\n",
    "          select_location = get_random_exclusive_element(all_movie_locations, locations)\n",
    "\n",
    "        contraddiction += \"premiered in \" + select_location + \" on \" + select_date + \".\"\n",
    "\n",
    "\n",
    "      elif len(locations) >= 1 and not date:\n",
    "        ... # change only location\n",
    "        new_location = get_random_exclusive_element(all_movie_locations, locations)\n",
    "        contraddiction += \"premiered in \" + new_location + \".\"\n",
    "\n",
    "      elif len(locations) <= 1 and date:\n",
    "        ... # change only date\n",
    "        sign = 1 if random.randint(1, 100) <= 50 else -1\n",
    "        new_date = date[:-4] + str(int(date[-4:]) + (sign * random.randint(1, 5)))\n",
    "        contraddiction += \"premiered on \" + new_date + \".\"\n",
    "\n",
    "      else:\n",
    "        ... # nothing to do \n",
    "        continue\n",
    "\n",
    "      if contraddiction: contraddictions.append(contraddiction)\n",
    "\n",
    "      neutral = \"\"\n",
    "      sign = 1 if random.randint(1, 100) <= 50 else -1\n",
    "      new_date = None\n",
    "      if date: new_date = date[:-4] + str(int(date[-4:]) + random.randint(1, 2))\n",
    "      if not locations: locations = ['unk']\n",
    "\n",
    "      new_location = get_random_exclusive_element(all_movie_locations, locations)\n",
    "\n",
    "      if new_date and new_location:\n",
    "        neutral = title + \" was also released at \" + new_location + \" on \" + new_date\n",
    "      else:\n",
    "        neutral = title + \" was also released at \" + new_location\n",
    "\n",
    "      if neutral: neutrals.append(neutral)\n",
    "\n",
    "print(f\"\\nContraddictions {len(contraddictions)}\")\n",
    "print(\"----------------------\")\n",
    "for sentence in contraddictions:\n",
    "  print(sentence)\n",
    "\n",
    "print(f\"\\nNeutrals {len(neutrals)}\")\n",
    "print(\"----------------------\")\n",
    "for sentence in neutrals:\n",
    "  print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spouses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = filtered_dataset\n",
    "dataset = fever_dataset['train']\n",
    "new_hypotesys = []\n",
    "\n",
    "curr_id = max_id\n",
    "\n",
    "# print(f\"All movies locations: {all_movie_locations}\")\n",
    "all_movie_dates = get_all_dates(relational_graph, \"marry.01\")\n",
    "# print(f\"All movies dates: {all_movie_dates}\")\n",
    "\n",
    "for frame in relational_graph.keys():\n",
    "\n",
    "  if frame == \"marry.01\":\n",
    "\n",
    "    # synset = get_synset_from_id('1718331v')\n",
    "    contraddictions = []\n",
    "    neutrals = []\n",
    "\n",
    "    # go over all the titles in the graph\n",
    "    for title, infos in relational_graph[frame].items():\n",
    "      spouses = relational_graph[frame][title]['spouses']\n",
    "      dates =  relational_graph[frame][title]['date']\n",
    "      ids =  relational_graph[frame][title]['id']\n",
    "\n",
    "      if locations: select_location = locations[random.randint(0, len(locations)-1)]\n",
    "      # print(dates)\n",
    "      date = format_date(dates)\n",
    "      select_date = date\n",
    "\n",
    "      contraddiction = title + \" \"\n",
    "      if len(locations) >= 1 and date:\n",
    "        ... # choose randomly what to change\n",
    "        # print(title)\n",
    "\n",
    "        if random.randint(1, 100) <= 33: \n",
    "          # change date by randomly adding or subtracting 1 to 10 years\n",
    "          sign = 1 if random.randint(1, 100) <= 50 else -1\n",
    "          new_date = date[:-4] + str(int(date[-4:]) + (sign * random.randint(1, 5)))\n",
    "          # print(date)\n",
    "          # print(new_date)\n",
    "          select_date = new_date\n",
    "\n",
    "        elif 33 <= random.randint(1, 100) <= 66:\n",
    "          # change location\n",
    "          new_location = get_random_exclusive_element(all_movie_locations, locations)\n",
    "          # print(f\"curr locations: {locations}\")\n",
    "          # print(f\"new location = {new_location}\\n\")\n",
    "          select_location = new_location\n",
    "\n",
    "        else:\n",
    "          # change both\n",
    "          sign = 1 if random.randint(1, 100) <= 50 else -1\n",
    "          select_date = date[:-4] + str(int(date[-4:]) + (sign * random.randint(1, 5)))\n",
    "          select_location = get_random_exclusive_element(all_movie_locations, locations)\n",
    "\n",
    "        contraddiction += \"premiered in \" + select_location + \" on \" + select_date + \".\"\n",
    "\n",
    "\n",
    "      elif len(locations) >= 1 and not date:\n",
    "        ... # change only location\n",
    "        new_location = get_random_exclusive_element(all_movie_locations, locations)\n",
    "        contraddiction += \"premiered in \" + new_location + \".\"\n",
    "\n",
    "      elif len(locations) <= 1 and date:\n",
    "        ... # change only date\n",
    "        sign = 1 if random.randint(1, 100) <= 50 else -1\n",
    "        new_date = date[:-4] + str(int(date[-4:]) + (sign * random.randint(1, 5)))\n",
    "        contraddiction += \"premiered on \" + new_date + \".\"\n",
    "\n",
    "      else:\n",
    "        ... # nothing to do \n",
    "        continue\n",
    "\n",
    "      if contraddiction: contraddictions.append(contraddiction)\n",
    "\n",
    "      neutral = \"\"\n",
    "      sign = 1 if random.randint(1, 100) <= 50 else -1\n",
    "      new_date = None\n",
    "      if date: new_date = date[:-4] + str(int(date[-4:]) + random.randint(1, 2))\n",
    "      if not locations: locations = ['unk']\n",
    "\n",
    "      new_location = get_random_exclusive_element(all_movie_locations, locations)\n",
    "\n",
    "      if new_date and new_location:\n",
    "        neutral = title + \" was also released at \" + new_location + \" on \" + new_date\n",
    "      else:\n",
    "        neutral = title + \" was also released at \" + new_location\n",
    "\n",
    "      if neutral: neutrals.append(neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 308.51it/s]\n"
     ]
    }
   ],
   "source": [
    "#@title Augmentation by Synonyms and Hypernyms\n",
    "\n",
    "if syn_hyp_augment:\n",
    "  \n",
    "  sample_range = 10 #len(fever_dataset['train'])\n",
    "  loop = tqdm(range(sample_range))\n",
    "\n",
    "  new_samples = {\n",
    "      'id': [],\n",
    "      'premise': [],\n",
    "      'hypothesis': [],\n",
    "      'label': [],\n",
    "      'wsd': [None for i in range(sample_range)],\n",
    "      'srl': [None for i in range(sample_range)]\n",
    "  }\n",
    "\n",
    "  syn_dict = dict()\n",
    "  syn_idx = 0\n",
    "  progressive_id = int(max_id)\n",
    "\n",
    "  for i in loop:\n",
    "\n",
    "    data =  fever_dataset['train'][i]\n",
    "    sample_id = data['id']\n",
    "\n",
    "    premise = data['premise']\n",
    "    hypothesis = data['hypothesis']\n",
    "    label = data['label']\n",
    "    hyp_wsd = data['wsd']['hypothesis']\n",
    "\n",
    "    new_samples['id'].append(str(progressive_id))\n",
    "    new_samples['premise'].append(premise)\n",
    "    new_samples['label'].append(label)\n",
    "    progressive_id += 1\n",
    "\n",
    "    hypothesis = data['hypothesis']\n",
    "    new_hypotesys = []\n",
    "\n",
    "    for hyp_wsd_dict in hyp_wsd:\n",
    "\n",
    "      # substitute word with 50% probability\n",
    "      skip = False\n",
    "      if random.randint(1, 100) <= 50: \n",
    "        skip = True\n",
    "\n",
    "      word = hyp_wsd_dict['text']\n",
    "      pos = hyp_wsd_dict['pos']\n",
    "      offset = hyp_wsd_dict['wnSynsetOffset']\n",
    "      synset = get_synset_from_id(offset)\n",
    "\n",
    "      related_words = None\n",
    "      if synset: related_words = get_related_word(synset, pos)\n",
    "\n",
    "      if related_words and not skip:\n",
    "        hypernyms = related_words['hypernyms']\n",
    "        synonyms = related_words['synonyms']\n",
    "\n",
    "        chosen_subs = synonyms if random.randint(1, 100) <= 75 else hypernyms\n",
    "\n",
    "        if not chosen_subs : \n",
    "          new_hypotesys.append(word)\n",
    "          continue\n",
    "\n",
    "        synonym = chosen_subs[syn_idx % (len(chosen_subs))]\n",
    "\n",
    "        if synonym in syn_dict and syn_dict[synonym] > 10:\n",
    "          syn_idx += 1\n",
    "          synonym = chosen_subs[syn_idx % (len(chosen_subs))]\n",
    "\n",
    "        syn_dict[synonym] = 1 if  synonym not in syn_dict else syn_dict[synonym] + 1\n",
    "\n",
    "        syn_idx += 1\n",
    "\n",
    "        if '_' in synonym: synonym = synonym.replace('_', ' ')\n",
    "        new_hypotesys.append(synonym)\n",
    "\n",
    "      else: new_hypotesys.append(word)\n",
    "\n",
    "    if not new_hypotesys: \n",
    "      new_samples['hypothesis'].append(hypothesis)\n",
    "    else:\n",
    "      new_hypotesys = join_strings_smartly(new_hypotesys)\n",
    "      new_samples['hypothesis'].append(new_hypotesys)\n",
    "    # print(f\"new: {new_hypotesys} \\n\")\n",
    "\n",
    "  # print(new_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Roman Atwood is a content person.',\n",
       " 'The Boston Celtics play their home games at TD Garden.',\n",
       " 'There is a picture called The Hunger Games.',\n",
       " 'Ryan Seacrest is a person.',\n",
       " 'Stranger than Fiction is a film.',\n",
       " 'Selena recorded music.',\n",
       " 'Selena tape music.',\n",
       " 'Selena recorded music.',\n",
       " 'Selena recorded music.',\n",
       " 'John Wick: Chapter 2 was theatrically released in the Oregon.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_samples['hypothesis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "Augmented dataset type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "Train split length: 51086\n",
      "Train split augmentated: 51096\n"
     ]
    }
   ],
   "source": [
    "#@title Add new samples to the original dataset\n",
    " \n",
    "augmentation = Dataset.from_dict(new_samples)\n",
    "print(f\"Augmentation type: {type(augmentation)}\")\n",
    "\n",
    "augmented_dataset = concatenate_datasets([fever_dataset['train'], augmentation])\n",
    "print(f\"Augmented dataset type: {type(augmented_dataset)}\")\n",
    "print(f\"Train split length: {len(fever_dataset['train'])}\")\n",
    "print(f\"Train split augmentated: {len(augmented_dataset)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
