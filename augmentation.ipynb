{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_from_remote = True\n",
    "load_from_local = True\n",
    "save_to_local = True\n",
    "\n",
    "local_dataset = \"./dataset/fever_dataset\"\n",
    "local_adversarial = \"./dataset/adversarial_dataset\"\n",
    "%mkdir -p {local_dataset}\n",
    "%mkdir -p {local_adversarial}\n",
    "\n",
    "print_statistics = False\n",
    "save_info = False\n",
    "info = \"./info/\"\n",
    "%mkdir -p {info}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/leeoos/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/leeoos/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# general imports\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# visualization and statistics\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from datasets import(\n",
    "  Dataset, \n",
    "  load_dataset, \n",
    "  load_from_disk, \n",
    "  concatenate_datasets,\n",
    ") \n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# spacy\n",
    "import spacy \n",
    "# Load the SpaCy model\n",
    "nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# huggingface\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "# set seeds\n",
    "set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data from local repository\n",
      "Done!\n",
      "Train ssplit length: 51086\n",
      "Max id in train split: 99998\n"
     ]
    }
   ],
   "source": [
    "# download data or just load from local \n",
    "\n",
    "download_from_remote = not(os.path.exists(local_dataset) and os.path.exists(local_adversarial))\n",
    "\n",
    "if download_from_remote:\n",
    "    print(\"Downloading data from remote repository\")\n",
    "\n",
    "    # load chunk of FEVER datyaset\n",
    "    fever_dataset = load_dataset(\"tommasobonomo/sem_augmented_fever_nli\", trust_remote_code=True)\n",
    "\n",
    "    # load adversarial \n",
    "    adversarial_testset = load_dataset(\"iperbole/adversarial_fever_nli\", trust_remote_code=True)\n",
    "\n",
    "    # structure of the dataset\n",
    "    print(fever_dataset)\n",
    "\n",
    "    if save_to_local:\n",
    "        print(f\"Save data in local {local_dataset}\")\n",
    "        fever_dataset.save_to_disk(local_dataset)\n",
    "        print(f\"Save adversarial dataset in {local_adversarial}\")\n",
    "        adversarial_testset.save_to_disk(local_adversarial)\n",
    "\n",
    "elif load_from_local:\n",
    "    print(f\"Load data from local repository\")\n",
    "    fever_dataset = load_from_disk(local_dataset)\n",
    "    adversarial_testset = load_from_disk(local_adversarial)\n",
    "    \n",
    "print(\"Done!\")\n",
    "\n",
    "print(f\"Train ssplit length: {len(fever_dataset['train'])}\")\n",
    "max_id = max(fever_dataset['train']['id'])\n",
    "print(f\"Max id in train split: {max_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'premise', 'hypothesis', 'label', 'wsd', 'srl'],\n",
       "        num_rows: 51086\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'premise', 'hypothesis', 'label', 'wsd', 'srl'],\n",
       "        num_rows: 2288\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'premise', 'hypothesis', 'label', 'wsd', 'srl'],\n",
       "        num_rows: 2287\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fever_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Tokenization function for datapoint visualization\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "label_map = {\n",
    "    'ENTAILMENT': 0,\n",
    "    'NEUTRAL': 1,\n",
    "    'CONTRADICTION': 2,\n",
    "    'NOT ENOUGH INFO': None\n",
    "}\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    examples['label'] = [label_map[label] for label in examples['label']]\n",
    "    return tokenizer(examples['premise'], examples['hypothesis'], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Exploration utils\n",
    "\n",
    "def plot_labels_distribution(target_set, title=''):\n",
    "\n",
    "    labels = [\n",
    "        'ENTAILMENT',\n",
    "        'NEUTRAL',\n",
    "        'CONTRADICTION',\n",
    "    ]\n",
    "\n",
    "    label_counts = {}\n",
    "    for label in target_set['label']:\n",
    "        if label not in label_counts:\n",
    "            label_counts[label] = 0\n",
    "        label_counts[label] += 1\n",
    "\n",
    "    plt.bar(labels, label_counts.values())\n",
    "    plt.xlabel('Label')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'Distribution of labels in {title}')\n",
    "    plt.show()\n",
    "    print()\n",
    "        \n",
    "\n",
    "def plot_lengths_distribution(target_set, title='', compare_length=False):\n",
    "    # Extract premises and hypotheses\n",
    "    premises = [item['premise'] for item in target_set]\n",
    "    hypotheses = [item['hypothesis'] for item in target_set]\n",
    "\n",
    "    # Compute lengths\n",
    "    premise_lengths = [len(premise.split()) for premise in premises]\n",
    "    hypothesis_lengths = [len(hypothesis.split()) for hypothesis in hypotheses]\n",
    "\n",
    "    # Plotting length distributions\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    sns.histplot(premise_lengths, bins=50, kde=True, ax=axes[0], color='blue', log_scale=(False, True))\n",
    "    axes[0].set_title('Premise Length Distribution')\n",
    "    axes[0].set_xlabel('Number of Words')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "\n",
    "    sns.histplot(hypothesis_lengths, bins=50, kde=True, ax=axes[1], color='green', log_scale=(False, True))\n",
    "    axes[1].set_title('Hypothesis Length Distribution')\n",
    "    axes[1].set_xlabel('Number of Words')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "\n",
    "    fig.suptitle(f'Premise and Hypothesis Length Distribution in {title}')\n",
    "    plt.show()\n",
    "\n",
    "    if compare_length:\n",
    "        # Plot premise vs hypothesis length scatter plot\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(premise_lengths, hypothesis_lengths, alpha=0.5, s=1)\n",
    "        plt.title('Premise vs Hypothesis Length')\n",
    "        plt.xlabel('Premise Length')\n",
    "        plt.ylabel('Hypothesis Length')\n",
    "        plt.yscale('log')\n",
    "        plt.xscale('log')\n",
    "        plt.grid(True, which=\"both\", ls=\"--\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_vocb_distribution():\n",
    "    # Tokenize words\n",
    "    # premise_words = [word for text in premises for word in word_tokenize(text)]\n",
    "    # hypothesis_words = [word for text in hypotheses for word in word_tokenize(text)]\n",
    "\n",
    "    # # Compute word frequencies\n",
    "    # premise_word_freq = Counter(premise_words)\n",
    "    # hypothesis_word_freq = Counter(hypothesis_words)\n",
    "\n",
    "    # # Plotting the most common words\n",
    "    # premise_common_words = premise_word_freq.most_common(20)\n",
    "    # hypothesis_common_words = hypothesis_word_freq.most_common(20)\n",
    "\n",
    "    # fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    # axes[0].barh([word[0] for word in premise_common_words], [word[1] for word in premise_common_words])\n",
    "    # axes[0].set_title('Premise Common Words')\n",
    "    # axes[1].barh([word[0] for word in hypothesis_common_words], [word[1] for word in hypothesis_common_words])\n",
    "    # axes[1].set_title('Hypothesis Common Words')\n",
    "    # plt.show()\n",
    "    ...\n",
    "\n",
    "# Prepare data for plotting\n",
    "# lengths_and_labels = [(length, item['label']) for length, item in zip(premise_lengths, train_dataset)]\n",
    "# premise_df = pd.DataFrame(lengths_and_labels, columns=['length', 'label'])\n",
    "\n",
    "# lengths_and_labels = [(length, item['label']) for length, item in zip(hypothesis_lengths, train_dataset)]\n",
    "# hypothesis_df = pd.DataFrame(lengths_and_labels, columns=['length', 'label'])\n",
    "\n",
    "# # Plot premise lengths across classes\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.boxplot(x='label', y='length', data=premise_df)\n",
    "# plt.title('Premise Length Distribution Across Classes')\n",
    "# plt.show()\n",
    "\n",
    "# # Plot hypothesis lengths across classes\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.boxplot(x='label', y='length', data=hypothesis_df)\n",
    "# plt.title('Hypothesis Length Distribution Across Classes')\n",
    "# plt.show()\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# import numpy as np\n",
    "\n",
    "# # Create combined list of premises and hypotheses\n",
    "# texts = premises + hypotheses\n",
    "\n",
    "# # Compute TF-IDF vectors\n",
    "# tfidf = TfidfVectorizer().fit_transform(texts)\n",
    "\n",
    "# # Compute cosine similarity between each premise and its corresponding hypothesis\n",
    "# cosine_sim = [cosine_similarity(tfidf[i], tfidf[i + len(premises)])[0][0] for i in range(len(premises))]\n",
    "\n",
    "# # Plot cosine similarity distribution\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.hist(cosine_sim, bins=50, alpha=0.5, color='purple')\n",
    "# plt.title('Cosine Similarity Distribution Between Premise and Hypothesis')\n",
    "# plt.xlabel('Cosine Similarity')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics about the regular dataset\n",
    "if print_statistics:\n",
    "  plot_labels_distribution(fever_dataset['train'], title=\"Fever Train Set\")\n",
    "  plot_lengths_distribution(fever_dataset['train'], title=\"Fever Train Set\", compare_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['premise', 'hypothesis'])\n",
      "[{'index': 0, 'rawText': 'Roman'}, {'index': 1, 'rawText': 'Atwood'}, {'index': 2, 'rawText': 'is'}, {'index': 3, 'rawText': 'a'}, {'index': 4, 'rawText': 'content'}, {'index': 5, 'rawText': 'creator'}, {'index': 6, 'rawText': '.'}]\n",
      "[{'tokenIndex': 2, 'verbatlas': {'frameName': 'COPULA', 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [0, 2]}, {'role': 'Attribute', 'score': 1.0, 'span': [3, 6]}]}, 'englishPropbank': {'frameName': 'be.01', 'roles': [{'role': 'ARG1', 'score': 1.0, 'span': [0, 2]}, {'role': 'ARG2', 'score': 1.0, 'span': [3, 6]}]}}]\n"
     ]
    }
   ],
   "source": [
    "# print(fever_dataset['train'][0]['srl'].keys())\n",
    "# print(fever_dataset['train'][0]['srl']['hypothesis']['tokens'])\n",
    "# print(fever_dataset['train'][0]['srl']['hypothesis']['annotations'])\n",
    "\n",
    "# for token in fever_dataset['train'][0]['srl']['premise']['tokens']:\n",
    "#     print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WSD exploration\n",
    "\n",
    "sample_range = 10 #len(fever_dataset['train'])\n",
    "loop = tqdm(range(sample_range))\n",
    "\n",
    "pos_info = dict()\n",
    "\n",
    "for i in loop:\n",
    "\n",
    "  data =  fever_dataset['train'][i]\n",
    "  sample_id = data['id']\n",
    "\n",
    "  hypothesis = data['hypothesis']\n",
    "  hyp_wsd = data['wsd']['hypothesis']\n",
    "\n",
    "  for hyp_wsd_dict in hyp_wsd:\n",
    "    \n",
    "    pos  = hyp_wsd_dict['pos']\n",
    "    pos_info[pos] = 1 if pos not in pos_info else pos_info[pos] + 1\n",
    "\n",
    "pos_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenIndex:\t4\n",
      "verbatlas:\t{'frameName': 'COPULA', 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [3, 4]}, {'role': 'Attribute', 'score': 1.0, 'span': [5, 22]}]}\n",
      "englishPropbank:\t{'frameName': 'be.01', 'roles': [{'role': 'ARG1', 'score': 1.0, 'span': [3, 4]}, {'role': 'ARG2', 'score': 1.0, 'span': [5, 22]}]}\n",
      "\n",
      "tokenIndex:\t6\n",
      "verbatlas:\t{'frameName': 'KNOW', 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [3, 4]}, {'role': 'Attribute', 'score': 1.0, 'span': [5, 6]}, {'role': 'Topic', 'score': 1.0, 'span': [7, 22]}]}\n",
      "englishPropbank:\t{'frameName': 'know.01', 'roles': [{'role': 'ARG1', 'score': 1.0, 'span': [3, 4]}, {'role': 'ARGM-MNR', 'score': 1.0, 'span': [5, 6]}, {'role': 'ARG2', 'score': 1.0, 'span': [7, 22]}]}\n",
      "\n",
      "tokenIndex:\t13\n",
      "verbatlas:\t{'frameName': 'RECORD', 'roles': [{'role': 'Location', 'score': 1.0, 'span': [8, 10]}, {'role': 'Agent', 'score': 1.0, 'span': [12, 13]}, {'role': 'Theme', 'score': 1.0, 'span': [14, 18]}, {'role': 'Time', 'score': 1.0, 'span': [18, 22]}]}\n",
      "englishPropbank:\t{'frameName': 'post.01', 'roles': [{'role': 'ARGM-LOC', 'score': 1.0, 'span': [8, 10]}, {'role': 'R-ARGM-LOC', 'score': 1.0, 'span': [11, 12]}, {'role': 'ARG0', 'score': 1.0, 'span': [12, 13]}, {'role': 'ARG1', 'score': 1.0, 'span': [14, 18]}, {'role': 'ARGM-TMP', 'score': 1.0, 'span': [18, 22]}]}\n",
      "\n",
      "tokenIndex:\t32\n",
      "verbatlas:\t{'frameName': 'EXIST-WITH-FEATURE', 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [23, 32]}, {'role': 'Attribute', 'score': 1.0, 'span': [33, 43]}]}\n",
      "englishPropbank:\t{'frameName': 'have.03', 'roles': [{'role': 'ARG0', 'score': 1.0, 'span': [23, 32]}, {'role': 'ARG1', 'score': 1.0, 'span': [33, 43]}]}\n",
      "\n",
      "tokenIndex:\t46\n",
      "verbatlas:\t{'frameName': 'EXIST-WITH-FEATURE', 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [44, 45]}, {'role': 'Attribute', 'score': 1.0, 'span': [47, 60]}]}\n",
      "englishPropbank:\t{'frameName': 'have.03', 'roles': [{'role': 'ARG0', 'score': 1.0, 'span': [44, 45]}, {'role': 'ARGM-DIS', 'score': 1.0, 'span': [45, 46]}, {'role': 'ARG1', 'score': 1.0, 'span': [47, 60]}]}\n",
      "\n",
      "tokenIndex:\t50\n",
      "verbatlas:\t{'frameName': 'NAME', 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [47, 50]}, {'role': 'Attribute', 'score': 1.0, 'span': [51, 55]}]}\n",
      "englishPropbank:\t{'frameName': 'call.01', 'roles': [{'role': 'ARG1', 'score': 1.0, 'span': [47, 50]}, {'role': 'ARG2', 'score': 1.0, 'span': [51, 55]}]}\n",
      "\n",
      "tokenIndex:\t58\n",
      "verbatlas:\t{'frameName': 'RECORD', 'roles': [{'role': 'Location', 'score': 1.0, 'span': [47, 55]}, {'role': 'Agent', 'score': 1.0, 'span': [57, 58]}, {'role': 'Theme', 'score': 1.0, 'span': [59, 60]}]}\n",
      "englishPropbank:\t{'frameName': 'post.01', 'roles': [{'role': 'ARGM-LOC', 'score': 1.0, 'span': [47, 55]}, {'role': 'R-ARGM-LOC', 'score': 1.0, 'span': [56, 57]}, {'role': 'ARG0', 'score': 1.0, 'span': [57, 58]}, {'role': 'ARG1', 'score': 1.0, 'span': [59, 60]}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for annotation in fever_dataset['train'][0]['srl']['premise']['annotations']:\n",
    "    for key, value in annotation.items():\n",
    "        print(f\"{key}:\\t{value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'premise': {'tokens': [{'index': 0, 'rawText': 'Roman'},\n",
       "    {'index': 1, 'rawText': 'Atwood'},\n",
       "    {'index': 2, 'rawText': '.'},\n",
       "    {'index': 3, 'rawText': 'He'},\n",
       "    {'index': 4, 'rawText': 'is'},\n",
       "    {'index': 5, 'rawText': 'best'},\n",
       "    {'index': 6, 'rawText': 'known'},\n",
       "    {'index': 7, 'rawText': 'for'},\n",
       "    {'index': 8, 'rawText': 'his'},\n",
       "    {'index': 9, 'rawText': 'vlogs'},\n",
       "    {'index': 10, 'rawText': ','},\n",
       "    {'index': 11, 'rawText': 'where'},\n",
       "    {'index': 12, 'rawText': 'he'},\n",
       "    {'index': 13, 'rawText': 'posts'},\n",
       "    {'index': 14, 'rawText': 'updates'},\n",
       "    {'index': 15, 'rawText': 'about'},\n",
       "    {'index': 16, 'rawText': 'his'},\n",
       "    {'index': 17, 'rawText': 'life'},\n",
       "    {'index': 18, 'rawText': 'on'},\n",
       "    {'index': 19, 'rawText': 'a'},\n",
       "    {'index': 20, 'rawText': 'daily'},\n",
       "    {'index': 21, 'rawText': 'basis'},\n",
       "    {'index': 22, 'rawText': '.'},\n",
       "    {'index': 23, 'rawText': 'His'},\n",
       "    {'index': 24, 'rawText': 'vlogging'},\n",
       "    {'index': 25, 'rawText': 'channel'},\n",
       "    {'index': 26, 'rawText': ','},\n",
       "    {'index': 27, 'rawText': '`'},\n",
       "    {'index': 28, 'rawText': '`'},\n",
       "    {'index': 29, 'rawText': 'RomanAtwoodVlogs'},\n",
       "    {'index': 30, 'rawText': \"''\"},\n",
       "    {'index': 31, 'rawText': ','},\n",
       "    {'index': 32, 'rawText': 'has'},\n",
       "    {'index': 33, 'rawText': 'a'},\n",
       "    {'index': 34, 'rawText': 'total'},\n",
       "    {'index': 35, 'rawText': 'of'},\n",
       "    {'index': 36, 'rawText': '3.3'},\n",
       "    {'index': 37, 'rawText': 'billion'},\n",
       "    {'index': 38, 'rawText': 'views'},\n",
       "    {'index': 39, 'rawText': 'and'},\n",
       "    {'index': 40, 'rawText': '11.9'},\n",
       "    {'index': 41, 'rawText': 'million'},\n",
       "    {'index': 42, 'rawText': 'subscribers'},\n",
       "    {'index': 43, 'rawText': '.'},\n",
       "    {'index': 44, 'rawText': 'He'},\n",
       "    {'index': 45, 'rawText': 'also'},\n",
       "    {'index': 46, 'rawText': 'has'},\n",
       "    {'index': 47, 'rawText': 'another'},\n",
       "    {'index': 48, 'rawText': 'YouTube'},\n",
       "    {'index': 49, 'rawText': 'channel'},\n",
       "    {'index': 50, 'rawText': 'called'},\n",
       "    {'index': 51, 'rawText': '`'},\n",
       "    {'index': 52, 'rawText': '`'},\n",
       "    {'index': 53, 'rawText': 'RomanAtwood'},\n",
       "    {'index': 54, 'rawText': \"''\"},\n",
       "    {'index': 55, 'rawText': ','},\n",
       "    {'index': 56, 'rawText': 'where'},\n",
       "    {'index': 57, 'rawText': 'he'},\n",
       "    {'index': 58, 'rawText': 'posts'},\n",
       "    {'index': 59, 'rawText': 'pranks'},\n",
       "    {'index': 60, 'rawText': '.'}],\n",
       "   'annotations': [{'tokenIndex': 4,\n",
       "     'verbatlas': {'frameName': 'COPULA',\n",
       "      'roles': [{'role': 'Theme', 'score': 1.0, 'span': [3, 4]},\n",
       "       {'role': 'Attribute', 'score': 1.0, 'span': [5, 22]}]},\n",
       "     'englishPropbank': {'frameName': 'be.01',\n",
       "      'roles': [{'role': 'ARG1', 'score': 1.0, 'span': [3, 4]},\n",
       "       {'role': 'ARG2', 'score': 1.0, 'span': [5, 22]}]}},\n",
       "    {'tokenIndex': 6,\n",
       "     'verbatlas': {'frameName': 'KNOW',\n",
       "      'roles': [{'role': 'Theme', 'score': 1.0, 'span': [3, 4]},\n",
       "       {'role': 'Attribute', 'score': 1.0, 'span': [5, 6]},\n",
       "       {'role': 'Topic', 'score': 1.0, 'span': [7, 22]}]},\n",
       "     'englishPropbank': {'frameName': 'know.01',\n",
       "      'roles': [{'role': 'ARG1', 'score': 1.0, 'span': [3, 4]},\n",
       "       {'role': 'ARGM-MNR', 'score': 1.0, 'span': [5, 6]},\n",
       "       {'role': 'ARG2', 'score': 1.0, 'span': [7, 22]}]}},\n",
       "    {'tokenIndex': 13,\n",
       "     'verbatlas': {'frameName': 'RECORD',\n",
       "      'roles': [{'role': 'Location', 'score': 1.0, 'span': [8, 10]},\n",
       "       {'role': 'Agent', 'score': 1.0, 'span': [12, 13]},\n",
       "       {'role': 'Theme', 'score': 1.0, 'span': [14, 18]},\n",
       "       {'role': 'Time', 'score': 1.0, 'span': [18, 22]}]},\n",
       "     'englishPropbank': {'frameName': 'post.01',\n",
       "      'roles': [{'role': 'ARGM-LOC', 'score': 1.0, 'span': [8, 10]},\n",
       "       {'role': 'R-ARGM-LOC', 'score': 1.0, 'span': [11, 12]},\n",
       "       {'role': 'ARG0', 'score': 1.0, 'span': [12, 13]},\n",
       "       {'role': 'ARG1', 'score': 1.0, 'span': [14, 18]},\n",
       "       {'role': 'ARGM-TMP', 'score': 1.0, 'span': [18, 22]}]}},\n",
       "    {'tokenIndex': 32,\n",
       "     'verbatlas': {'frameName': 'EXIST-WITH-FEATURE',\n",
       "      'roles': [{'role': 'Theme', 'score': 1.0, 'span': [23, 32]},\n",
       "       {'role': 'Attribute', 'score': 1.0, 'span': [33, 43]}]},\n",
       "     'englishPropbank': {'frameName': 'have.03',\n",
       "      'roles': [{'role': 'ARG0', 'score': 1.0, 'span': [23, 32]},\n",
       "       {'role': 'ARG1', 'score': 1.0, 'span': [33, 43]}]}},\n",
       "    {'tokenIndex': 46,\n",
       "     'verbatlas': {'frameName': 'EXIST-WITH-FEATURE',\n",
       "      'roles': [{'role': 'Theme', 'score': 1.0, 'span': [44, 45]},\n",
       "       {'role': 'Attribute', 'score': 1.0, 'span': [47, 60]}]},\n",
       "     'englishPropbank': {'frameName': 'have.03',\n",
       "      'roles': [{'role': 'ARG0', 'score': 1.0, 'span': [44, 45]},\n",
       "       {'role': 'ARGM-DIS', 'score': 1.0, 'span': [45, 46]},\n",
       "       {'role': 'ARG1', 'score': 1.0, 'span': [47, 60]}]}},\n",
       "    {'tokenIndex': 50,\n",
       "     'verbatlas': {'frameName': 'NAME',\n",
       "      'roles': [{'role': 'Theme', 'score': 1.0, 'span': [47, 50]},\n",
       "       {'role': 'Attribute', 'score': 1.0, 'span': [51, 55]}]},\n",
       "     'englishPropbank': {'frameName': 'call.01',\n",
       "      'roles': [{'role': 'ARG1', 'score': 1.0, 'span': [47, 50]},\n",
       "       {'role': 'ARG2', 'score': 1.0, 'span': [51, 55]}]}},\n",
       "    {'tokenIndex': 58,\n",
       "     'verbatlas': {'frameName': 'RECORD',\n",
       "      'roles': [{'role': 'Location', 'score': 1.0, 'span': [47, 55]},\n",
       "       {'role': 'Agent', 'score': 1.0, 'span': [57, 58]},\n",
       "       {'role': 'Theme', 'score': 1.0, 'span': [59, 60]}]},\n",
       "     'englishPropbank': {'frameName': 'post.01',\n",
       "      'roles': [{'role': 'ARGM-LOC', 'score': 1.0, 'span': [47, 55]},\n",
       "       {'role': 'R-ARGM-LOC', 'score': 1.0, 'span': [56, 57]},\n",
       "       {'role': 'ARG0', 'score': 1.0, 'span': [57, 58]},\n",
       "       {'role': 'ARG1', 'score': 1.0, 'span': [59, 60]}]}}]},\n",
       "  'hypothesis': {'tokens': [{'index': 0, 'rawText': 'Roman'},\n",
       "    {'index': 1, 'rawText': 'Atwood'},\n",
       "    {'index': 2, 'rawText': 'is'},\n",
       "    {'index': 3, 'rawText': 'a'},\n",
       "    {'index': 4, 'rawText': 'content'},\n",
       "    {'index': 5, 'rawText': 'creator'},\n",
       "    {'index': 6, 'rawText': '.'}],\n",
       "   'annotations': [{'tokenIndex': 2,\n",
       "     'verbatlas': {'frameName': 'COPULA',\n",
       "      'roles': [{'role': 'Theme', 'score': 1.0, 'span': [0, 2]},\n",
       "       {'role': 'Attribute', 'score': 1.0, 'span': [3, 6]}]},\n",
       "     'englishPropbank': {'frameName': 'be.01',\n",
       "      'roles': [{'role': 'ARG1', 'score': 1.0, 'span': [0, 2]},\n",
       "       {'role': 'ARG2', 'score': 1.0, 'span': [3, 6]}]}}]}},\n",
       " {'premise': {'tokens': [{'index': 0, 'rawText': 'Boston'},\n",
       "    {'index': 1, 'rawText': 'Celtics'},\n",
       "    {'index': 2, 'rawText': '.'},\n",
       "    {'index': 3, 'rawText': 'The'},\n",
       "    {'index': 4, 'rawText': 'Celtics'},\n",
       "    {'index': 5, 'rawText': 'play'},\n",
       "    {'index': 6, 'rawText': 'their'},\n",
       "    {'index': 7, 'rawText': 'home'},\n",
       "    {'index': 8, 'rawText': 'games'},\n",
       "    {'index': 9, 'rawText': 'at'},\n",
       "    {'index': 10, 'rawText': 'the'},\n",
       "    {'index': 11, 'rawText': 'TD'},\n",
       "    {'index': 12, 'rawText': 'Garden'},\n",
       "    {'index': 13, 'rawText': ','},\n",
       "    {'index': 14, 'rawText': 'which'},\n",
       "    {'index': 15, 'rawText': 'they'},\n",
       "    {'index': 16, 'rawText': 'share'},\n",
       "    {'index': 17, 'rawText': 'with'},\n",
       "    {'index': 18, 'rawText': 'the'},\n",
       "    {'index': 19, 'rawText': 'National'},\n",
       "    {'index': 20, 'rawText': 'Hockey'},\n",
       "    {'index': 21, 'rawText': 'League'},\n",
       "    {'index': 22, 'rawText': '('},\n",
       "    {'index': 23, 'rawText': 'NHL'},\n",
       "    {'index': 24, 'rawText': ')'},\n",
       "    {'index': 25, 'rawText': \"'\"},\n",
       "    {'index': 26, 'rawText': 's'},\n",
       "    {'index': 27, 'rawText': 'Boston'},\n",
       "    {'index': 28, 'rawText': 'Bruins'},\n",
       "    {'index': 29, 'rawText': '.'}],\n",
       "   'annotations': [{'tokenIndex': 5,\n",
       "     'verbatlas': {'frameName': 'PLAY_SPORT/GAME',\n",
       "      'roles': [{'role': 'Agent', 'score': 1.0, 'span': [3, 5]},\n",
       "       {'role': 'Theme', 'score': 1.0, 'span': [6, 9]},\n",
       "       {'role': 'Location', 'score': 1.0, 'span': [9, 29]}]},\n",
       "     'englishPropbank': {'frameName': 'play.01',\n",
       "      'roles': [{'role': 'ARG0', 'score': 1.0, 'span': [3, 5]},\n",
       "       {'role': 'ARG1', 'score': 1.0, 'span': [6, 9]},\n",
       "       {'role': 'ARGM-LOC', 'score': 1.0, 'span': [9, 29]}]}},\n",
       "    {'tokenIndex': 16,\n",
       "     'verbatlas': {'frameName': 'SHARE',\n",
       "      'roles': [{'role': 'Theme', 'score': 1.0, 'span': [10, 13]},\n",
       "       {'role': 'Agent', 'score': 1.0, 'span': [15, 16]},\n",
       "       {'role': 'Co-Agent', 'score': 1.0, 'span': [17, 29]}]},\n",
       "     'englishPropbank': {'frameName': 'share.01',\n",
       "      'roles': [{'role': 'ARG1', 'score': 1.0, 'span': [10, 13]},\n",
       "       {'role': 'R-ARG1', 'score': 1.0, 'span': [14, 15]},\n",
       "       {'role': 'ARG0', 'score': 1.0, 'span': [15, 16]},\n",
       "       {'role': 'ARG2', 'score': 1.0, 'span': [17, 29]}]}}]},\n",
       "  'hypothesis': {'tokens': [{'index': 0, 'rawText': 'The'},\n",
       "    {'index': 1, 'rawText': 'Boston'},\n",
       "    {'index': 2, 'rawText': 'Celtics'},\n",
       "    {'index': 3, 'rawText': 'play'},\n",
       "    {'index': 4, 'rawText': 'their'},\n",
       "    {'index': 5, 'rawText': 'home'},\n",
       "    {'index': 6, 'rawText': 'games'},\n",
       "    {'index': 7, 'rawText': 'at'},\n",
       "    {'index': 8, 'rawText': 'TD'},\n",
       "    {'index': 9, 'rawText': 'Garden'},\n",
       "    {'index': 10, 'rawText': '.'}],\n",
       "   'annotations': [{'tokenIndex': 3,\n",
       "     'verbatlas': {'frameName': 'PLAY_SPORT/GAME',\n",
       "      'roles': [{'role': 'Agent', 'score': 1.0, 'span': [0, 3]},\n",
       "       {'role': 'Theme', 'score': 1.0, 'span': [4, 7]},\n",
       "       {'role': 'Location', 'score': 1.0, 'span': [7, 10]}]},\n",
       "     'englishPropbank': {'frameName': 'play.01',\n",
       "      'roles': [{'role': 'ARG0', 'score': 1.0, 'span': [0, 3]},\n",
       "       {'role': 'ARG1', 'score': 1.0, 'span': [4, 7]},\n",
       "       {'role': 'ARGM-LOC', 'score': 1.0, 'span': [7, 10]}]}}]}}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fever_dataset['train'][0:2]['srl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title SRL exploration: collect frame-names and roles \n",
    "\n",
    "def get_srl_info(dataset):\n",
    "\n",
    "  sample_range = len(dataset)\n",
    "  loop = tqdm(range(sample_range))\n",
    "\n",
    "  relevant_info = dict()\n",
    "\n",
    "  set_of_va_frames = dict() # verb atlas frames\n",
    "  set_of_pb_frames = dict() # prop bank frames\n",
    "\n",
    "  verbs_freqs = dict()\n",
    "\n",
    "  set_of_va_roles = set() # verb atlas frames\n",
    "  set_of_pb_roles = set() # prop bank frames\n",
    "\n",
    "  for i in loop:\n",
    "\n",
    "    data =  dataset[i]\n",
    "    sample_id = data['id']\n",
    "    \n",
    "    tokens = data['srl']['premise']['tokens']\n",
    "    annotations = data['srl']['premise']['annotations']\n",
    "\n",
    "    for annotation in annotations:\n",
    "\n",
    "      token_index = annotation['tokenIndex']\n",
    "      verb = tokens[token_index]['rawText']\n",
    "      verbs_freqs[verb] =  1 if verb not in  verbs_freqs else verbs_freqs[verb] + 1\n",
    "\n",
    "      vb_frame = annotation['verbatlas']['frameName']\n",
    "      pb_frame = annotation['englishPropbank']['frameName']\n",
    "\n",
    "      set_of_va_frames[vb_frame] = 1 if vb_frame not in set_of_va_frames else set_of_va_frames[vb_frame] + 1 \n",
    "      set_of_pb_frames[pb_frame] = 1 if pb_frame not in set_of_pb_frames else set_of_pb_frames[pb_frame] + 1 \n",
    "\n",
    "      va_roles = annotation['verbatlas']['roles']\n",
    "      pb_roles = annotation['englishPropbank']['roles']\n",
    "\n",
    "      for role in va_roles:\n",
    "        set_of_va_roles.add(role['role'])\n",
    "\n",
    "      for role in pb_roles:\n",
    "        set_of_pb_roles.add(role['role'])\n",
    "\n",
    "  return set_of_va_frames, set_of_pb_frames, set_of_va_roles, set_of_pb_roles, verbs_freqs\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51086/51086 [01:13<00:00, 690.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Verb Atlas frames: 388\n",
      "Number of Verb Atlas roles: 30\n",
      "Number of Propbank frames: 1932\n",
      "Number of Propbank roles: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "set_of_va_frames, set_of_pb_frames, set_of_va_roles, set_of_pb_roles, verbs_freqs = get_srl_info(fever_dataset['train'])\n",
    "\n",
    "verbs_freqs = sorted(verbs_freqs.items(), key=lambda item: item[1], reverse=True)\n",
    "set_of_pb_frames = sorted(set_of_pb_frames.items(), key=lambda item: item[1], reverse=True)\n",
    "set_of_va_frames = sorted(set_of_va_frames.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "print(f\"Number of Verb Atlas frames: {len(set_of_va_frames)}\")\n",
    "print(f\"Number of Verb Atlas roles: {len(set_of_va_roles)}\")\n",
    "\n",
    "print(f\"Number of Propbank frames: {len(set_of_pb_frames)}\")\n",
    "print(f\"Number of Propbank roles: {len(set_of_pb_roles)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving all Verb Atlas roles of the dataset into ./info/va_roles.txt\n",
      "Saving all Verb Atlas frames of the dataset into ./info/va_frames.txt\n",
      "Saving all Propbank roles of the dataset into ./info/pb_roles.txt\n",
      "Saving all Propbank frames of the dataset into ./info/pb_frames.txt\n",
      "Saving all verbs frequencies count into ./info/verbs_freqs.txt\n"
     ]
    }
   ],
   "source": [
    "#@title Save srl info\n",
    "\n",
    "if save_info:\n",
    "\n",
    "  with open(info + \"va_roles.txt\", \"w\") as va_roles:\n",
    "    print(f\"Saving all Verb Atlas roles of the dataset into {info + 'va_roles.txt'}\")\n",
    "    for elem in set_of_va_roles: va_roles.write(f\"{elem}\\n\")\n",
    "\n",
    "  with open(info + \"va_frames.txt\", \"w\") as va_frames:\n",
    "    print(f\"Saving all Verb Atlas frames of the dataset into {info + 'va_frames.txt'}\")\n",
    "    for elem in set_of_pb_frames: va_frames.write(f\"{elem}\\n\")\n",
    "\n",
    "\n",
    "  with open(info + \"pb_roles.txt\", \"w\") as pb_roles:\n",
    "    print(f\"Saving all Propbank roles of the dataset into {info + 'pb_roles.txt'}\")\n",
    "    for elem in set_of_pb_roles: pb_roles.write(f\"{elem}\\n\")\n",
    "\n",
    "\n",
    "  with open(info + \"pb_frames.txt\", \"w\") as pb_frames:\n",
    "    print(f\"Saving all Propbank frames of the dataset into {info + 'pb_frames.txt'}\")\n",
    "    for elem in set_of_pb_frames: pb_frames.write(f\"{elem}\\n\")\n",
    "\n",
    "  with open(info + \"verbs_freqs.txt\", \"w\") as verbs:\n",
    "    print(f\"Saving all verbs frequencies count into {info + 'verbs_freqs.txt'}\")\n",
    "    for elem in set_of_pb_frames: verbs.write(f\"{elem}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Augmentation\n",
    "The following prompt can be used to correct the grammar of a modified hypotesis so if just a not is added the phrase is transformed in negative form:\n",
    "\n",
    "\"Correct the grammar in the following inputs, rephrase the input if necessary to make them more accurate. Provide just the correct version, no explanation.\"\n",
    "\n",
    "Ask for:\n",
    "symmetric relation --> Marry\n",
    "antisymmetric relation --> Kill\n",
    "\n",
    "born\n",
    "sell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Augmentation utils\n",
    "\n",
    "def join_strings_smartly(words):\n",
    "    \"\"\" Joins a list of words smartly:\n",
    "    - Adds spaces between words when appropriate.\n",
    "    - Avoids adding spaces before punctuation.\n",
    "    \"\"\"\n",
    "    punctuation = {'.', ',', ';', ':', '!', '?',')'}\n",
    "    result = words[0]\n",
    "    prev = result\n",
    "\n",
    "    for word in words[1:]:\n",
    "      if word in punctuation or \\\n",
    "        (\"'\" in prev) or \\\n",
    "        word.startswith(\"'\") or \\\n",
    "        (\".\" in prev and \".\" in word) or \\\n",
    "        (\"(\" in prev) :\n",
    "        # add word without space before\n",
    "        result += word\n",
    "      else:\n",
    "        # add with space before\n",
    "        result += \" \" + word\n",
    "      # keep track of previous word  \n",
    "      prev = word\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_synset_from_id(synset_id):\n",
    "    if synset_id == 'O':\n",
    "        return None\n",
    "    try:\n",
    "        offset = int(''.join(filter(str.isdigit, synset_id)))\n",
    "        pos = synset_id[-1]\n",
    "        synset = wn.synset_from_pos_and_offset(pos, offset)\n",
    "        return synset\n",
    "    except:\n",
    "        print(\"exception\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "def get_related_word(synset, pos): \n",
    "\n",
    "    info = dict()\n",
    "\n",
    "    # Map POS tags to WordNet POS tags\n",
    "    pos_map = {\n",
    "        'NOUN': wn.NOUN,\n",
    "        'VERB': wn.VERB,\n",
    "        'ADJ': wn.ADJ,\n",
    "        'ADV': wn.ADV\n",
    "    }\n",
    "    \n",
    "    if pos not in pos_map:\n",
    "        return None \n",
    "    \n",
    "    # get hypernyms\n",
    "    hypernyms = synset.hypernyms()\n",
    "    if not hypernyms:\n",
    "        return None\n",
    "    hypernyms = synset.hypernyms()\n",
    "    hypernym_words = set()\n",
    "    for hypernym in hypernyms:\n",
    "        hypernym_words.update(hypernym.lemma_names())\n",
    "    info['hypernyms'] = list(hypernym_words)\n",
    "\n",
    "    # get synonyms \n",
    "    synonyms = synset.lemmas()\n",
    "    if not synonyms:\n",
    "        return None\n",
    "    info['synonyms'] = [synonym.name() for synonym in synonyms if synonym.name() != synset.lemmas()[0].name()]\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 301.98it/s]\n"
     ]
    }
   ],
   "source": [
    "#@title Augmentation by Synonyms and Hypernyms\n",
    "\n",
    "sample_range = 10 #len(fever_dataset['train'])\n",
    "loop = tqdm(range(sample_range))\n",
    "\n",
    "new_samples = {\n",
    "    'id': [],\n",
    "    'premise': [],\n",
    "    'hypothesis': [],\n",
    "    'label': [],\n",
    "    'wsd': [None for i in range(sample_range)],\n",
    "    'srl': [None for i in range(sample_range)]\n",
    "}\n",
    "\n",
    "syn_dict = dict()\n",
    "syn_idx = 0\n",
    "progressive_id = int(max_id)\n",
    "\n",
    "for i in loop:\n",
    "\n",
    "  data =  fever_dataset['train'][i]\n",
    "  sample_id = data['id']\n",
    "\n",
    "  premise = data['premise']\n",
    "  label = data['label']\n",
    "  hyp_wsd = data['wsd']['hypothesis']\n",
    "\n",
    "  new_samples['id'].append(str(progressive_id))\n",
    "  new_samples['premise'].append(premise)\n",
    "  new_samples['label'].append(label)\n",
    "  progressive_id += 1\n",
    "\n",
    "  hypothesis = data['hypothesis']\n",
    "  new_hypotesys = []\n",
    "\n",
    "  for hyp_wsd_dict in hyp_wsd:\n",
    "\n",
    "    word = hyp_wsd_dict['text']\n",
    "    pos = hyp_wsd_dict['pos']\n",
    "    offset = hyp_wsd_dict['wnSynsetOffset']\n",
    "    synset = get_synset_from_id(offset)\n",
    "\n",
    "    related_words = None\n",
    "    if synset: related_words = get_related_word(synset, pos)\n",
    "\n",
    "    if related_words:\n",
    "      hypernyms = related_words['hypernyms']\n",
    "      synonyms = related_words['synonyms']\n",
    "\n",
    "      if not synonyms : \n",
    "        new_hypotesys.append(word)\n",
    "        continue\n",
    "\n",
    "      synonym = synonyms[syn_idx % (len(synonyms))]\n",
    "\n",
    "      if synonym in syn_dict and syn_dict[synonym] > 10:\n",
    "        syn_idx += 1\n",
    "        synonym = synonyms[syn_idx % (len(synonyms))]\n",
    "\n",
    "      syn_dict[synonym] = 1 if  synonym not in syn_dict else syn_dict[synonym] + 1\n",
    "      # if synonym in syn_dict \n",
    "\n",
    "      syn_idx += 1\n",
    "\n",
    "      if '_' in synonym: synonym = synonym.replace('_', ' ')\n",
    "      new_hypotesys.append(synonym)\n",
    "\n",
    "    else: new_hypotesys.append(word)\n",
    "\n",
    "\n",
    "  new_hypotesys = join_strings_smartly(new_hypotesys)\n",
    "  new_samples['hypothesis'].append(new_hypotesys)\n",
    "  # print(f\"new: {new_hypotesys} \\n\")\n",
    "\n",
    "# print(new_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "Augmented dataset type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "Train split length: 51086\n",
      "Train split augmentated: 51096\n"
     ]
    }
   ],
   "source": [
    "#@title Add new samples to the original dataset\n",
    " \n",
    "augmentation = Dataset.from_dict(new_samples)\n",
    "print(f\"Augmentation type: {type(augmentation)}\")\n",
    "\n",
    "augmented_dataset = concatenate_datasets([fever_dataset['train'], augmentation])\n",
    "print(f\"Augmented dataset type: {type(augmented_dataset)}\")\n",
    "print(f\"Train split length: {len(fever_dataset['train'])}\")\n",
    "print(f\"Train split augmentated: {len(augmented_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John Wick: Chapter 2 . The film premiered in Los Angeles on January 30 , 2017 , and was theatrically released in the United States on February 10 , 2017 .\n",
      "[{'index': 0, 'rawText': 'John'}, {'index': 1, 'rawText': 'Wick'}, {'index': 2, 'rawText': ':'}, {'index': 3, 'rawText': 'Chapter'}, {'index': 4, 'rawText': '2'}, {'index': 5, 'rawText': '.'}, {'index': 6, 'rawText': 'The'}, {'index': 7, 'rawText': 'film'}, {'index': 8, 'rawText': 'premiered'}, {'index': 9, 'rawText': 'in'}, {'index': 10, 'rawText': 'Los'}, {'index': 11, 'rawText': 'Angeles'}, {'index': 12, 'rawText': 'on'}, {'index': 13, 'rawText': 'January'}, {'index': 14, 'rawText': '30'}, {'index': 15, 'rawText': ','}, {'index': 16, 'rawText': '2017'}, {'index': 17, 'rawText': ','}, {'index': 18, 'rawText': 'and'}, {'index': 19, 'rawText': 'was'}, {'index': 20, 'rawText': 'theatrically'}, {'index': 21, 'rawText': 'released'}, {'index': 22, 'rawText': 'in'}, {'index': 23, 'rawText': 'the'}, {'index': 24, 'rawText': 'United'}, {'index': 25, 'rawText': 'States'}, {'index': 26, 'rawText': 'on'}, {'index': 27, 'rawText': 'February'}, {'index': 28, 'rawText': '10'}, {'index': 29, 'rawText': ','}, {'index': 30, 'rawText': '2017'}, {'index': 31, 'rawText': '.'}]\n",
      "{'frameName': 'SHOW', 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [6, 8]}, {'role': 'Location', 'score': 1.0, 'span': [9, 12]}, {'role': 'Time', 'score': 1.0, 'span': [12, 17]}]}\n",
      "John Wick: Chapter 2 was theatrically released in the Oregon.\n",
      "John Wick: Chapter 2 . The film premiered in Los Angeles on January 30 , 2017 , and was theatrically released in the United States on February 10 , 2017 .\n",
      "[{'index': 0, 'rawText': 'John'}, {'index': 1, 'rawText': 'Wick'}, {'index': 2, 'rawText': ':'}, {'index': 3, 'rawText': 'Chapter'}, {'index': 4, 'rawText': '2'}, {'index': 5, 'rawText': '.'}, {'index': 6, 'rawText': 'The'}, {'index': 7, 'rawText': 'film'}, {'index': 8, 'rawText': 'premiered'}, {'index': 9, 'rawText': 'in'}, {'index': 10, 'rawText': 'Los'}, {'index': 11, 'rawText': 'Angeles'}, {'index': 12, 'rawText': 'on'}, {'index': 13, 'rawText': 'January'}, {'index': 14, 'rawText': '30'}, {'index': 15, 'rawText': ','}, {'index': 16, 'rawText': '2017'}, {'index': 17, 'rawText': ','}, {'index': 18, 'rawText': 'and'}, {'index': 19, 'rawText': 'was'}, {'index': 20, 'rawText': 'theatrically'}, {'index': 21, 'rawText': 'released'}, {'index': 22, 'rawText': 'in'}, {'index': 23, 'rawText': 'the'}, {'index': 24, 'rawText': 'United'}, {'index': 25, 'rawText': 'States'}, {'index': 26, 'rawText': 'on'}, {'index': 27, 'rawText': 'February'}, {'index': 28, 'rawText': '10'}, {'index': 29, 'rawText': ','}, {'index': 30, 'rawText': '2017'}, {'index': 31, 'rawText': '.'}]\n",
      "{'frameName': 'LIBERATE_ALLOW_AFFORD', 'roles': [{'role': 'Patient', 'score': 1.0, 'span': [6, 8]}, {'role': 'Attribute', 'score': 1.0, 'span': [20, 21]}, {'role': 'Location', 'score': 1.0, 'span': [22, 26]}, {'role': 'Time', 'score': 1.0, 'span': [26, 31]}]}\n",
      "John Wick: Chapter 2 was theatrically released in the Oregon.\n"
     ]
    }
   ],
   "source": [
    "sample_range = 10 \n",
    "# sample_range = len(fever_dataset['train'])\n",
    "\n",
    "locations = set()\n",
    "# loop = tqdm(range(sample_range))\n",
    "\n",
    "# for i in loop:\n",
    "for i in range(sample_range):\n",
    "\n",
    "    data = fever_dataset['train'][i]\n",
    "\n",
    "    # id\n",
    "    id =  data['id']\n",
    "\n",
    "    # premise\n",
    "    sentence = data['premise']\n",
    "    hypothesis = data['hypothesis']\n",
    "\n",
    "    # wsd\n",
    "    wsd =  data['wsd']\n",
    "\n",
    "    # srl\n",
    "    possible_locations = []\n",
    "    tokens = data['srl']['premise']['tokens']\n",
    "    annotations = data['srl']['premise']['annotations']\n",
    "\n",
    "    # print(tokens)\n",
    "    # print(annotations)\n",
    "\n",
    "    for annotation in annotations:\n",
    "        # print(annotation['verbatlas']['roles'])\n",
    "        token_idx = annotation['tokenIndex'] \n",
    "        roles = annotation['verbatlas']['roles']\n",
    "        # index = annotation['verbatlas']\n",
    "\n",
    "        location = False\n",
    "        agent = False\n",
    "\n",
    "        # print(index)\n",
    "        text_location = \"\"\n",
    "        for role in roles:\n",
    "            if role['role'] == 'Location':\n",
    "                span = role['span']\n",
    "\n",
    "                for item in tokens:\n",
    "                    if span[0] <= item['index'] <= span[1]:\n",
    "                        text_location += item['rawText'] + \" \"\n",
    "\n",
    "                # filtered_texts = [item['rawText'] for item in tokens if span[0] <= item['index'] <= span[1]]\n",
    "                # text_location = \" \".join(filtered_texts)\n",
    "                # filtered_words = [word for word in filtered_texts if any(char.isupper() for char in word)]\n",
    "\n",
    "                if text_location:\n",
    "                    # possible_locations.append(text_location)\n",
    "                    # print(f\"sample id: {id}\")\n",
    "                    # print(f\"text: {text_location}\")\n",
    "\n",
    "                    entities =  nlp_spacy(text_location).ents\n",
    "                    # print(f\"entities: {entities}\")\n",
    "                    if entities:\n",
    "                        found_target = False\n",
    "                        for ent in entities:\n",
    "                            if ent.label_ == \"GPE\":\n",
    "                                found_target = True\n",
    "                                locations.add(ent.text)\n",
    "\n",
    "                                # print info related to location\n",
    "                                print(sentence)\n",
    "                                print(tokens)\n",
    "                                print(annotation['verbatlas'])\n",
    "                                print(hypothesis)\n",
    "                                # print(f\"target entity: {ent.text}\")\n",
    "                            # else:\n",
    "                            #     print(\"no target \")\n",
    "                        # if not found_target :  print(\"no location target \")\n",
    "                    else:\n",
    "                        # print(\"no target \")\n",
    "                        ...\n",
    "                    \n",
    "                    # print(\"----------------------------------------- \\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new samples to the dataset\n",
    "filtered_dataset = fever_dataset['train'].filter(lambda example: 'born' in example['premise'].lower() or 'born' in example['hypothesis'].lower())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
