{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# download configuratiins\n",
    "download_from_remote = True\n",
    "load_from_local = True\n",
    "save_to_local = True\n",
    "local_dataset = \"./dataset/fever_dataset\"\n",
    "local_adversarial = \"./dataset/adversarial_dataset\"\n",
    "%mkdir -p {local_dataset}\n",
    "%mkdir -p {local_adversarial}\n",
    "\n",
    "# data analytics\n",
    "print_statistics = False\n",
    "save_info = False\n",
    "info = \"./info/\"\n",
    "%mkdir -p {info}\n",
    "\n",
    "# augmentation\n",
    "syn_hyp_augment = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/leeoos/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/leeoos/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# general imports\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# visualization and statistics\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from datasets import(\n",
    "  Dataset, \n",
    "  load_dataset, \n",
    "  load_from_disk, \n",
    "  concatenate_datasets,\n",
    ") \n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# spacy\n",
    "import spacy \n",
    "# Load the SpaCy model\n",
    "nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# huggingface\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "# set seeds\n",
    "set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data from local repository\n",
      "Done!\n",
      "Train ssplit length: 51086\n",
      "Max id in train split: 99998\n"
     ]
    }
   ],
   "source": [
    "# download data or just load from local \n",
    "\n",
    "download_from_remote = not(os.path.exists(local_dataset) and os.path.exists(local_adversarial))\n",
    "\n",
    "if download_from_remote:\n",
    "    print(\"Downloading data from remote repository\")\n",
    "\n",
    "    # load chunk of FEVER datyaset\n",
    "    fever_dataset = load_dataset(\"tommasobonomo/sem_augmented_fever_nli\", trust_remote_code=True)\n",
    "\n",
    "    # load adversarial \n",
    "    adversarial_testset = load_dataset(\"iperbole/adversarial_fever_nli\", trust_remote_code=True)\n",
    "\n",
    "    # structure of the dataset\n",
    "    print(fever_dataset)\n",
    "\n",
    "    if save_to_local:\n",
    "        print(f\"Save data in local {local_dataset}\")\n",
    "        fever_dataset.save_to_disk(local_dataset)\n",
    "        print(f\"Save adversarial dataset in {local_adversarial}\")\n",
    "        adversarial_testset.save_to_disk(local_adversarial)\n",
    "\n",
    "elif load_from_local:\n",
    "    print(f\"Load data from local repository\")\n",
    "    fever_dataset = load_from_disk(local_dataset)\n",
    "    adversarial_testset = load_from_disk(local_adversarial)\n",
    "    \n",
    "print(\"Done!\")\n",
    "\n",
    "print(f\"Train ssplit length: {len(fever_dataset['train'])}\")\n",
    "max_id = max(fever_dataset['train']['id'])\n",
    "print(f\"Max id in train split: {max_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'premise', 'hypothesis', 'label', 'wsd', 'srl'],\n",
       "        num_rows: 51086\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'premise', 'hypothesis', 'label', 'wsd', 'srl'],\n",
       "        num_rows: 2288\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'premise', 'hypothesis', 'label', 'wsd', 'srl'],\n",
       "        num_rows: 2287\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fever_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Tokenization function for datapoint visualization\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "label_map = {\n",
    "    'ENTAILMENT': 0,\n",
    "    'NEUTRAL': 1,\n",
    "    'CONTRADICTION': 2,\n",
    "    'NOT ENOUGH INFO': None\n",
    "}\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    examples['label'] = [label_map[label] for label in examples['label']]\n",
    "    return tokenizer(examples['premise'], examples['hypothesis'], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Exploration utils\n",
    "\n",
    "def pretty_print_dict(d, indent=0):\n",
    "    for key, value in d.items():\n",
    "        print(' ' * indent + str(key) + ':', end=' ')\n",
    "        if isinstance(value, dict):\n",
    "            print()\n",
    "            pretty_print_dict(value, indent + 4)\n",
    "        else:\n",
    "            print(value)\n",
    "\n",
    "\n",
    "def plot_labels_distribution(target_set, title=''):\n",
    "\n",
    "    labels = [\n",
    "        'ENTAILMENT',\n",
    "        'NEUTRAL',\n",
    "        'CONTRADICTION',\n",
    "    ]\n",
    "\n",
    "    label_counts = {}\n",
    "    for label in target_set['label']:\n",
    "        if label not in label_counts:\n",
    "            label_counts[label] = 0\n",
    "        label_counts[label] += 1\n",
    "\n",
    "    plt.bar(labels, label_counts.values())\n",
    "    plt.xlabel('Label')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'Distribution of labels in {title}')\n",
    "    plt.show()\n",
    "    print()\n",
    "        \n",
    "\n",
    "def plot_lengths_distribution(target_set, title='', compare_length=False):\n",
    "    # Extract premises and hypotheses\n",
    "    premises = [item['premise'] for item in target_set]\n",
    "    hypotheses = [item['hypothesis'] for item in target_set]\n",
    "\n",
    "    # Compute lengths\n",
    "    premise_lengths = [len(premise.split()) for premise in premises]\n",
    "    hypothesis_lengths = [len(hypothesis.split()) for hypothesis in hypotheses]\n",
    "\n",
    "    # Plotting length distributions\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    sns.histplot(premise_lengths, bins=50, kde=True, ax=axes[0], color='blue', log_scale=(False, True))\n",
    "    axes[0].set_title('Premise Length Distribution')\n",
    "    axes[0].set_xlabel('Number of Words')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "\n",
    "    sns.histplot(hypothesis_lengths, bins=50, kde=True, ax=axes[1], color='green', log_scale=(False, True))\n",
    "    axes[1].set_title('Hypothesis Length Distribution')\n",
    "    axes[1].set_xlabel('Number of Words')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "\n",
    "    fig.suptitle(f'Premise and Hypothesis Length Distribution in {title}')\n",
    "    plt.show()\n",
    "\n",
    "    if compare_length:\n",
    "        # Plot premise vs hypothesis length scatter plot\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(premise_lengths, hypothesis_lengths, alpha=0.5, s=1)\n",
    "        plt.title('Premise vs Hypothesis Length')\n",
    "        plt.xlabel('Premise Length')\n",
    "        plt.ylabel('Hypothesis Length')\n",
    "        plt.yscale('log')\n",
    "        plt.xscale('log')\n",
    "        plt.grid(True, which=\"both\", ls=\"--\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_vocb_distribution():\n",
    "    # Tokenize words\n",
    "    # premise_words = [word for text in premises for word in word_tokenize(text)]\n",
    "    # hypothesis_words = [word for text in hypotheses for word in word_tokenize(text)]\n",
    "\n",
    "    # # Compute word frequencies\n",
    "    # premise_word_freq = Counter(premise_words)\n",
    "    # hypothesis_word_freq = Counter(hypothesis_words)\n",
    "\n",
    "    # # Plotting the most common words\n",
    "    # premise_common_words = premise_word_freq.most_common(20)\n",
    "    # hypothesis_common_words = hypothesis_word_freq.most_common(20)\n",
    "\n",
    "    # fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    # axes[0].barh([word[0] for word in premise_common_words], [word[1] for word in premise_common_words])\n",
    "    # axes[0].set_title('Premise Common Words')\n",
    "    # axes[1].barh([word[0] for word in hypothesis_common_words], [word[1] for word in hypothesis_common_words])\n",
    "    # axes[1].set_title('Hypothesis Common Words')\n",
    "    # plt.show()\n",
    "    ...\n",
    "\n",
    "# Prepare data for plotting\n",
    "# lengths_and_labels = [(length, item['label']) for length, item in zip(premise_lengths, train_dataset)]\n",
    "# premise_df = pd.DataFrame(lengths_and_labels, columns=['length', 'label'])\n",
    "\n",
    "# lengths_and_labels = [(length, item['label']) for length, item in zip(hypothesis_lengths, train_dataset)]\n",
    "# hypothesis_df = pd.DataFrame(lengths_and_labels, columns=['length', 'label'])\n",
    "\n",
    "# # Plot premise lengths across classes\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.boxplot(x='label', y='length', data=premise_df)\n",
    "# plt.title('Premise Length Distribution Across Classes')\n",
    "# plt.show()\n",
    "\n",
    "# # Plot hypothesis lengths across classes\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.boxplot(x='label', y='length', data=hypothesis_df)\n",
    "# plt.title('Hypothesis Length Distribution Across Classes')\n",
    "# plt.show()\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# import numpy as np\n",
    "\n",
    "# # Create combined list of premises and hypotheses\n",
    "# texts = premises + hypotheses\n",
    "\n",
    "# # Compute TF-IDF vectors\n",
    "# tfidf = TfidfVectorizer().fit_transform(texts)\n",
    "\n",
    "# # Compute cosine similarity between each premise and its corresponding hypothesis\n",
    "# cosine_sim = [cosine_similarity(tfidf[i], tfidf[i + len(premises)])[0][0] for i in range(len(premises))]\n",
    "\n",
    "# # Plot cosine similarity distribution\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.hist(cosine_sim, bins=50, alpha=0.5, color='purple')\n",
    "# plt.title('Cosine Similarity Distribution Between Premise and Hypothesis')\n",
    "# plt.xlabel('Cosine Similarity')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics about the regular dataset\n",
    "if print_statistics:\n",
    "  plot_labels_distribution(fever_dataset['train'], title=\"Fever Train Set\")\n",
    "  plot_lengths_distribution(fever_dataset['train'], title=\"Fever Train Set\", compare_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['premise', 'hypothesis'])\n",
      "[{'index': 0, 'rawText': 'Roman'}, {'index': 1, 'rawText': 'Atwood'}, {'index': 2, 'rawText': 'is'}, {'index': 3, 'rawText': 'a'}, {'index': 4, 'rawText': 'content'}, {'index': 5, 'rawText': 'creator'}, {'index': 6, 'rawText': '.'}]\n",
      "[{'tokenIndex': 2, 'verbatlas': {'frameName': 'COPULA', 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [0, 2]}, {'role': 'Attribute', 'score': 1.0, 'span': [3, 6]}]}, 'englishPropbank': {'frameName': 'be.01', 'roles': [{'role': 'ARG1', 'score': 1.0, 'span': [0, 2]}, {'role': 'ARG2', 'score': 1.0, 'span': [3, 6]}]}}]\n"
     ]
    }
   ],
   "source": [
    "#@title Dataset structure\n",
    "\n",
    "# print(fever_dataset['train'][0]['srl'].keys())\n",
    "# print(fever_dataset['train'][0]['srl']['hypothesis']['tokens'])\n",
    "# print(fever_dataset['train'][0]['srl']['hypothesis']['annotations'])\n",
    "\n",
    "# for token in fever_dataset['train'][0]['srl']['premise']['tokens']:\n",
    "#     print(token)\n",
    "\n",
    "# for annotation in fever_dataset['train'][0]['srl']['premise']['annotations']:\n",
    "#     for key, value in annotation.items():\n",
    "#         print(f\"{key}:\\t{value}\")\n",
    "#     print()\n",
    "\n",
    "\n",
    "# WSD exploration\n",
    "\n",
    "# sample_range = 10 #len(fever_dataset['train'])\n",
    "# loop = tqdm(range(sample_range))\n",
    "\n",
    "# pos_info = dict()\n",
    "\n",
    "# for i in loop:\n",
    "\n",
    "#   data =  fever_dataset['train'][i]\n",
    "#   sample_id = data['id']\n",
    "\n",
    "#   hypothesis = data['hypothesis']\n",
    "#   hyp_wsd = data['wsd']['hypothesis']\n",
    "\n",
    "#   for hyp_wsd_dict in hyp_wsd:\n",
    "    \n",
    "#     pos  = hyp_wsd_dict['pos']\n",
    "#     pos_info[pos] = 1 if pos not in pos_info else pos_info[pos] + 1\n",
    "\n",
    "# pos_info\n",
    "\n",
    "# fever_dataset['train'][0:2]['srl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title SRL exploration: collect frame-names and roles \n",
    "\n",
    "def get_srl_info(dataset):\n",
    "\n",
    "  sample_range = len(dataset)\n",
    "  loop = tqdm(range(sample_range))\n",
    "\n",
    "  relevant_info = dict()\n",
    "\n",
    "  set_of_va_frames = dict() # verb atlas frames\n",
    "  set_of_pb_frames = dict() # prop bank frames\n",
    "\n",
    "  verbs_freqs = dict()\n",
    "\n",
    "  set_of_va_roles = set() # verb atlas frames\n",
    "  set_of_pb_roles = set() # prop bank frames\n",
    "\n",
    "  for i in loop:\n",
    "\n",
    "    data =  dataset[i]\n",
    "    sample_id = data['id']\n",
    "    \n",
    "    tokens = data['srl']['premise']['tokens']\n",
    "    annotations = data['srl']['premise']['annotations']\n",
    "\n",
    "    for annotation in annotations:\n",
    "\n",
    "      token_index = annotation['tokenIndex']\n",
    "      verb = tokens[token_index]['rawText']\n",
    "      verbs_freqs[verb] =  1 if verb not in  verbs_freqs else verbs_freqs[verb] + 1\n",
    "\n",
    "      vb_frame = annotation['verbatlas']['frameName']\n",
    "      pb_frame = annotation['englishPropbank']['frameName']\n",
    "\n",
    "      set_of_va_frames[vb_frame] = 1 if vb_frame not in set_of_va_frames else set_of_va_frames[vb_frame] + 1 \n",
    "      set_of_pb_frames[pb_frame] = 1 if pb_frame not in set_of_pb_frames else set_of_pb_frames[pb_frame] + 1 \n",
    "\n",
    "      va_roles = annotation['verbatlas']['roles']\n",
    "      pb_roles = annotation['englishPropbank']['roles']\n",
    "\n",
    "      for role in va_roles:\n",
    "        set_of_va_roles.add(role['role'])\n",
    "\n",
    "      for role in pb_roles:\n",
    "        set_of_pb_roles.add(role['role'])\n",
    "\n",
    "  return set_of_va_frames, set_of_pb_frames, set_of_va_roles, set_of_pb_roles, verbs_freqs\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_info:\n",
    "\n",
    "  set_of_va_frames, set_of_pb_frames, set_of_va_roles, set_of_pb_roles, verbs_freqs = get_srl_info(fever_dataset['train'])\n",
    "\n",
    "  verbs_freqs = sorted(verbs_freqs.items(), key=lambda item: item[1], reverse=True)\n",
    "  set_of_pb_frames = sorted(set_of_pb_frames.items(), key=lambda item: item[1], reverse=True)\n",
    "  set_of_va_frames = sorted(set_of_va_frames.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "  print(f\"Number of Verb Atlas frames: {len(set_of_va_frames)}\")\n",
    "  print(f\"Number of Verb Atlas roles: {len(set_of_va_roles)}\")\n",
    "\n",
    "  print(f\"Number of Propbank frames: {len(set_of_pb_frames)}\")\n",
    "  print(f\"Number of Propbank roles: {len(set_of_pb_roles)}\")\n",
    "\n",
    "  with open(info + \"va_roles.txt\", \"w\") as va_roles:\n",
    "    print(f\"Saving all Verb Atlas roles of the dataset into {info + 'va_roles.txt'}\")\n",
    "    for elem in set_of_va_roles: va_roles.write(f\"{elem}\\n\")\n",
    "\n",
    "  with open(info + \"va_frames.txt\", \"w\") as va_frames:\n",
    "    print(f\"Saving all Verb Atlas frames of the dataset into {info + 'va_frames.txt'}\")\n",
    "    for elem in set_of_va_frames: va_frames.write(f\"{elem}\\n\")\n",
    "\n",
    "\n",
    "  with open(info + \"pb_roles.txt\", \"w\") as pb_roles:\n",
    "    print(f\"Saving all Propbank roles of the dataset into {info + 'pb_roles.txt'}\")\n",
    "    for elem in set_of_pb_roles: pb_roles.write(f\"{elem}\\n\")\n",
    "\n",
    "\n",
    "  with open(info + \"pb_frames.txt\", \"w\") as pb_frames:\n",
    "    print(f\"Saving all Propbank frames of the dataset into {info + 'pb_frames.txt'}\")\n",
    "    for elem in set_of_pb_frames: pb_frames.write(f\"{elem}\\n\")\n",
    "\n",
    "  with open(info + \"verbs_freqs.txt\", \"w\") as verbs:\n",
    "    print(f\"Saving all verbs frequencies count into {info + 'verbs_freqs.txt'}\")\n",
    "    for elem in set_of_pb_frames: verbs.write(f\"{elem}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Augmentation\n",
    "The following prompt can be used to correct the grammar of a modified hypotesis so if just a not is added the phrase is transformed in negative form:\n",
    "\n",
    "\"Correct the grammar in the following inputs, rephrase the input if necessary to make them more accurate. Provide just the correct version, no explanation.\"\n",
    "\n",
    "Ask for:\n",
    "symmetric relation --> Marry\n",
    "antisymmetric relation --> Kill\n",
    "\n",
    "born\n",
    "sell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Augmentation utils\n",
    "\n",
    "def join_strings_smartly(words):\n",
    "    \"\"\" Joins a list of words smartly:\n",
    "    - Adds spaces between words when appropriate.\n",
    "    - Avoids adding spaces before punctuation.\n",
    "    \"\"\"\n",
    "    punctuation = {'.', ',', ';', ':', '!', '?',')'}\n",
    "    result = words[0]\n",
    "    prev = result\n",
    "\n",
    "    for word in words[1:]:\n",
    "      if word in punctuation or \\\n",
    "        (\"'\" in prev) or \\\n",
    "        word.startswith(\"'\") or \\\n",
    "        (\".\" in prev and \".\" in word) or \\\n",
    "        (\"(\" in prev) :\n",
    "        # add word without space before\n",
    "        result += word\n",
    "      else:\n",
    "        # add with space before\n",
    "        result += \" \" + word\n",
    "      # keep track of previous word  \n",
    "      prev = word\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_synset_from_id(synset_id):\n",
    "    if synset_id == 'O':\n",
    "        return None\n",
    "    try:\n",
    "        offset = int(''.join(filter(str.isdigit, synset_id)))\n",
    "        pos = synset_id[-1]\n",
    "        synset = wn.synset_from_pos_and_offset(pos, offset)\n",
    "        return synset\n",
    "    except:\n",
    "        print(\"exception\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "def get_related_word(synset, pos): \n",
    "\n",
    "    info = dict()\n",
    "\n",
    "    # Map POS tags to WordNet POS tags\n",
    "    pos_map = {\n",
    "        'NOUN': wn.NOUN,\n",
    "        'VERB': wn.VERB,\n",
    "        'ADJ': wn.ADJ,\n",
    "        'ADV': wn.ADV\n",
    "    }\n",
    "    \n",
    "    if pos not in pos_map:\n",
    "        return None \n",
    "    \n",
    "    # get hypernyms\n",
    "    hypernyms = synset.hypernyms()\n",
    "    if not hypernyms:\n",
    "        return None\n",
    "    hypernyms = synset.hypernyms()\n",
    "    hypernym_words = set()\n",
    "    for hypernym in hypernyms:\n",
    "        hypernym_words.update(hypernym.lemma_names())\n",
    "    info['hypernyms'] = list(hypernym_words)\n",
    "\n",
    "    # get synonyms \n",
    "    synonyms = synset.lemmas()\n",
    "    if not synonyms:\n",
    "        return None\n",
    "    info['synonyms'] = [synonym.name() for synonym in synonyms if synonym.name() != synset.lemmas()[0].name()]\n",
    "\n",
    "    return info\n",
    "\n",
    "\n",
    "def extract_names(wsd_data):\n",
    "    names = []\n",
    "    current_name = []\n",
    "\n",
    "    for entry in wsd_data:\n",
    "        if entry['pos'] == 'PROPN':\n",
    "            current_name.append(entry['text'])\n",
    "        else:\n",
    "            if current_name:\n",
    "                names.append(' '.join(current_name))\n",
    "                current_name = []\n",
    "\n",
    "    # catch any remaining name at the end\n",
    "    if current_name:\n",
    "        names.append(' '.join(current_name))\n",
    "    \n",
    "    return names\n",
    "\n",
    "\n",
    "def extract_partial_match_name(text, name_list):\n",
    "    # tokenize the input text\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    names = set()\n",
    "    \n",
    "    # tterate through each name in the name list\n",
    "    for name in name_list:\n",
    "        name_parts = name.split()\n",
    "\n",
    "        # check for partial match in the tokenized words\n",
    "        for i in range(len(words) - len(name_parts) + 1):\n",
    "            if words[i:i + len(name_parts)] == name_parts[:len(words[i:i + len(name_parts)])]:\n",
    "              names.add(name)\n",
    "              \n",
    "    if names: return names\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 301.98it/s]\n"
     ]
    }
   ],
   "source": [
    "#@title Augmentation by Synonyms and Hypernyms\n",
    "\n",
    "if syn_hyp_augment:\n",
    "  sample_range = len(fever_dataset['train'])\n",
    "  loop = tqdm(range(sample_range))\n",
    "\n",
    "  new_samples = {\n",
    "      'id': [],\n",
    "      'premise': [],\n",
    "      'hypothesis': [],\n",
    "      'label': [],\n",
    "      'wsd': [None for i in range(sample_range)],\n",
    "      'srl': [None for i in range(sample_range)]\n",
    "  }\n",
    "\n",
    "  syn_dict = dict()\n",
    "  syn_idx = 0\n",
    "  progressive_id = int(max_id)\n",
    "\n",
    "  for i in loop:\n",
    "\n",
    "    data =  fever_dataset['train'][i]\n",
    "    sample_id = data['id']\n",
    "\n",
    "    premise = data['premise']\n",
    "    label = data['label']\n",
    "    hyp_wsd = data['wsd']['hypothesis']\n",
    "\n",
    "    new_samples['id'].append(str(progressive_id))\n",
    "    new_samples['premise'].append(premise)\n",
    "    new_samples['label'].append(label)\n",
    "    progressive_id += 1\n",
    "\n",
    "    hypothesis = data['hypothesis']\n",
    "    new_hypotesys = []\n",
    "\n",
    "    for hyp_wsd_dict in hyp_wsd:\n",
    "\n",
    "      word = hyp_wsd_dict['text']\n",
    "      pos = hyp_wsd_dict['pos']\n",
    "      offset = hyp_wsd_dict['wnSynsetOffset']\n",
    "      synset = get_synset_from_id(offset)\n",
    "\n",
    "      related_words = None\n",
    "      if synset: related_words = get_related_word(synset, pos)\n",
    "\n",
    "      if related_words:\n",
    "        hypernyms = related_words['hypernyms']\n",
    "        synonyms = related_words['synonyms']\n",
    "\n",
    "        if not synonyms : \n",
    "          new_hypotesys.append(word)\n",
    "          continue\n",
    "\n",
    "        synonym = synonyms[syn_idx % (len(synonyms))]\n",
    "\n",
    "        if synonym in syn_dict and syn_dict[synonym] > 10:\n",
    "          syn_idx += 1\n",
    "          synonym = synonyms[syn_idx % (len(synonyms))]\n",
    "\n",
    "        syn_dict[synonym] = 1 if  synonym not in syn_dict else syn_dict[synonym] + 1\n",
    "        # if synonym in syn_dict \n",
    "\n",
    "        syn_idx += 1\n",
    "\n",
    "        if '_' in synonym: synonym = synonym.replace('_', ' ')\n",
    "        new_hypotesys.append(synonym)\n",
    "\n",
    "      else: new_hypotesys.append(word)\n",
    "\n",
    "\n",
    "    new_hypotesys = join_strings_smartly(new_hypotesys)\n",
    "    new_samples['hypothesis'].append(new_hypotesys)\n",
    "    # print(f\"new: {new_hypotesys} \\n\")\n",
    "\n",
    "  # print(new_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "Augmented dataset type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "Train split length: 51086\n",
      "Train split augmentated: 51096\n"
     ]
    }
   ],
   "source": [
    "#@title Add new samples to the original dataset\n",
    " \n",
    "augmentation = Dataset.from_dict(new_samples)\n",
    "print(f\"Augmentation type: {type(augmentation)}\")\n",
    "\n",
    "augmented_dataset = concatenate_datasets([fever_dataset['train'], augmentation])\n",
    "print(f\"Augmented dataset type: {type(augmented_dataset)}\")\n",
    "print(f\"Train split length: {len(fever_dataset['train'])}\")\n",
    "print(f\"Train split augmentated: {len(augmented_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John Wick: Chapter 2 . The film premiered in Los Angeles on January 30 , 2017 , and was theatrically released in the United States on February 10 , 2017 .\n",
      "[{'index': 0, 'rawText': 'John'}, {'index': 1, 'rawText': 'Wick'}, {'index': 2, 'rawText': ':'}, {'index': 3, 'rawText': 'Chapter'}, {'index': 4, 'rawText': '2'}, {'index': 5, 'rawText': '.'}, {'index': 6, 'rawText': 'The'}, {'index': 7, 'rawText': 'film'}, {'index': 8, 'rawText': 'premiered'}, {'index': 9, 'rawText': 'in'}, {'index': 10, 'rawText': 'Los'}, {'index': 11, 'rawText': 'Angeles'}, {'index': 12, 'rawText': 'on'}, {'index': 13, 'rawText': 'January'}, {'index': 14, 'rawText': '30'}, {'index': 15, 'rawText': ','}, {'index': 16, 'rawText': '2017'}, {'index': 17, 'rawText': ','}, {'index': 18, 'rawText': 'and'}, {'index': 19, 'rawText': 'was'}, {'index': 20, 'rawText': 'theatrically'}, {'index': 21, 'rawText': 'released'}, {'index': 22, 'rawText': 'in'}, {'index': 23, 'rawText': 'the'}, {'index': 24, 'rawText': 'United'}, {'index': 25, 'rawText': 'States'}, {'index': 26, 'rawText': 'on'}, {'index': 27, 'rawText': 'February'}, {'index': 28, 'rawText': '10'}, {'index': 29, 'rawText': ','}, {'index': 30, 'rawText': '2017'}, {'index': 31, 'rawText': '.'}]\n",
      "{'frameName': 'SHOW', 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [6, 8]}, {'role': 'Location', 'score': 1.0, 'span': [9, 12]}, {'role': 'Time', 'score': 1.0, 'span': [12, 17]}]}\n",
      "John Wick: Chapter 2 was theatrically released in the Oregon.\n",
      "John Wick: Chapter 2 . The film premiered in Los Angeles on January 30 , 2017 , and was theatrically released in the United States on February 10 , 2017 .\n",
      "[{'index': 0, 'rawText': 'John'}, {'index': 1, 'rawText': 'Wick'}, {'index': 2, 'rawText': ':'}, {'index': 3, 'rawText': 'Chapter'}, {'index': 4, 'rawText': '2'}, {'index': 5, 'rawText': '.'}, {'index': 6, 'rawText': 'The'}, {'index': 7, 'rawText': 'film'}, {'index': 8, 'rawText': 'premiered'}, {'index': 9, 'rawText': 'in'}, {'index': 10, 'rawText': 'Los'}, {'index': 11, 'rawText': 'Angeles'}, {'index': 12, 'rawText': 'on'}, {'index': 13, 'rawText': 'January'}, {'index': 14, 'rawText': '30'}, {'index': 15, 'rawText': ','}, {'index': 16, 'rawText': '2017'}, {'index': 17, 'rawText': ','}, {'index': 18, 'rawText': 'and'}, {'index': 19, 'rawText': 'was'}, {'index': 20, 'rawText': 'theatrically'}, {'index': 21, 'rawText': 'released'}, {'index': 22, 'rawText': 'in'}, {'index': 23, 'rawText': 'the'}, {'index': 24, 'rawText': 'United'}, {'index': 25, 'rawText': 'States'}, {'index': 26, 'rawText': 'on'}, {'index': 27, 'rawText': 'February'}, {'index': 28, 'rawText': '10'}, {'index': 29, 'rawText': ','}, {'index': 30, 'rawText': '2017'}, {'index': 31, 'rawText': '.'}]\n",
      "{'frameName': 'LIBERATE_ALLOW_AFFORD', 'roles': [{'role': 'Patient', 'score': 1.0, 'span': [6, 8]}, {'role': 'Attribute', 'score': 1.0, 'span': [20, 21]}, {'role': 'Location', 'score': 1.0, 'span': [22, 26]}, {'role': 'Time', 'score': 1.0, 'span': [26, 31]}]}\n",
      "John Wick: Chapter 2 was theatrically released in the Oregon.\n"
     ]
    }
   ],
   "source": [
    "sample_range = 10 \n",
    "# sample_range = len(fever_dataset['train'])\n",
    "\n",
    "locations = set()\n",
    "# loop = tqdm(range(sample_range))\n",
    "\n",
    "# for i in loop:\n",
    "for i in range(sample_range):\n",
    "\n",
    "    data = fever_dataset['train'][i]\n",
    "\n",
    "    # id\n",
    "    id =  data['id']\n",
    "\n",
    "    # premise\n",
    "    sentence = data['premise']\n",
    "    hypothesis = data['hypothesis']\n",
    "\n",
    "    # wsd\n",
    "    wsd =  data['wsd']\n",
    "\n",
    "    # srl\n",
    "    possible_locations = []\n",
    "    tokens = data['srl']['premise']['tokens']\n",
    "    annotations = data['srl']['premise']['annotations']\n",
    "\n",
    "    # print(tokens)\n",
    "    # print(annotations)\n",
    "\n",
    "    for annotation in annotations:\n",
    "        # print(annotation['verbatlas']['roles'])\n",
    "        token_idx = annotation['tokenIndex'] \n",
    "        roles = annotation['verbatlas']['roles']\n",
    "        # index = annotation['verbatlas']\n",
    "\n",
    "        location = False\n",
    "        agent = False\n",
    "\n",
    "        # print(index)\n",
    "        text_location = \"\"\n",
    "        for role in roles:\n",
    "            if role['role'] == 'Location':\n",
    "                span = role['span']\n",
    "\n",
    "                for item in tokens:\n",
    "                    if span[0] <= item['index'] <= span[1]:\n",
    "                        text_location += item['rawText'] + \" \"\n",
    "\n",
    "                # filtered_texts = [item['rawText'] for item in tokens if span[0] <= item['index'] <= span[1]]\n",
    "                # text_location = \" \".join(filtered_texts)\n",
    "                # filtered_words = [word for word in filtered_texts if any(char.isupper() for char in word)]\n",
    "\n",
    "                if text_location:\n",
    "                    # possible_locations.append(text_location)\n",
    "                    # print(f\"sample id: {id}\")\n",
    "                    # print(f\"text: {text_location}\")\n",
    "\n",
    "                    entities =  nlp_spacy(text_location).ents\n",
    "                    # print(f\"entities: {entities}\")\n",
    "                    if entities:\n",
    "                        found_target = False\n",
    "                        for ent in entities:\n",
    "                            if ent.label_ == \"GPE\":\n",
    "                                found_target = True\n",
    "                                locations.add(ent.text)\n",
    "\n",
    "                                # print info related to location\n",
    "                                print(sentence)\n",
    "                                print(tokens)\n",
    "                                print(annotation['verbatlas'])\n",
    "                                print(hypothesis)\n",
    "                                # print(f\"target entity: {ent.text}\")\n",
    "                            # else:\n",
    "                            #     print(\"no target \")\n",
    "                        # if not found_target :  print(\"no location target \")\n",
    "                    else:\n",
    "                        # print(\"no target \")\n",
    "                        ...\n",
    "                    \n",
    "                    # print(\"----------------------------------------- \\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 51086/51086 [01:12<00:00, 700.84 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# add new samples to the dataset\n",
    "filtered_dataset = fever_dataset['train'].filter(lambda example: 'is married to ' in example['premise'].lower()) # or 'is a' in example['hypothesis'].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ad-Rock . He is married to musician and feminist activist Kathleen Hanna .',\n",
       " 'Claire Danes . She is married to actor Hugh Dancy , with whom she has one child .',\n",
       " \"Claire Danes . She is married to actor Hugh Dancy , with whom she has one child . Hugh Michael Horace Dancy ( born 19 June 1975 ) is an English actor and model . He is best known for his roles as Will Graham in the television series Hannibal for which he was nominated for two Critics ' Choice Television Award for Best Actor in a Drama Series and as Prince Charmont in Ella Enchanted . In 2006 , he was nominated for a Primetime Emmy Award for his portrayal of the Earl of Essex in the Channel 4 miniseries Elizabeth I.\",\n",
       " \"Claire Danes . She is married to actor Hugh Dancy , with whom she has one child . Hugh Dancy . He is best known for his roles as Will Graham in the television series Hannibal for which he was nominated for two Critics ' Choice Television Award for Best Actor in a Drama Series and as Prince Charmont in Ella Enchanted .\",\n",
       " 'Claire Danes . She is married to actor Hugh Dancy , with whom she has one child . Hugh Michael Horace Dancy ( born 19 June 1975 ) is an English actor and model .',\n",
       " 'Ad-Rock . He is married to musician and feminist activist Kathleen Hanna . Kathleen Hanna ( born November 12 , 1968 ) is an American singer , musician , artist , feminist activist , pioneer of the feminist punk riot grrrl movement , and punk zine writer .',\n",
       " 'Andy Roddick . He is married to Brooklyn Decker , a former Sports Illustrated swimwear model and actress .',\n",
       " 'Emma Thompson ( born 15 April 1959 ) is a British actress , activist , author , comedienne and screenwriter . Born in London to English actor Eric Thompson and Scottish actress Phyllida Law , Thompson was educated at Newnham College , University of Cambridge , where she became a member of the Footlights troupe . Thompson is married to actor Greg Wise , with whom she lives in London .',\n",
       " 'Claire Danes . She is married to actor Hugh Dancy , with whom she has one child .',\n",
       " 'Ellen DeGeneres . In 2008 , she married her longtime girlfriend Portia de Rossi . Portia Lee James DeGeneres ( born Amanda Lee Rogers ; January 31 , 1973 ) , also known professionally as Portia de Rossi ( [ ˈpɔərʃə_də_ˈrɒsi ] ) , is an Australian and American actress , model , and philanthropist . De Rossi is married to comedian and television host Ellen DeGeneres .',\n",
       " 'Régine Chassagne ( [ ʁeˈʒin ʃaˈsaːɲ ] ; born 19 August 1976 ) is a Canadian multi-instrumentalist musician , singer and songwriter , and is a founding member of the band Arcade Fire . She is married to co-founder Win Butler .',\n",
       " 'Claire Danes . She is married to actor Hugh Dancy , with whom she has one child .',\n",
       " \"William Jefferson Clinton ( born August 19 , 1946 ) is an American politician who served as the 42nd President of the United States from 1993 to 2001 . Prior to the Presidency he was the 40th Governor of Arkansas from 1979 to 1981 and the state 's 42nd Governor from 1983 to 1992 . Clinton is married to Hillary Rodham Clinton , who served as United States Secretary of State from 2009 to 2013 and U.S. Senator from New York from 2001 to 2009 , and was the Democratic nominee for President in 2016 .\",\n",
       " 'Michelle Obama . She is married to the 44th President of the United States , Barack Obama , and is the first African-American First Lady . Barack and Michelle married in 1992 and have two daughters .',\n",
       " 'Paul Bettany . Bettany is married to American actress Jennifer Connelly , with whom he has two children .',\n",
       " 'Michelle Obama . She is married to the 44th President of the United States , Barack Obama , and is the first African-American First Lady .',\n",
       " 'Claire Danes . She is married to actor Hugh Dancy , with whom she has one child . Germain Houde ( born December 14 , 1952 ) is a Quebec Genie Award-winning actor . He has won two Genie Award for Best Performance by an Actor in a Supporting Role for his roles in Les bons débarras and Un Zoo la Nuit .',\n",
       " 'Michelle Obama . She is married to the 44th President of the United States , Barack Obama , and is the first African-American First Lady . She returned to speak at the 2012 Democratic National Convention , and again during the 2016 Democratic National Convention in Philadelphia , where she delivered a speech in support of the Democratic Presidential nominee , and fellow First Lady , Hillary Clinton .',\n",
       " 'Claire Danes . She is married to actor Hugh Dancy , with whom she has one child .',\n",
       " 'Ajay Devgn . He is widely considered as one of the most popular actors of Hindi cinema , who has appeared in over a hundred Hindi films . Devgan has won numerous accolades , including two National Film Awards and four Filmfare Awards . He is married to film actress , Kajol since 1999 and the couple have two children .',\n",
       " 'Caroline, Princess of Hanover . Caroline is married to Ernst August , Prince of Hanover ( born 1954 ) , the heir to the former throne of the Kingdom of Hanover , as well as the heir male of George III of the United Kingdom .',\n",
       " 'Claire Danes . She is married to actor Hugh Dancy , with whom she has one child .',\n",
       " 'Andy Roddick . He is married to Brooklyn Decker , a former Sports Illustrated swimwear model and actress .',\n",
       " \"First Lady of the United States ( FLOTUS ) is the informal but accepted title held by the wife of the President of the United States , concurrent with the president 's term of office . Michelle Obama . She is married to the 44th President of the United States , Barack Obama , and is the first African-American First Lady . Barack and Michelle married in 1992 and have two daughters .\",\n",
       " \"First Lady of the United States ( FLOTUS ) is the informal but accepted title held by the wife of the President of the United States , concurrent with the president 's term of office . Michelle Obama . She is married to the 44th President of the United States , Barack Obama , and is the first African-American First Lady . Raised on the South Side of Chicago , Illinois , Obama is a graduate of Princeton University and Harvard Law School , and spent her early legal career working at the law firm Sidley Austin , where she met her husband .\",\n",
       " \"Jared Kushner . Kushner is the elder son of real-estate developer Charles Kushner and is married to Trump 's daughter Ivanka .\",\n",
       " 'Régine Chassagne ( [ ʁeˈʒin ʃaˈsaːɲ ] ; born 19 August 1976 ) is a Canadian multi-instrumentalist musician , singer and songwriter , and is a founding member of the band Arcade Fire . She is married to co-founder Win Butler .',\n",
       " 'Ad-Rock . He is married to musician and feminist activist Kathleen Hanna .',\n",
       " \"Jennifer Connelly . She gained critical acclaim for her work in the 1998 science fiction film Dark City and for her portrayal of Marion Silver in the 2000 drama Requiem for a Dream . In 2012 , she was named the first global face of the Shiseido Company . Magazines including Time , Vanity Fair and Esquire , as well as the Los Angeles Times newspaper have included her on their lists of the world 's most beautiful women . Paul Bettany . Bettany is married to American actress Jennifer Connelly , with whom he has two children .\",\n",
       " 'Caroline , Princess of Hanover ( Caroline Louise Marguerite ; born 23 January 1957 ) , is the eldest child of Rainier III , Prince of Monaco , and American actress Grace Kelly . She is the elder sister of Prince Albert II and Princess Stéphanie . Caroline is married to Ernst August , Prince of Hanover ( born 1954 ) , the heir to the former throne of the Kingdom of Hanover , as well as the heir male of George III of the United Kingdom .',\n",
       " 'Andy Roddick . He is married to Brooklyn Decker , a former Sports Illustrated swimwear model and actress .',\n",
       " 'Claire Danes . She is married to actor Hugh Dancy , with whom she has one child . Hugh Michael Horace Dancy ( born 19 June 1975 ) is an English actor and model .',\n",
       " 'Andy Roddick . He is married to Brooklyn Decker , a former Sports Illustrated swimwear model and actress . Brooklyn Danielle Decker Roddick ( born April 12 , 1987 ) is an American fashion model and actress best known for her appearances in the Sports Illustrated Swimsuit Issue , including the cover of the 2010 issue .',\n",
       " 'AC/DC are an Australian rock band , formed in 1973 by brothers Malcolm and Angus Young . Malcolm Mitchell Young ( born 6 January 1953 ) is an Australian retired musician and songwriter , best known as a co-founder , rhythm guitarist , backing vocalist and songwriter for the hard rock band AC/DC . He is married to Linda Young and has two children , Cara and Ross .',\n",
       " 'Ellen DeGeneres . In 2008 , she married her longtime girlfriend Portia de Rossi . Portia Lee James DeGeneres ( born Amanda Lee Rogers ; January 31 , 1973 ) , also known professionally as Portia de Rossi ( [ ˈpɔərʃə_də_ˈrɒsi ] ) , is an Australian and American actress , model , and philanthropist . De Rossi is married to comedian and television host Ellen DeGeneres .',\n",
       " 'Michelle Obama . She is married to the 44th President of the United States , Barack Obama , and is the first African-American First Lady .',\n",
       " 'Michelle Obama . She is married to the 44th President of the United States , Barack Obama , and is the first African-American First Lady . As First Lady , Obama became a fashion icon , a role model for women , and an advocate for poverty awareness , nutrition , physical activity , and healthy eating .',\n",
       " 'Caroline, Princess of Hanover . Caroline is married to Ernst August , Prince of Hanover ( born 1954 ) , the heir to the former throne of the Kingdom of Hanover , as well as the heir male of George III of the United Kingdom .',\n",
       " 'Michelle Obama . She is married to the 44th President of the United States , Barack Obama , and is the first African-American First Lady .',\n",
       " 'Paul Bettany . Bettany is married to American actress Jennifer Connelly , with whom he has two children .',\n",
       " 'Emma Thompson ( born 15 April 1959 ) is a British actress , activist , author , comedienne and screenwriter . Born in London to English actor Eric Thompson and Scottish actress Phyllida Law , Thompson was educated at Newnham College , University of Cambridge , where she became a member of the Footlights troupe . Her first film role was in the 1989 romantic comedy The Tall Guy , and in the early 1990s she frequently collaborated with her then husband , actor and director Kenneth Branagh . Thompson is married to actor Greg Wise , with whom she lives in London . They have one daughter and an adopted son .',\n",
       " 'Paul Bettany . Bettany is married to American actress Jennifer Connelly , with whom he has two children .',\n",
       " 'Andy Roddick . He is married to Brooklyn Decker , a former Sports Illustrated swimwear model and actress . Brooklyn Danielle Decker Roddick ( born April 12 , 1987 ) is an American fashion model and actress best known for her appearances in the Sports Illustrated Swimsuit Issue , including the cover of the 2010 issue .',\n",
       " 'Claire Danes . She is married to actor Hugh Dancy , with whom she has one child .',\n",
       " 'Ad-Rock . He is married to musician and feminist activist Kathleen Hanna . Kathleen Hanna ( born November 12 , 1968 ) is an American singer , musician , artist , feminist activist , pioneer of the feminist punk riot grrrl movement , and punk zine writer .',\n",
       " 'Emma Thompson . Her first film role was in the 1989 romantic comedy The Tall Guy , and in the early 1990s she frequently collaborated with her then husband , actor and director Kenneth Branagh . Thompson is married to actor Greg Wise , with whom she lives in London .',\n",
       " 'Emma Thompson . Thompson is married to actor Greg Wise , with whom she lives in London .',\n",
       " 'Michelle Obama . She is married to the 44th President of the United States , Barack Obama , and is the first African-American First Lady . Barack and Michelle married in 1992 and have two daughters .',\n",
       " 'Caroline, Princess of Hanover . Caroline is married to Ernst August , Prince of Hanover ( born 1954 ) , the heir to the former throne of the Kingdom of Hanover , as well as the heir male of George III of the United Kingdom .',\n",
       " 'Caroline , Princess of Hanover ( Caroline Louise Marguerite ; born 23 January 1957 ) , is the eldest child of Rainier III , Prince of Monaco , and American actress Grace Kelly . Caroline is married to Ernst August , Prince of Hanover ( born 1954 ) , the heir to the former throne of the Kingdom of Hanover , as well as the heir male of George III of the United Kingdom .',\n",
       " \"Gone with the Wind (film) . Set in the American South against the backdrop of the American Civil War and Reconstruction era , the film tells the story of Scarlett O'Hara , the strong-willed daughter of a Georgia plantation owner , from her romantic pursuit of Ashley Wilkes , who is married to his cousin , Melanie Hamilton , to her marriage to Rhett Butler .\",\n",
       " 'Andy Roddick . He is married to Brooklyn Decker , a former Sports Illustrated swimwear model and actress .',\n",
       " \"Jared Corey Kushner ( born January 10 , 1981 ) is an American real estate investor and developer , newspaper owner , and senior advisor to President Donald Trump . Kushner is the elder son of real-estate developer Charles Kushner and is married to Trump 's daughter Ivanka .\",\n",
       " 'Caroline , Princess of Hanover ( Caroline Louise Marguerite ; born 23 January 1957 ) , is the eldest child of Rainier III , Prince of Monaco , and American actress Grace Kelly . Caroline is married to Ernst August , Prince of Hanover ( born 1954 ) , the heir to the former throne of the Kingdom of Hanover , as well as the heir male of George III of the United Kingdom .',\n",
       " 'Caroline, Princess of Hanover . Caroline is married to Ernst August , Prince of Hanover ( born 1954 ) , the heir to the former throne of the Kingdom of Hanover , as well as the heir male of George III of the United Kingdom .',\n",
       " 'Emma Thompson . Her first film role was in the 1989 romantic comedy The Tall Guy , and in the early 1990s she frequently collaborated with her then husband , actor and director Kenneth Branagh . Thompson is married to actor Greg Wise , with whom she lives in London .',\n",
       " 'Michelle LaVaughn Robinson Obama ( born January 17 , 1964 ) is an American lawyer and writer who was First Lady of the United States from 2009 to 2017 . She is married to the 44th President of the United States , Barack Obama , and is the first African-American First Lady . Raised on the South Side of Chicago , Illinois , Obama is a graduate of Princeton University and Harvard Law School , and spent her early legal career working at the law firm Sidley Austin , where she met her husband .',\n",
       " 'Ad-Rock . He is married to musician and feminist activist Kathleen Hanna . Kathleen Hanna ( born November 12 , 1968 ) is an American singer , musician , artist , feminist activist , pioneer of the feminist punk riot grrrl movement , and punk zine writer .',\n",
       " 'Claire Danes . She is married to actor Hugh Dancy , with whom she has one child .',\n",
       " \"Gone with the Wind (film) . Set in the American South against the backdrop of the American Civil War and Reconstruction era , the film tells the story of Scarlett O'Hara , the strong-willed daughter of a Georgia plantation owner , from her romantic pursuit of Ashley Wilkes , who is married to his cousin , Melanie Hamilton , to her marriage to Rhett Butler . Filming was delayed for two years due to Selznick 's determination to secure Gable for the role of Rhett Butler , and the `` search for Scarlett '' led to 1,400 women being interviewed for the part . The film was immensely popular , becoming the highest-earning film made up to that point , and retained the record for over a quarter of a century .\",\n",
       " 'Paul Bettany . Bettany is married to American actress Jennifer Connelly , with whom he has two children .',\n",
       " 'Claire Danes . She is married to actor Hugh Dancy , with whom she has one child .',\n",
       " 'David Beckham . He has been married to Victoria Beckham since 1999 and they have four children . Victoria Beckham . She is married to David Beckham , and they have four children .']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dataset['premise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_dataset['premise'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': [{'index': 0, 'rawText': 'Ad'},\n",
       "  {'index': 1, 'rawText': '-'},\n",
       "  {'index': 2, 'rawText': 'Rock'},\n",
       "  {'index': 3, 'rawText': '.'},\n",
       "  {'index': 4, 'rawText': 'He'},\n",
       "  {'index': 5, 'rawText': 'is'},\n",
       "  {'index': 6, 'rawText': 'married'},\n",
       "  {'index': 7, 'rawText': 'to'},\n",
       "  {'index': 8, 'rawText': 'musician'},\n",
       "  {'index': 9, 'rawText': 'and'},\n",
       "  {'index': 10, 'rawText': 'feminist'},\n",
       "  {'index': 11, 'rawText': 'activist'},\n",
       "  {'index': 12, 'rawText': 'Kathleen'},\n",
       "  {'index': 13, 'rawText': 'Hanna'},\n",
       "  {'index': 14, 'rawText': '.'}],\n",
       " 'annotations': [{'tokenIndex': 1,\n",
       "   'verbatlas': {'frameName': 'AUXILIARY', 'roles': []},\n",
       "   'englishPropbank': {'frameName': 'be.03', 'roles': []}},\n",
       "  {'tokenIndex': 5,\n",
       "   'verbatlas': {'frameName': 'COPULA',\n",
       "    'roles': [{'role': 'Theme', 'score': 1.0, 'span': [4, 5]},\n",
       "     {'role': 'Attribute', 'score': 1.0, 'span': [6, 14]}]},\n",
       "   'englishPropbank': {'frameName': 'be.01',\n",
       "    'roles': [{'role': 'ARG1', 'score': 1.0, 'span': [4, 5]},\n",
       "     {'role': 'ARG2', 'score': 1.0, 'span': [6, 14]}]}}]}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dataset[0]['srl']['premise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/51086 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/51086 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'extract_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# wsd info\u001b[39;00m\n\u001b[1;32m     24\u001b[0m wsd \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwsd\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpremise\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 25\u001b[0m proper_nouns \u001b[38;5;241m=\u001b[39m \u001b[43mextract_names\u001b[49m(wsd)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# print(proper_nouns)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m annotation \u001b[38;5;129;01min\u001b[39;00m annotations:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'extract_names' is not defined"
     ]
    }
   ],
   "source": [
    "# dataset = filtered_dataset\n",
    "dataset = fever_dataset['train']\n",
    "\n",
    "sample_range = len(dataset)\n",
    "loop = tqdm(range(sample_range))\n",
    "\n",
    "relational_graph = dict()\n",
    "old_max_span = 1_000_000\n",
    "\n",
    "for i in loop:\n",
    "\n",
    "  data =  dataset[i]\n",
    "  sample_id = data['id']\n",
    "  premise = data['premise']\n",
    "  hypothesis = data['hypothesis']\n",
    "\n",
    "  # print(f\"premise: {premise}\")\n",
    "  \n",
    "  # srl info\n",
    "  tokens = data['srl']['premise']['tokens']\n",
    "  annotations = data['srl']['premise']['annotations']\n",
    "\n",
    "  # wsd info\n",
    "  wsd = data['wsd']['premise']\n",
    "  proper_nouns = extract_names(wsd)\n",
    "  # print(proper_nouns)\n",
    "\n",
    "  for annotation in annotations:\n",
    "\n",
    "    token_index = annotation['tokenIndex']\n",
    "    verb = tokens[token_index]['rawText']\n",
    "\n",
    "    # frame = annotation['verbatlas']['frameName']\n",
    "    frame = annotation['englishPropbank']['frameName']\n",
    "    # relational_graph[pb_frame]\n",
    "\n",
    "    # if frame not in  ['ALLY_ASSOCIATE_MARRY']: continue\n",
    "    if frame not in  ['marry.01']: continue\n",
    "    if frame not in relational_graph: relational_graph[frame] = dict()\n",
    "    print(premise)\n",
    "\n",
    "    # roles = annotation['verbatlas']['roles']\n",
    "    roles = annotation['englishPropbank']['roles']\n",
    "\n",
    "    try:\n",
    "      span_begin = roles[0]['span'][0]\n",
    "      span_end = roles[-1]['span'][1]\n",
    "    except:\n",
    "      # print(roles)\n",
    "      continue\n",
    "\n",
    "    if span_begin > 0 and span_begin < old_max_span: \n",
    "      span_begin = 0\n",
    "      old_max_span = span_end\n",
    "\n",
    "    sentence = \"\"\n",
    "    try: \n",
    "      sentence = \" \".join([tokens[index]['rawText']  for index in range(span_begin, span_end + 1)]) # if tokens[index]['rawText'] in names])\n",
    "    except:\n",
    "      sentence = \" \".join([tokens[index]['rawText'] for index in range(span_begin, span_end)]) #if tokens[index]['rawText'] in names])\n",
    "\n",
    "    # print(sentence)\n",
    "    target = extract_partial_match_name(sentence, proper_nouns)\n",
    "    # print(target)\n",
    "    if not target: continue\n",
    "\n",
    "    for elem in target:\n",
    "      if elem not in relational_graph[frame]: relational_graph[frame][elem] = [sample_id]\n",
    "      else: relational_graph[frame][elem].append(sample_id)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # srl_roles = []\n",
    "    # for role in roles:\n",
    "    #   # print(role)\n",
    "\n",
    "    #   span = role['span']\n",
    "    #   role_tag = role['role']\n",
    "\n",
    "    #   # print(role_tag)\n",
    "    #   if role_tag not in accetable_roles : continue\n",
    "\n",
    "    #   curr_roles = \"\"\n",
    "    #   try: \n",
    "    #     curr_roles = \" \".join([tokens[index]['rawText']  for index in range(span[0], span[1] + 1)]) # if tokens[index]['rawText'] in names])\n",
    "    #   except:\n",
    "    #     curr_roles = \" \".join([tokens[index]['rawText'] for index in range(span[0], span[1])]) #if tokens[index]['rawText'] in names])\n",
    "\n",
    "    #   # print(names)\n",
    "    #   # print(curr_roles)\n",
    "    #   target = extract_partial_match_name(curr_roles, proper_nouns)\n",
    "\n",
    "    #   if target: \n",
    "    #     print(premise)\n",
    "    #     print(target)\n",
    "    #     print(role_tag)\n",
    "\n",
    "    #     if 'Agent' in role_tag:\n",
    "    #       if target not in relational_graph:\n",
    "    #         relational_graph[target] = [(frame, 1)]\n",
    "        \n",
    "    #     else:\n",
    "    #       if frame not in relational_graph:\n",
    "    #         relational_graph[frame] = [(target, sample_id)]\n",
    "\n",
    "    #     # print(names)\n",
    "    #     # print(wsd)\n",
    "    #     # print(tokens)\n",
    "    #     print(annotation['verbatlas'])\n",
    "\n",
    "    #     # print(target)\n",
    "    #     # srl_roles.append((curr_roles, role_tag))\n",
    "    #   else: continue\n",
    "\n",
    "\n",
    "      # if frame not in relational_graph:\n",
    "      #   relational_graph[frame] = [(curr_roles, role_tag, sample_id)]\n",
    "      # else:\n",
    "      #   relational_graph[frame].append((curr_roles, role_tag, sample_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_dict(relational_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rachel', 'Hathaway', 'Anne Hathaway', 'Wyatt Earp', 'Urilla Sutherland Earp', 'Earp', 'Millionaire', 'Peggy Sue Got', 'Paul McCartney', 'Children', 'Beatles', 'Jennifer Garner', 'Garner', 'Scott Foley', 'Ellen Pompeo', 'Chris Ivery', 'Pompeo', 'Harald', 'Harald V', 'Sonja Haraldsen', 'Norway', 'Germany', 'King Henry', 'Holy Roman Emperor Henry V', 'England', 'Empress Matilda', 'Brigham Young University', 'Mitt Romney', 'Ann Romney', 'BYU', 'Ellen DeGeneres', 'Portia de Rossi', 'Brad Pitt', 'Jennifer Aniston', 'Justin Theroux', 'Elizabeth Swann', 'World', 'Lady McCartney', 'Linda Louise', 'September', 'April', 'Ben Affleck', 'Joan Crawford', 'Crawford', 'George VI', 'Lyon', 'Elizabeth', 'Lady Elizabeth Bowes', 'Greece', 'Wales', 'Prince', 'Charles', 'Edinburgh', 'Duke', 'Philip', 'Elizabeth II', 'Denmark', 'Christie Brinkley', 'Billy Joel', 'Brinkley', 'Fearless Vampire Killers', 'August', 'Sharon Marie Tate Polanski', 'Tate', 'Roman Polanski', 'January', 'Peggy Sue', 'Chairman Alfred Steele', 'Alfred Steele', 'Cola Company', 'Pepsi', 'Peter Andre', 'Katie Price', 'Alex Reid', 'Kieran Hayler', 'Henrietta Maria', 'Bourbon', 'France', 'New York', 'Christopher', 'Bad Boy', 'Wallace', 'Obama', 'Michelle Obama', 'Anne Bancroft', 'Oscar', 'Brooks', 'Aristotle Onassis', 'Patti Scialfa', 'Bruce Springsteen', 'Ann', 'Ann Davies', 'Polanski', 'Sharon Tate', 'Triple H', 'H', 'McMahon', 'Caitlyn', 'Robert', 'Olympic Games', 'Kris Jenner', 'Bruce Jenner', 'Jenner', 'Robert Kardashian', 'Kardashian', 'Boldt', 'Benjamin Geza Affleck', 'Wings', 'Linda', 'Denny Laine', 'Mamie', 'Mamie Doud', 'West Point', 'Dwight Eisenhower', 'Matilda', 'Faith Evans', 'Evans', 'Princess', 'November', 'Prince Rainier III', 'Monaco', 'Patricia Kelly', 'Eleanor Roosevelt', 'Tommy Lee', 'Anderson', 'Pamela Lee', 'Pamela Anderson Lee', 'King Henry III', 'Valois', 'Navarre', 'Margaret', 'Charles IX', 'Judith', 'Susanna', 'Hamnet', 'Mob', 'Patriots', 'New England Patriots', 'Tom Brady', 'Brady', 'Gisele', 'Diana Prince', 'South America', 'Army', 'Sarah', 'Keturah', 'Abraham', 'Book', 'Genesis', 'America', 'Kelly', 'Paul', 'Columbia Law School', 'Harvard College', 'World War II', 'Rye', 'Lady Diana Spencer', 'Edward', 'Furnish', 'David Furnish', 'John', 'Elton John', 'December', 'John McCain', 'Linda McCartney', 'Monroe', 'Marilyn Monroe', 'Los Angeles', 'Henry III', 'Cindy McCain', 'Rachel Donelson Robards', 'Kanye West', 'Kim Kardashian', 'Natalie Wood', 'Robert Wagner', 'Golden Globe Award', 'Eye', 'Dolls', 'Valley', 'Devil', 'Jennifer North', 'King', 'Henry', 'Abhishek Bachchan', 'Rai', 'Barack Obama', 'American First Lady', 'United States', 'President', 'Presley', 'Michael Jackson', 'Nicolas Cage', 'Lisa Marie Presley', 'Michael Lockwood', 'Anarchy', 'Empress Maude', 'February', 'Demi Moore', 'Cindy Lou Hensley McCain', 'May', 'Arizona', 'United States Senator', 'Europe', 'Wallis', 'Selena', 'Chris', 'Selena y Los Dinos', 'Tejano', 'Christopher Gilbert', 'Dax Shepard', 'Stephen', 'Infinity', 'Ptolemy XIV', 'Ptolemy XIII', 'David Beckham', 'Geoffrey', 'Anjou']\n"
     ]
    }
   ],
   "source": [
    "def graph_query(graph, frame, given_id, query):\n",
    "   if query in 'CONTRADDICTION':\n",
    "      return [name for name, ids in graph[frame].items() if given_id not in ids]\n",
    "   \n",
    "   elif query in 'ENTAILMENT':\n",
    "      return [name for name, ids in graph[frame].items() if given_id in ids]\n",
    "\n",
    "   else:\n",
    "      print(\"Error: invalid query\")\n",
    "\n",
    "\n",
    "id = '93454'\n",
    "filtered_values = graph_query(\n",
    "   graph=relational_graph, \n",
    "   frame='marry.01', \n",
    "   given_id=id,\n",
    "   query='C'\n",
    ")\n",
    "print(filtered_values)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relational_graph['ALLY_ASSOCIATE_MARRY'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
